

# 스프링 마이크로서비스 코딩 공작소



스프링 마이크로서비스 코딩 공작소 책 내용 및 정리한 글



<br>



## MSA란 ?

- 2014년 즈음부터 시작한 마이크로서비스는 느슨히 결합된 작은 분산서비스
- 대형 어플리케이션을 관리하기 쉽고, 제한된 책임을 담받하는 컴포넌트로 분해할 수 있음
- 코드 베이스를 명확히 정의한 작은 조각으로 분리해서 대형 코드 베이스에서 발생하는 전통적인 복잡성 문제를 해결 가능함
- 어플리케이션 기능을 분해하고 분리해서 완전히 상호 독립적으로 구성해야함
- 어플리케이션 로직을 각자 책임이 명확한 작은 컴포넌트로 분해하고 이들을 조합해서 솔루션을 제공함

<br>

## MSA vs Monolithic

- 유연성 : MSA는 새로운 기능을 신속하게 제공하도록 분리된 서비스를 구성하고 재배치할 수 있음. 결합도를 낮추기때문에 코드단위, 복잡성 등이 낮아 배포 및 테스트 하는 시간도 줄음.
- 회복성: Monolithic에서의 장애는 전체 시스템 장애를 야기시키나 MSA의 경우 어플리케이션의 작은 부분에 국한되어 어플리케이션의 전체 장애로 확대되지  않음.
- 확장성: 분리된 서비스를 여러 서버에 수평벅으로 쉽게 분산할 수 있음. 기능 및 서비스를 적절히 확장 가능. 어플리케이션의 모든 로직이 얽혀 있는 Monolithic 어플리케이션은 한 부분이 병목점이 되더라도 전체를 확장해야함.



**MSA = 작고 단순하며 분리된 서비스 = 확장 가능하고 회복적이며 유연한 어플리케이션**

<br>

## MSA를 구축할때 고려해야할 점

- 적정 크기: 마이크로서비스가 과도한 책임을 맡지 않도록 어떻게 적절한 크기로 만들 수 있는가?
- 위치투명성: 마이크로서비스 어플리케이션에서 서비스의 다수 인스턴스가 재빨리 시작하고 종료될 때 서비스 호출에 대한 물리적 상세 정보를 관리할 수 있는 방법은 무엇인가?
- 회복성: 장애가 발생한 서비스를 우회하고 '빨리 실패'하는 방법을 사용해 어떻게 사용자를 보호하고 어플리케이션의 전반적 무결성을 유지할 것인가?
- 반복성: 새로운 서비스 인스턴스가 시작할 떄마다 운영 환경의 다른 서비스 인스턴스 구성과 코드베이스를 동일하게 만드는 방법은 무엇인가?
- 확장성: 비동기 프로세싱과 이벤트를 사용해 서비스 간 의존성을 최소화하고 마이크로서비스를 원만하게 확장할 수 있는 방법은 무엇인가?



<br>

## MSA 개발 패턴



### 1. 핵심 개발 패턴(core development patterns)

- 서비스 세분성: 비즈니스 영역을 마이크로 서비스로 분해해서 각 마이크로서비스가 적정 수준의 책임을 갖게하는 방법은 무엇인가? 서비스가 다른 비즈니스 문제 영역과 책임이 겹치도록 넘 굵게 나뉘면 시간이 지나면서 유지보수와 변경이 어려워진다. 서비스가 너무 잘게 나뉘면 어플리케이션의 전반적 복잡성이 증가되고 서비스는 데이터 저장소에 액세스하는 것 외에 아무런 로직도 없는 멍청한 데이터 추상화 계층으로 정락한다.
- 통신프로토콜: 개발자가 여러분의 서비스와 어떻게 통신하는가? 마이크로서비스와 데이터 교환을 위해 XML 이나 JSON 등 ..
- 인터페이스설계 : 개발자가 서비스 호출에 사용하는 실제 서비스 인터페이스를 설계하는 최선의 방법은 무엇인가? 서비스 의도를 전달하도록 서비스 url을 구조화하는 방법은 무엇인가? 서비스 버전관리는? 잘 설계된 마이크로서비스 인터페이스가 있다면 서비스를 직관적으로 사용할 수 있다.
- 서비스 간 이벤트 프로세싱: 서비스 간 하드 코딩된 의존성을 최소화하고 어플리케이션 회복성을 높이기 위해 마이크로 서비스를 분리하는 방법은 무엇인가?



### 2. 라우팅 패턴(routing patterns)

- 서비스 디스커버리: 클라이언트 어플리케이션이 서비스 위치를 하드 코딩하지 않고 찾을 수 있도록 마이크로서비스를 어떻게 탐색 가능하게 만들 수 있을까? 오작동하는 마이크로서비스 인스턴스를 가용 서비스 인스턴스 풀에서 어떻게 제거해야 할까?
- 서비스 라우팅: 보안 정책과 라우팅 규칙을 마이크로서비스 어플리케이션의 여러 서비스와 서비스 인스턴스에 차별 없이 적용하기 위해 모든 서비스에 대한 단일 진입점을 제공하는 방법은 무엇인가? 팀의 각 개발자가 서비스에 라우팅을 제공하기 위한 솔루션을 자체적으로 마련할 필요가 없도록 하는 방법은 무엇인가? 



### 3. 클라이언트 회복성 패턴(client resiliency patterns)

- 클라이언트 측 부하 분산: 카이크로서비스의 여러 인스턴스에 대한 호출이 정상 인스턴스에 분산하도록 서비스 인스턴스의 위치를 캐싱하는 방법은 무엇인가?

- 회로 차단기 패턴: 클라이언트가 고장 나거나 성능 문제가 있는 서비스를 계속 호출하지 않게 하는 방법은 무엇인가? 서비스가 느리게 실행되면 서비스를 호출하는 클라이언트의 리소스가 소모된다. 비정상 마이크로서비스 호출이 빨리 실패하면 호출 클라이언트가 신속하게 응답하고 적절한 조취를 취할수 있다.
- 폴백 패턴: 서비스 호출이 실패할 때 호출한 마이크로서비스가 아닌 다른 대체 수단을 사용해 서비스 클라이언트가 작업을 수행할 수 있게 하는 플러그인 매커니즘을 어떻게 제공할 것인가? 
- 벌크헤드 패턴: 마이크로서비스 어플리케이션은 작업을 수행하기 위해 여러 분산 자원을 사용한다. 오작동하는 서비스 호출 하나가 나머지 어플리케이션에 부정적인 영향을 미치지 않도록 이러한 호출을 구분하는 방법은 무엇인가? 

### 4. 보안 패턴(seucirty patterns)

- 인증: 서비스를 호출하는 서비스 클라이언트가 자신이라는 것을 어떻게 알 수 있는가?
- 인가: 마이크로서비스를 호출하는 서비스 클라이언트가 수행하려는 작업을 수행할 자격이 있는지 어떻게 알 수 있는가?
- 자격증명관리와 전파: 서비스 클라이언트가 한 트랜잭션과 관련된 여러 서비스 호출에서 자격증명을 항상 제시하지 않아도 될 방법은 무엇인가? 특히 OAuth와 JWT 같은 토큰 기반 보안 표준을 사용해 토큰을 얻는 방법 등 ..

### 5. 로깅 및 추적 패턴(logging and tracing patterns)

- 로그 상관관계: 단일 트랜잭션에 대해 여러 서비스 간 생성된 모든 로그를 함께 연결하는 방법은 무엇인가? 이 패턴을 이용해 상관관계 ID를 구현하는 방법을 살펴볼 것이다.
- 로그 수집: 이 패턴으로 서비스에서 생성된 모든 로그의 질의 가능한 단일 데이터베이스로 취합하는 방법을 살펴볼 것이다.
- 마이크로서비스 추적: 끝으로 트랜잭션과 연관된 모든 서비스에서 클라이언트 트랜잭션 흐름을 시각화하고 트랜잭션과 관련된 서비스의 성능 특성을 이해하는 방법을 살펴볼 것이다.



### 6. 빌드 및 배포패턴(build a deployment patterns)

- 빌드 및 배포 파이프라인: 조직의 모든 환경에서 원클릭 빌드와 배포를 강조하는 반복 가능한 빌드 및 배포 프로세스를 어떻게 만드는가?
- 코드형 인프라스트럭처: 소스 제어를 사용해 실행하고 관리할 수 있는 코드로 서비스 프로비저닝을 처리하는 방법은 무엇인가?
- 불변 서버: 마이크로서비스 이미지를 생성하고 배포한 후 변경하지 못하게 하려면 어떻게 해야하는가?
- 피닉스 서버: 서버가 오래 실행될수록 구성 편차가 발생할 가능성도 높아진다. 마이크로서비스를 실행하는 서버를 정기적으로 해체하고 불변 이미지를 재생성하려면 어떻게 해야 하는가?



<br>



## @EnableCircuitBreaker

스프링 마이크로서비스에 이 어플리케이션에서 넷플릭스 히스트릭스 라이브러리가 사용된다고 알려준다. 



## @EnableEurekaClient

@EnableEurekaClient 어노테이션은 마이크로서비스 자신을 유레카 서비스 디스커버리 에이전트에 등록하고 서비스 디스커버리를 사용해 코드에서 원격 REST 서비스의 엔드포인트를 검색할 것을 지정한다. 

Spring 의 기본 RestTemplate가 아니라 **수정된 RestTemplate**을 사용하도록 지정한다. 이 RestTemplate 클래스로 호출하려는 서비스의 논리적 서비스 ID를 전달할 수 있다.



```
ResponseEntity<Strping> restExchange = restTemplate.exchange(http://logical-service-id/name/{firstName}/{lastName})
```



내부적으로 RestTemplate는 유레카 서비스에 접속해 개 이상의 logical-service-id 서비스 인스턴스에 대한 물리적인 위치를 검색한다.



## @HystrixCommand

1. 이 어노테이션이 붙어있는 메서드는 메서드가 호출될 때 직접 호출되지 않고 히스트릭스가 관리하는 스레드 풀에 위임한다. 호출이 너무 오래걸리면(기본값 1초) 히스트릭스가 개입하고 호출을 중단시킨다. 이것이 바로 **회로차단기 패턴**의 구현이다. 
2. 히스트릭스가 관리하는 helloThreadPool 이라는 스레드 풀을 만드는것이다. 이 메서드에 대한 모든 호출은 이 스레드 풀에서만 발생하며 수행중인 다른 원격 서비스 호출과 격리된다.

<br>



## 비지니스 문제를 인식하고 마이크로 서비스 후보로 분해하기

1. 비지니스 문제를 기술하고 그 문제를 기술하는데 사용된 명사에 주목하라

문제를 기술하는 데 동일한 명사가 반복해서 사용되면 대개 핵심 비지니스 영역과 마이크로서비스로 만들 기회가 드러난다.

2. 동사에 주목하라

동사는 행위를 부각하고 문제가 되는 영역의 윤과을 자연스럽게 드러낸다.

3. 데이터 응집성을 찾아라

비지니스 문제를 각 부분으로 분해할 때 서로 연관성이 높은 데이터 부분들을 찾는다. 마이크로 서비스는 자기 데이터를 완전히 소유해야 한다.

<br>

## MSA 세분화시 고려사항

1. 큰 마이크로서비스에서 시작해 작게 리팩토링하는 것이 더 낫다

마이크로 서비스의 여정을 시작할 때 의욕이 지나쳐 모든 것을 마이크로서비스로 만들어 버리기 쉽다. 하지만 문제 영역을 한 번에 자은 서비스들로 분해하는 것은 마이크로서비스가 그저 단순한 데이터 서비스로 전락하기 때문에 너무 일찍 복잡함을 겪게 된다.

2. 서비스 간 교류하는 방식에 먼저 집중한다

이는 문제 영역에 대한 큰 단위의 인터페이스를 맏느는데 도움이 된다. 큰 것을 작게 리팩토링하는 것이 더 쉽다.

3. 문제 영역에 대한 이해가 깊어짐에 따라 서비스 책임도 계속 변한다.

새로운 어플리케이션 기능이 요구될 때 종종 마이크로서비스가 책임을 맡는다. 마이크로서비스는 단일 서비스에서 시작해 여러 서비스로 분화되며 성장하는데, 원래 서비스는 새로운 서비스들을 오케스트레이션하고 어플리케이션의 다른 부분에서 새 서비스들의 기능을 캡슐화한다.

<br>

## 안좋은 마이크로서비스의 징후



### 너무 큰 단위의 마이크로 서비스

- 책임이 너무 많은 서비스: 이 서비스에서 비즈니스 로직의 일반 흐름은 복잡하며 지나치게 다양한 종류의 비지니스 규칙을 시행하게 된다.
- 많은 테이블의 데이터를 관리하는 서비스: 마이크로서비스는 자기가 관리하는 데이터를 기록하는 시스템이다. 여러 테이블에 데티러르 저장하거나 직속 데이터베이스 외부의 테이블에 액세스하고 있다면 서비스가 너무 크다는 것을 암시한다. 이 책의 저자는 3~5개 이내의 테이블을 소유해야 한다는 지침을 세웠다 이보다 더 많다먼 서비스가 너무 많은 책임을 담당할 가능성이 높다 ... 와우
- 과다한 테스트 케이스: 시간이 지나면서 서비스와 크기와 책임이 늘어날 수 있다. 서비스가 적은 수의 테스트 케이스로 시작해 수백개의 단위 테스트와 통합 테스트 케이스로 늘어난다면 리팩토링이 필요할 것이다.



### 너무 작은 단위의 마이크로 서비스

- 한 문제 영역 부분에 속한 마이크로서비스가 토끼처럼 번식한다: 몯느 것이 마이크로서비스로 되면 작업 수행에 필요한 서비스 개수가 엄청나게 증가해서 서비스에서 비지니스 로직을 만드는 것이 복잡하고 어려워진다. 어플리케이션에 수십개의 마이크로서비스가 있고 각 서비스가 하나의 데이터베이스 테이블과 통신할 때 악취가 나곤 한다.
- 마이크로서비스가 지나치게 상호 의존적이다: 문제 영역의 한 부분에 이쓴ㄴ 마이크로서비스는 하나의 사용자 요청을 완료하기 위해 각 서비스가 서로 계속 호출한다.
- 마이크로서비스가 단순한 CRUD 집합이 된다: 마이크로서비스는 비지니스 로직의 표현이지 데이터 소스의 추상화 계층이 아니다. 마이크로서비스가 CRUD 관련 로직만 수행한다면 너무 잘게 나뉘어 있다는 의미다.



마이크로 서비스 아키텍쳐는 처음부터 올바르게 설계하기가 어렵기때문에 진화론적 사고 과정으로 개발해야한다.위에서 언급했듯이 **큰 것에서 작은 것 으로 나누기**의 좋은 예다. 

<br>



## 서비스 인터페이스 지침

개발자가 1~2개의 마이크로 서비스를 익혔다면 모든 서비스에 대한 동작 규칙을 습들할 수 있어야한다

- REST 철학을 수용하라: 서비스에 대한 REST 방식은 서비스 호출 프로토콜로 HTTP를 수용하고 HTTP 동사를 사용하는 것이 핵심이다. HTTP 동사를 기반으로 기본 행동 양식을 모델린하다.
- URI를 사용해 의도를 전달하라: 서비스의 엔드포인트로 사용되는 URI는 문제 영역에 존재하는 다양한 자원을 기술하고 자원 관계에 대한 기본 메커니즘을 제공해야 한다.
- 요청과 응답에 JSON을 사용하라
- HTTP 상태 코드로 결과를 전달하라: HTTP 프로토콜에는 서비스의 성광과 실패를 명시하는 풍부한 표준 응다봌드가 있다. 상태코드를 익히고 모든 서비스에 일관되게 사용하는 것이 매우 중요하다.



## 마이크로서비스를 사용하지 않아야 할 때

### 1.분산 시스템 구축의 복잡성

마이크로 서비스는 모놀리틱에 없던 복잡성을 가져온다. 높은 수준의 운영 성숙도도 필요로 하기 떄문에 고도로 분산된 어플리케이션의 성공에 필요한 자동화와 운영 작업에 투자할 의사가 없는 조직이라면 마이크로 서비스를 고려하지 않는 것이 좋다.

### 2. 서버 스프롤(server sprawl)

마이크로서비스의 유연성은 모든 서버를 운영하는데 드는 비용과 함께 따져 보아야 한다. 스프롤은 활용도가 낮은 여러 서버가 실제 작업량보다 더 많은 공간과 리소르를 차지하는 현상을 의미한다.

### 3. 어플리케이션 유형

마이크로서비스는 재사용성을 추구하며 높은 회복성과 확장성이 필요한 대규모 어플리케이션의 구축에 매우 유용하다. 소형 어플리케이션이나 소수 사용자를 위한 어플리케이션을 개발할 때 마이크로서비스와 같은 분산 모델로 구축한다면 배보다 배꼽이 더 클 수도 있다.

### 4. 데이터 변환과 일관성

서비스의 데이터 사용 패턴과 서비스 소비자가 어떻게 서비스를 사용하는지 고민해야한다. 마이크로서비스는 적은 수의 테이블을 둘러싸고 추상화하며 저장소에 단순한 질의 생성, 추가, 실행 등 '운영상의' 작업을 수행하는 메커니즘으로도 잘 동작한다. 어플리케이션이 여러 데이터소스에서 복잡한 데이터를 취합하고 변환해야 할 경우 마이크로서비스의 분산된 특성때문에 작업이 어려워진다. 마이크로 서비스에는 서비스 사이에 트랜잭션을 처리하는 표준이 없기때문에 필요하다면 직접 만들어야한다.  메시징을 사용해서 통신 가능하지만 메시징에는 지연시간이 발생하기때문에 어플리케이션은 최종 일관성을 유지해아한다.

<br>

## 마이크로서비스에서 JSON을 사용하는 이유

1. xml 기반의 SOAP와 같은 프로토콜과 비교할 때 JSON은 적은 텍스트로 데이트 표현이 가능하다
2. JSON은 가독성이 높고 사용하기 쉽다. 
3. JSON은 javscript에서 사용하는 기본 직렬화 프로토콜이다.

<br>

## 엔드포인트 작명 지침

1. **서비스가 제공하는 리소스를 알 수 있는 명확한 URL이름을 사용하라**: URL을 정의하는데 표준 형식을 사용하면 API의 직관성과 사용 편의성이 향상된다. 일관된 명명 규칙을 사용한다.
2. **리소스 간 관계를 알 수 있는 URL을 사용하라**: 마이크로서비스들이 가진 리소스 사이에서 부모 자식 관계가 생기는데, 외부에 부모 리소스 컨텍스트는 존재하지만 자식 리소스는 외부에 노출되지 않는 관계가 있다. 이러한 관계는 URL을 사용해 표현한다. 하지만 RUL이 지나치게 길어지거나 중첩되는 경향이 있다면 해당 마이크로서비스가 너무 많은 일을 하려는 것일지도 모른다.
3. **URL 버전 체계를 일찍 세워라**: RUL과 엔드포인트는 서비스 소유자와 서비스 소비자 간의 계약을 의미한다. 일반적 패턴 중 하나는 모든 엔드포인트 앞에 버전 번호를 붙이는 것이다. 일찍 버전 체계를 갖추고 준수하자. 소비자가 URL을 사용한 후 URL 버전 체계를 개량하는 것은 매우 어려운 일이다

<br>

## Twelve-Factor 마이크로서비스 어플리케이션 구축 지침

1. **코드베이스**: 모든 어플리케이션 코드와 서버 프로비저닝 정보는 버전관리 되어야한다. 각 마이크로 서비스는 소스 제어 시스템 안에 독립적인 코드 저장소를 가져야한다.
2. **의존성**: 어플리케이션이 사용하는 의존성을 메이븐 같은 빌드 도구를 이용해 명시적으로 선언해야 한다. 제3자의 jar의존성은 특정 버전 번호를 붙여 명시해 선언해야 한다. 따라서 동일 버전의 라이브러리를 사용해 항상 마이크로서비스를 빌드할 수 있다.
3. **구성**: 어플리케이션 구성(특히 환경별 구성)을 코드와 독립적으로 저장하자. 어플리케이션 구성운 절대로 소스코드와 동일한 저장소에 있으면 안된다.
4. **백엔드 서비스**: 마이크로서비스는 대게 네트워크를 거쳐 데이터베이스나 메시징 서비스와 통신한다. 그렇다면 언제든 데이터베이스 구현을 자체 관리형 서비스에서 외부업체 서비스로 교체할 수 있어야 한다.
5. **빌드, 릴리즈, 실행**: 배포할 어플리케이션의 빌드, 릴리즈, 실행 부분을 철처하게 분리하라. 코드가 빌드되면 개발자는 실행중에 코드를 변경할 수 없다. 모든 변경사항을 빌드 프로세스로 되돌려 재배포해야한다. 빌드된 서비스는 불변적이므로 변경할 수 없다.
6. **프로세스**: 마이크로서비스는 항상 무상태방식을 사용해야 한다. 서비스 인스턴스 손실에 의해 데이터가 손실될 것이라는 우려 없이 언제든 서비스를 강제 종료하거나 교체할 수 있다.
7. **포트 바인딩**: 마이크로서비스는 서비스용 런타임 엔진을 포함한(실행 파일에 패키징된 서비스를 포함한) 완전히 자체 완비형이다. 별도의 웹 또는 어플리케이션 서버 없이도 서비스는 실행되어야 한다. 서비스는 명령행에서 단독으로 시작하고 노출한 http포트를 통해 즉시 액세스 할 수 있어야 한다.
8. **동시성**: 확장해야 한다면 단일 서비스 안에서 스레드 모델에 의존하지 마라. 그 대신 더 많은 마이크로서비스를 시작하고 수평 확장하라. 마이크로서비스 안에서 스레드 사용을 배제하지는 않지만 확장을 위한 유일한 메커니즘으로 믿지 말라. 수직 대신 수평 확장하라
9. **폐기 가능**: 마이크로서비스는 폐기 가능하므로 요구에 따라 시작 및 중지할 수 있다. 시작 시간은 최소화하고 운영 체제에서 강제 종료 신호를 받으면 프로세스는 적절히 종료해야한다.
10. **개발 및 운영 환경 일치**: 서비스가 실행되는 모든 환경(개발자의 데스크톱 환경도 포함) 사이의 차이를 최소화하라 개발자는 서비스 개발을 위해 실제 서비스가 실행되는 동일한 인프라스트럭처를 로컬에 사용해야 한다. 이는 환경 간 서비스 배포가 수 주가 아닌 수 시간 안에 이루어져야 한다는 것을 의미한다. 코드가 커밋되자마자 테스트가 되고 가능한 신속하게 개발 환경에서 운영 환경으로 전파되어야 한다.
11. **로그**: 로그는 이벤트 스트림이다. 로그가 기록될 때 Splunk, Fluentd 같은 도구로 로그가 스트리밍되어야 한다. 이들 도구는 로그를 수집해 중앙에 기록한다. 마이크로서비스는 이러한 로깅 동작방식에 신경쓰지 않아야 하며, 개발자는 표준 출력으로 출력된 로그를 시각적으로 확인할 수 있어야 한다.
12. **관리 프로세스**: 개발자는 종종 담당 서비스에 대해 데이터 마ㅇ그레이션이나 변환처럼 관리 작업을 수행해야 한다. 이러한 작업은 임의로 수정되면 안되고 소스 코드 저장소에 유지 및 관리되는 스크립트에 의해 수행되어야 한다. 이 스크립트는 실행될 각 환경에 반복적으로 수행 가능하고 환경을 위해 변경되지 않아야 한다. 즉, 각 환경에 맞추어 스크립트를 수정하지 않는다.

<br>

## 마이크로서비스 역할 관점

1. **아키텍트**: 비지니스 문제의 실제 윤곽을 잡는다. 비지니스 문제 영역을 기술하고 이야기되는 시토리를 경청하고, 출현할 마이크로서비스 후보에 주시하자. 처음부터 잘게 나뉜 많은 서비스에서 시작하는 것보다 굵게 나뉜 마이크로서비스에서 시작해서 작은 서비스로 리팩토링하면 더 낫다는 것도 기억하자. 대부분 좋은 아키텍처와 마찬가지로 마이크로서비스 아키텍처도 창발적이며 사전에 세세히 계획되는 것은 아니다.

2. **소프트웨어 엔지니어**: 서비스가 작다는 사실이 좋은 설계 원칙을 포기하라는 것은 아니다. 서비스안의 각 계층이 개별 책임을 맡는 계층적 서비스를 구축하는데 집중한다. 코드 내 프레임워크를 만들려는 유호긍ㄹ 피하고 완전히 독립적인 마이크로서비스를 지향한다. 미슥한 프레임워크설계와 도입은 어플리케이션의 수명 주기 후반에 막대한 유지 보수 비용을 초래할 수 있다.
3. **데브옵스 엔지니어**: 서비스는 외부와 단절된 것이 아니다. 서비스 수명 주기를 일찍 수립하자. 데브옵스 관점에서 서비스 빌드와 배포를 자동화하는 방법뿐 아니라 서비스 상태를 모니터링하고 문제가 발생할 때 대응하는 방법도 집중해야 한다. 대개 서비스 운영은 비지니스 로직의 작성보다 더 많은 업무와 사전 숙고가 필요하다



# #3 스프링 클라우드 컨피그 서버로 구성 관리하기



## 구성관리 원칙

1. **분리**: 실제 물리적인 서비스의 배포와 서비스 구성 정보를 완전히 분리하고자 한다. 어플리케이션 구성 정보를 서비스 인스턴스와 함께 배포하면 안된다. 그 대신 시작하는 서비스에 환경변수로 전달하거나 중앙 저장소에서 읽어와 구성 정보를 전달해야 한다.
2. **추상화**: 서비스 인터페이스 뒷 단에 있는 구성 데이터의 접근 방식을 추상화한다. 서비스 저장소에 직접 액세스하는 코드를 작성하기(즉, 파일이나 jdbc를 사용해 데이터베이스에서 데이터를 읽기)보다 어플리케이션이 REST 기반의 JSON 서비스를 사용해 구성 데이터를 조회하게 만들어야 한다.
3. **중앙 집중화**: 클라우드 기반의 어플리케이션에는 말 그대로 수백개의 서비스가 존재할 수 있으므로 구성 정보를 보관하는 저장소 개수를 최소로 줄이는 것이 매우 중요하다. 어플리케이션의 구성 정보를 가능한 소수 저장소에 집중화 한다.
4. **견고성**: 어플리케이션 구성 정보를 배포된 서비스와 완전히 분리하고 중앙 집중화 하므로 어떤 솔루션을 사용하더라도 고가용성과 다중성을 구현할 수 있어야 한다.

<br>

## 구성관리 lifecycle

1. 마이크로서비스 인스턴스가 시작하면 서비스 엔드포인트를 호출해 동작중인 환경별 구성 정보를 읽어온다. 구성 관리 서비스에 연결할 정보는 마이크로서비스가 시작할 때 전달된다.
2. 실제 구성 정보는 저장소에 상주한다. 구성 데이터를 보관할 수 있는 구성 저장소 구현 방식이 다양하며, 소스 관리되는 파일이나 관계형 데이터베이스, 키-값 짝 데이터 저장소 같은 구현 방식을 택할 수 있다.
3. 실제로 어플리케이션 배포 방식과 독립적으로 어플리케이션의 구성 데이터를 관리한다. 대개 빌드 및 배포 파이프라인으로 구성 관리를 변경하며 변경된 구성은 버전 정보 태그를 달아 다른 환경에 배포될 수 있게 한다.
4. 구성 관리가 변경되면 어플리케이션 구성 데이터를 사용하는 서비스는 변경 통보를 받고 보유한 어플리케이션 데이터 사본을 갱신해야 한다.

<br>

## 구성관리 오픈소스 툴



| 프로젝트 이름               | 설명                                                         | 특성                                                         |
| :-------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Etcd                        | Go로 작성된 오픈소스 프로젝트, 서비스 검색과 key-value 관리에 사용되며 분산 컴퓨팅 모델용 Raft 프로토콜을 사용한다 | 1. 초고속이며 확장 가능<br />2. 분산 가능<br />3.명령줄 위주<br />4.사용과 설치가 쉬움 |
| 유레카                      | 넷플릭스가 만들었으며, 수많은 실전 테스트를 거쳤다. 서비스 검삭과 key-value 관리에 사용된다 | 1. 분산 key-value 저장소<br />2.유연하지만 설정하는데 공수가 든다.<br />3. 동적 클라이언트 갱신 기능을 제공한다 |
| 콘설                        | 하시코프가 만들었따. Etcd 및 유레카와 유사한 긴으을 제공하나 분산 컴퓨팅 모델에 따른 알고리즘을 사용한다. | 1.빠르다<br />2. DNS와 직접 통합해 네이티브 서비스 검색을 제공한다<br />3. 동적 클라이언트 갱신은 기본 기능으로 제공하지 않는다. |
| 주키퍼                      | 분산 잠금 기능을 제공하는 아파치 프로젝트로 key-value 데이터용 구성관리 솔루션으로 자주 사용된다 | 1. 가장 오래되고 실전 경험이 많은 솔루션<br />2. 가장 사용하기 복잡하다.<br />3. 구성 관리에 사용 가능하지만 이미 사용중일떄만 고려해야 한다. |
| 스프링 클라우드 컨피그 서버 | 다양한 백엔드와 함께 일반적인 구성관리 솔루션을 제공하는 오픈소스 프로젝트다. 깃, 유레카 및 콘설 같은 백엔드와 통합 가능하다 | 1.비분산 key-value 저장소<br />2. 스프링 및 스프링 기반이 아닌 서비스와 통합 가능하다.<br />3. 공유 파일과 시스템, 유레카, 콘설, 깃 등 구성 데이터를 저장하기 위한 다양한 백엔드 사용이 가능하다. |

<br>

## 스프링 클라우드 컨피그 서버의 특징

1. 스프링 클라우드 컨피그 서버는 쉽게 설치하고 사용할 수 있다.
2. 스프링 클라우드 컨피그는 스프링 부트와 긴밀히 통합되어 있다. 따라서 모든 어플리케이션의 구성 데이터를 사용이 간편한 어노테이션으로 읽어올 수 있다.
3. 스프링 클라우드 컨피그 서버는 구성 데이터를 저장할 수 있는 여러 백엔드를 지원한다. 유레카나 콘설같은 도구를 이미 사용하고 있다면 바로 스프링 클라우드 컨피그 서버에 연결할 수 있다.
4. 스프링 클라우드 컨피그 서버는 깃 소스제어 플랫폼과 직접 통합할 수 있다. 스프링 클라우드 컨피그를 깃과 통합하면 다른 솔루션의 추가 의존성을 제거하고 어플리케이션의 구성 데이터를 손쉽게 버전 관리할 수 있다.Etcd, 콘설, 유레카 같은 다른 도구는 자체 버전 관리 기능이 없으므로 필요하다면 직접 구축해야 한다. 이미 깃을 사용하고 있다면 스프링 클라우드 컨피그 서버는 매력적인 옵션이다.



## 스프링 클라우드 컨피그 서버의 대칭키 암호화



### 설치 및 적용

1. 암호화에 필요한 오라클 JCE jar 파일을 내려받고 설치한다.
2. 암호화 키를 설정한다.
3. 프로퍼티를 암호화 및 복호화한다.
4. 클라이언트측에서 암호화하도록 마이크로서비스를 구성한다.



다운로드 경로: 

https://www.oracle.com/java/technologies/javase-jce8-downloads.html

검색 키워드 : Java Cryptography Extensions



1. $JAVA_HME/jre/lib/security에 있는 local_policy.jar와 US_export_policy.jar 파일을 다른 위치로 백업한다
2. 오라클에서 내려받은 JCE.zip 안에 있는 local_policy.jar와 US_export_policy.jar \을 복사한다
3. 스프링 클라우드 컨피그가 암호화를 사용하도록 구성한다.

### 암호화 키 설정

1. 대칭 키의 길이는 12자 이상이어야하며 불규칙 문자열이 이상적

2. 대칭키를 분실하면 안된다. 암호화 키로 한번 암호화한 것은 그 키 없이는 복호화 할 수 없다.

   



# #4 서비스 위치 찾기

분산 아키텍처에서는 시스템의 물리적 위치를 찾아야함 공식적으로 서비스 디스커버리라는 명칭을 사용,

서비스 디스커버리는 어플리케이션에서 사용하는 모든 원격 서비스의 주소가 포함된 프로퍼티 파일을 관리하는 것처럼 단순하거나 UDDI 저장소처럼 정형화되고 복잡한 것일 수 있음

## 서비스 디스커버리의 중요성

1. 어플리케이션 팀은 서비스 디스커버리를 사용해 해당 환경에서 실해앟는 서비스 인스턴스 개수를 신속하게 수평 확장하거나 축소할 수 있다. 서비스 디스커버리를 통해 서비스의 물리적인 위치는 서비스 소비자에게 드러나지 않는다. 서비스 소비자는 실제 서비스 인스턴스의 물리적 위치를 모르기 때문에 서비스 풀에서 새로운 서비스 인스턴스의 추가나 삭제가 자유롭다. 이런 개념은 모놀리틱과 싱글테넌트 어플리케이션에 익숙한 개발팀에게 수평적인 확장하는 접근방식으로 사고의 전환 가능
2. 어플리케이션 회복성을 향상하는데 도움이 된다. 마이크로서비스 인스턴스가 비정상이거나 가용하지 않다면 대부분의 서비스 디스커버리 엔진은 내부의 가용 서비스 목록에서 해당 인스턴스를 제거시킨다. 서비스 디스커버리 엔진이 사용할 수 없는 서비스를 피해 라우팅하므로 다운된 서비스가 야기한 피해를 최소화 한다.

## 서비스 위치 찾기

서비스 디스커버리를 사용하지 않은 환경에서는 대개 DNS와 네트워크 로드 밸런서로 어플리케이션을 호출한다.

서비스요청자에게 요청을 받고 -> 로드밸런서는 사용자가 액세스하려는 경로를 기반으로 라우팅 테이블에서 물리적 주소 항목 찾기 -> 로드밸런서가 서버 목록에서 한개의 서버에 요청 전달

위 방법은 마이크로 서비스에서는 적합하지 못함

1. 단일 장애 지점: 로드 밸런서가 고가용성을 지원한다고 해도 여전히 전체 인프라의 단일 장애 지점임. 로드 밸런서가 다운되면 로드 밸런서에 의존하는 모든 어플리케이션도 다운됨. 로드 밸런서를 고가용하게 만들더라도 어플리케이션 인프라 안에서 집중화된 병목 지점이 될 가능성이 높음
2. 수평 확장의 제약성: 로드 밸런서 클러스터에 서비스를 모아 연결하므로 부하 분산 인프라를 여러 서버에 수평적으로 확장할 수 있는 능력이 제한됨 상용 로드밸런서는 다수의 중복성 모델과 라이선싱 비용이라는 두가지 요소에 제약을 받음.  본직적으로 하드웨어의 제약을 받음. 
3. 정적 관리: 전통적 로드 밸런서 대부분은 서비스를 신속히 등록하고 취소하도록 설계되지 않음. 중앙 집중식 데이터베이스를 사용해 경로 규칙을 저장하고 대개 공급업체의 독점적인 api를 사용해야만 새로운 경로를 저장할 수 있음
4. 복잡성: 로드 밸런서가 서비스에 대한 프록시 역할을 하므로 서비스 소비자에게 요청할때 물리적인 서비스에 매핑된 요청 정보가 있어야 함.  이 변환 계층은 서비스 매핑 규칙을 수동으로 정의 배포해야하므로 복잡성 증가. 

## 클라우드에서 서비스 디스커버리

### 서비스 디스커버리 메커니즘

1. 고가용성: 서비스 디스커버리는 서비스 검색 정보를 서비스 디스커버리 클러스터의 여러 노드가 공유하는 핫 클러스터링 환경을 지원해야함. 한 노드가 사용할 수 없게 되면 클러스터의 다른 노드가 인계를 받을수 있어야 함.
2. 피어 투 피어: 서비스 디스커버리 클러스터의 각 노드는 서비스 인스턴스의 상태를 공유함
3. 부하 분산: 서비스 디스커버리는 요청을 동적으로 부하 분산해서 서비스 디스커버리가 관리하는 모든 서비스 인스턴스에 분배해야함. 
4. 회복성: 서비스 디스커버리 클라이언트는 서비스 정보를 로컬에 캐시해야함. 로컬 캐싱 자체가 서비스 디스커버리 기능을 점진적으로 저하시킬 수 있는데 서비스 디스커버리 서비스가 가용하지 않을 때 어플러키에션이 로컬 캐시에 저장된 정보를 기반으로 서비스를 계속 찾을 수 있고 동작하게 해야함.
5. 장애 내성: 서비스 디스커버리는 서비스 인스턴스의 비정상을 탐지하고 가용 서비스 목록에서 인스턴스를 제거해야함. 그리고 이러한 서비스 장애를 감지하고 사람의 개입 없이 조치를 취해야함

### 서비스 디스커버리 아키텍처

1. 서비스 등록: 서비스를 서비스 디스커버리 에이전트에 어떻게 등록하는가?
2. 클라이언트가 서비스 주소 검색: 서비스 클라이언트가 어떻게 서비스 정보를 검색하는가?
3. 정보 공유: 서비스 정보를 노드간에 어떻게 공유하는가?
4. 상태 모니터링: 서비스가 자신의 상태 정보를 서비스 디스커버리 에이전트에 어떻게 전달하는가?

<br>

### 서비스 디스커버리 패턴 흐름

1. 서비스 인스턴스가 시작하면 서비스 디스커버리 인스턴스가 접근할 수 있는 자신의 물리적 위치와 경로, 포트를 등록함 서로 동일한 서비스 id를 등록함
2. 서비스는 일반적으로 개의 서비스 디스커버리 인스턴스에만 등록함
3. 서비스 디스커버리 구현체는 대부분 P2P모델을 사용해 서비스 인스턴스의 데이터를 클러스터에 있는 다른 노드에 전파함
4. 서비스 디스커버리 구현에 따라 전파 메커니즘을 하드코딩된 서비스 목록을 사용하거나 gossip, infection-style 프로토콜을 사용해 클러스터에서 발생된 변경을 다른노드가 발견할 수 있음
5. 서비스 인스턴스는 자기 상태를 서비스 디스커버리 서비스에 푸시하거나 그의 반대로 상태를 추출함. 정상 상태를 반환하지 못한 서비스는 가용한 서비스 인스턴스 풀에서 제거됨
6. 서비스가 서비스 디스커버리에 등록되면 그 서비스의 기능을 사용해야하는 어플리케이션이나 다른 서비스에서 사용할 준비가 된 것임
7. 클라이언트는 서비스 디스커버리 엔진에만 의존함



### 위에있는것 보다 더 강화된 서비스 디스커버리 패턴 (클라이언트 측 부하 분산)

1. 서비스 소비자가 요청한 모든 서비스 인스턴스를 위해 서비스 디스커버리 서비스에 접속한 후 데이터를 서비스 소비자 기기에 로컬 캐시함
2. 클라이언트가 서비스를 호출할 때마다 서비스 소비자는 캐시에서 위치 정보를 검색함. 일반적으로 클라이언트 측 캐싱은 라운드로빈 부하 분산 알고리즘처럼 단순한 알고리즘을 사용해 서비스 호출을 여러 인스턴스로 분산함
3. 클라이언트는 주기적으로 서비스 디스커버리 서비스에 접속해 서비스 인스턴스 캐시를 새로고침함. 클라이언트 캐시는 최종 일관성을 유지하지만 클라이언트가 목록을 새로고침하기 위해 서비스 디스커버리 인스턴스에 접속하고 서비스에 호출할 때 비정상적인 서비스 인스턴스를 호출할 위험성은 항상 존재함. 
4. 서비스를 호출하는 동안 서비스 호출이 실패하면 로컬에 있는 서비스 디스커버리 캐시가 무효화되고 서비스 디스커버리 클라이언트는 서비스 디스커버리 에이전트에 목록 새로고침을 시도함





## 스프링 유레카 설정

### application.yml

- eureka.client.registerWithEureka: 자신을 유레카 서비스에 등록하지 않도록 설정
- eureka.client.fetchRegistry: false로 설정시 유레카 서비스가 시작할때 레지스트리 정보를 로컬에 저장하지 않음. 스프링 부트 서비스로 된 유레카 클라이언트를 유레카에 등록할 경우 이 값을 바꿀 수 있음
- eureka.server.waitTimeInMsWhenSyncEmpty: 유레카는 기본적으로 모든 서비스가 등록할 기회를 갖도록 5분을 기다린 후 등록된 서비스 정보를 공유함. 로컬에서 테스트할때 시간단축용으로 사용



유레카는 등록된 서비스에서 10초 간격으로 연속 3회의 상태정보를 받아야하므로 등록된 개별 서비스를 보여주는데 30초가 걸림

## 스프링 유레카에 서비스 등록

bootstrap.yml 이 application.yml보다 먼저 인식함 장기적인 관점으로 봤을때 bootstrap.yml에 설정해야하는 파일은 bootstrap.yml에 분리하도록 지정하는게 좋음

서비스 인스턴스는 두개의 id를 가져야함

- 어플리케이션 ID: 서비스 인스턴스의 그룹화 ID
- 인스턴스 ID: 서비스 인스턴스의 고유 식별 ID
- eureka.instance.preferIpAddress: 서비스 호스트 이름이 아닌 IP 주소를 유레카에 등록하도록 지정

기본적으로 유레카는 호스트이름으로 접속하는 서비스를 등록함. 이 설정은 DNS가 지원된 호스트 이름을 할당하는 서버 기반 환경에서 잘 동작함. 그러나 컨테이너 기반의 배포에서 컨테이너는 DNS 엔트리가 없는 임의로 생성된 호스트 이름을 부여받아서 시작함. eureka.instance.preferIpAddress을 true로 설정해주어 IP 주소를 전달받는것으로 설정해야 어플리케이션의 호스트를 구해올 수 있음. 저자는 true를 추천함

## 서비스 디스커버리를 사용해 서비스 검색

- Discovery: 디스커버리 클라이언트와 표준 스프링 RestTemplate를 통한 서비스 호출
- Rest: 향상된 스프링 RestTemplate를 사용해 리본 기반의 서비스 호출
- Feign: 넷플릭스 Feign 클라이언트를 사용해서 리본을 통해 서비스 호출

### DisCoveryClient로 서비스 인스턴스 검색

- 리본과 등록된 서비스에 가장 저수준의 접근성 제공
- DisCoveryClient를 사용하면 리본 클라이언트와 해당 url에 등록된 모든 서비스에 대해 질의 가능
- @EnableDiscoveryClient를 이용해 DisCoveryClient와 리본 라이브러리를 사용 가능하도록 명시

**DisCoveryClient의 단점** 

1. 리본 클라이언트 측 부하 분산의 장점 X : 서비스의 호출 책임이 사용자에게 있음
2. 서비스 호출에 사용될 URL을 생성해야함
3. 서비스를 호출하는 더 좋은 방식이 존재함



### 리본 지원 스프링 RestTmeplate를 사용한 서비스 호출

DisCoveryClient를 사용하는것보다  더 일반적인 메커니즘

- @LoadBalanced를 사용해서 RestTemplate 객체를 미리 생성
- @EnableDiscoveryClient 같은 어노테이션은 불필요
- @LoadBalanced를 사용해서 RestTemplate 객체는 스프링 표준 RestTemplate와 유사하게 동작하지만 서비스의 물리적 위치를 사용하는 대신 호출하려는 서비스의 유레카 서비스 ID를 사용함
- ex : http://{applicationId}/{serviceUrl}/{serviceInstanceId}
- 리본은 RestTemplate를 클래스를 사용해 서비스 인스턴스에 대한 모든 요청을 라운드로빈 방식으로 부하 분산

### 넷플릭스 Feign 클라이언트로 서비스 호출

- @EnableFeignClients를 사용함
- java의 interface를 기반으로 사용함 다른코드보다 대체적으로 깔끔하고 추상화도 잘 되어있음
- Feign이 전부 알아서해줌 그냥 연결만 시켜주면됨
- 에러처리의 경우 4xx-5xx는 FeignException으로 반환됨
- 사용자 정의 Exception 클래스로 재 매핑하는 에러 디코더 클래스 작성 기능을 제공함

## 요약

- 서비스 디스커버리 패턴은 서비스의 물리적 위치를 추상화하는데 사용함
- 유레카 같은 서비스 디스커버리 엔진은 서비스 클라이언트에 영향을 주지 않고 해당 환경의 서비스 인스턴스를 원활하게 추가 삭제 가능
- 클라이언트 측 부하 분산을 사용하면 서비스를 호출하는 클라이언트에서 서비스의 물리적인 위치를 캐싱해더 나은 성능과 회복성을 제공 가능 
- 유레카는 네슾ㄹ릭스 프로젝트의 스프링 클라우드와 사용하면 쉽게 구축하고 구성할 수 있음
- 스프링 클라우드와 넷플릭스 유레카 그리고 서비스를 호출하는 넷플릭스 리본으로 다은 세가지 메커니즘 사용 가능
  1. DisCoveryClient
  2. 리본기반 RestTemplate
  3. 넷플릭스 Feign 클라이언트

# #5 나쁜 상황에 대비한 스프링 클라우드와 넷플릭스 히스트릭스의 클라이언트 회복성 패턴

서비스 하나가 충돌하면 쉽게 감지가 가능하지만 서비스가 느려질 경우 성능 저하를 감지하고 우회하는것은 다음 이유로 매우 어렵다

1. **서비스 저하는 간헐적으로 발생하고 확산될 수 있다**: 서비스 저하는 사소한 부분에서 갑자기 발생 가능함. 순식간에 어플리케이션 컨테이너가 스레드 풀을 모두 소진해 완전히 무너지기 전까지 장애 징후는 일부 사용자가 문제점을 불평하는 정도이기 때문
2. **원격 서비스 호출은 대개 동기식이며 오래 걸리는 호출을 중단하지 않는다**: 서비스 호출자에게는 호출이 영구 수행되는 것을 방지하는 타임아웃 개념이 없음. 어플리케이션 개발자는 서비스를 호출해 작업을 수행하고 서비스가 응답할 때까지 기다림
3. **어플리케이션은 대개 부분적인 저하가 아닌 원격 자원의 완전한 장애를 처리하도록 설계된다**: 서비스가 완전히 붕괴되지 않는 이상 서비스를 계속 호출하고 빠른 실패 확인이 불가함. 호출하는 서비스는 제대로 동작하지 않는 어플리케이션을 호출해서 일부 호출은 비정상적인 종료가 있을 수 있다. 

안전 장치가 없다면 제대로 동작하지 않는 서비스 하나가 여러 어플리케이션을 짧은 시간 동안 다운 시킬 수 있다.

클라우드와 마이크로서비스에 기반을 둔 어플리케이션은 사용자의 트랜잭션을 완료하는데 필요한 여러 인프라위에서 세밀하게 분산되어있기 때문에 이런 에러에 매우 취약하다



## 클라이언트 회복성 패턴이란? 

- 클라이언트의 회복성패턴은 원격 서비스가 에러를 던지거나 제대로 동작하지 못해 원격 자원의 접근이 실패할때 원격 자원을 호출하는 클라이언트 충돌을 막는데 초점이 맞추어져 있음
- 이 패턴의 목적은 클라이언트의 상향 전파되는 것을 막음
- 원격자원을 소비하는 클라이언트와 자원 사이에서 패턴을 구현

### **1. 클라이언트 측 부하 분산**

- 클라이언트가 넷플릭스 유레카 같은 서비스 디스커버리 에이전트를 이용해 서비스의 모든 인스턴스를 검색한 후 해당 서비스 인스턴스의 실제 위치를 캐싱함 (서비스 디스커버리 에이전트의 의존성을 어느정도 제거)
- 넷플릭스의 리본 라이브러리
- 서비스 클라이언트가 서비스 인스턴스를 호출해야할 때마다 서비스 클라이언트 측 로드 밸런서는 서비스 위치 풀에서 관리하는 서비스 위치를 하나씩 전달함
- 서비스 클라이언트 측 로드 밸런서는 서비스 클라이언트와 서비스 소비자 사이에 위치하므로 서비스 인스턴스가 에러를 전달하거나 불량 동작하는지 감시함
- 서비스 클라이언트측 로드 밸런서가 문제를 감지할 수 있다면 가용 서비스 위치 풀에서 문제가 된 서비스 인스턴스를 제거해 서비스 호출이 그 인스턴스로 전달되는 것을 막음

### 2. 회로 차단기

- 전기 회로의 차단기를 본 떠 만든 클라이언트 회복성 패턴, 전기 시스템에서 회로 차단기는 전기선에 유입된 과젼류를 감지하는데 회로 차단기가 문제를 감지하면 모든 전기 시스템과 연결된 접속을 차단하고 하위 컴포넌트가 과전류에 손상되지 않도록 보호하는 역할을 함
- 소프트웨어 회로 차단기는 원격 서비스 호출을 모니터링함. 호출이 필요한 만큼 실패하면 회로차단기가 활성화 되어 빨리 실패하게 만들고 고장난 원격 자원은 더이상 호출되지 않도록 차단함

### 3. 폴백

- 원격 서비스에 대한 호출이 실패할 때 예외를 발생시키지 않고 서비스 소비자가 대체 코드 경로를 실행해 다른 방법으로 작업 수행이 가능하도록 함
- 예를들어 사용자에게 이 사용자가 관심있는 물품을 추천하는 기능이 있다고 했을때 보통 마이크로서비스를 호출해 사용자의 과거 행위를 분석하고 특정사용자에게 맞춤화된 추천 목록을 전달함. 하지만 만약 이 서비스가 고장 난다면 폴백은 모든 사용자의 구매 정보를 기반으로 더 일반화된 선호 목록을 조회함 이 일반화된 선호 목록은 완전히 다른 서비스에서 추출됨

### 4. 벌크헤드

- 선박을 건조하는 개념에서 유래 벌크헤드 설계를 적용하면 배는 격벽이라는 완전히 격리된 수밀 구획으로 나뉨. 만약 선체에 구멍이 뚫리더라도 격벽으로 분리되어 있기 때문에 침수 구역을 제한하고 배 전체의 침수와 침몰 방지가 가능함
- 벌크헤드 패턴을 사용하면 원격 자원에 대한 호출을 자원별 스레드 풀로 분리하므로 특정 원격 자원의 호출이 느려져 전체 어플리케이션이 다운될 수 있는 위험을 줄일 수 있음
- 스레드 풀은 서비스를 위한 벌크헤드 역할을 하고 각 원격 자원은 분리되어 스레드 풀에 할당됨
- 한 서비스가 느리게 반응한다면 해당 서비스 호출을 위한 스레드풀은 포함되지만 격벽(스레드 풀)으로 구분된 다른 스레드들은 정상 작동함

## 클라이언트 회복성이 중요한 이유

여러개가 묶여있는 마이크로서비스에서 만약 한부분의 서비스가 매우 느리게 답할 경우 최초에 호출했던 서비스들의 스레드풀이 계속 중첩되고 결국엔 포화상태가 되어 전체 어플리케이션의 고장을 야기한다. 이런 경우 회로차단기를 구성했다면 느리게 답하는 한 부분의 특정 호출을 감지해서 스레드를 소진하지 않도록 빠르게 실패시키고 해당 원격자원을 사용하는 부분을 제거함으로써 그나마 나머지 엮이지 않은 부분까지는 제 기능을 하도록 구성할 수 있다. 

회로차단기로 타임아웃이 되었다면 회로 차단기는 발생한 실패 횟수를 추적하기 시작하고 특정 시간동안 특정 서비스 에러가 기대 이상 발생하면 회로 차단기는 회로를 차단시키고 다음 세가지 일이 일어난다

A에서 B를 호출하는것으로 가정하고 A와 B 사이에 회로차단기가 있다고 가정한다.

1. 서비스 A는 회로 차단기가 타임아웃하기 전에 문제가 있다는 것을 즉시 알아챈다.
2. 서비스 A는 완전히 실패하거나 대체 코드를(폴백) 사용하는 조치를 취하는 것 중에서 선택할 수있다.
3. 회로 차단기가 차단된 동안 서비스 A는 서비스 B를 호출하지 못하므로 서비스 B에 복구할 수 있는 여유가 생긴다. 이 기회를 이용해 서비스 B는 회복할 시간을 갖고 서비스 저하로 발생되는 연쇄 장애를 막을수 있다.

### 원격 호출 회로 차단 패턴의 핵심 기능

1. **빠른 실패**: 원격 서비스의 호출이 성능 저하를 겪으면 해당 호출은 빨리 실패함으로써 어플리케이션 전체를 다운시킬 수 있는 자원 고갈 이슈를 방지함. 대부분의 장애 상황에서 완전히 다운되는 것 보다 부분 다운되는게 더 나음
2. **원만한 실패**: 타임아웃과 빠른 실패 방법을 사용하는 회로 차단기 패턴으로 어플리케이션 개발자는 원만하게 실패하거나 사용 의도로 수행하는 대체 머커니즘을 찾을 수 있음(폴백같음)
3. **원활한 회복**: 회로차단기 패턴을 통해 요청 자원이 정상인지 주기적으로 확인하고 사랍의 개입 없이 자원 접근을 다시 허용 가능

## 히스트릭스

위에서 언급한 폴백, 회로차단기, 벌크헤드 패턴을 직접 구현하는건 정말정말 어려운일이지만 다행히도 넷플릭스가 마이크로서비스를 직접 사용하면서 실전에서 검증한 라이브러리인 히스트릭스를 제공함

## 히스트릭스를 사용한 회로 차단기 구현

@EnableCircuitBreak를 부트스트랩클래스에 추가해야함

히스트릭스는 두가지의 경우로 사용 가능함 

1. 서비스와 데이터베이스
2. 서비스와 서비스

@HystrixCommand 어노테이션을 사용해 히스트릭스 회로 차단기가 관리하는 자바 클래스 메서드라고 표시함 스프링이 @HystrixCommand을 만나면 메서드를 감싸는 프ㅡ록시를 동적으로 생성하고 원격 호출을 처리하기 위해 확보한 스레드가 있는 스레드풀로 해당 메서드에 대한 모든 호출을 관리함

### @HystrixCommand 사용자 설정

commandProperties 속성을 사용해서 사용자 정의 설정을 함

```java
 @HystrixCommand(
   commandProperties={
         @HystrixProperty(name="circuitBreaker.requestVolumeThreshold", value="10"),
         @HystrixProperty(name="circuitBreaker.errorThresholdPercentage", value="75"),
         @HystrixProperty(name="circuitBreaker.sleepWindowInMilliseconds", value="7000"),
         @HystrixProperty(name="metrics.rollingStats.timeInMilliseconds", value="15000"),
         @HystrixProperty(name="metrics.rollingStats.numBuckets", value="5")}
```

- **metrics.rollingStats.timeInMilliseconds**: 회로 차단기의 타임아웃 설정

분산 환경에서 너무 느리게 실행되는 서비스 문제에 대해 절대로 해결 할 수 없는 일이 아닌 경우 이 설정을 늘리는 유혹을 최대한 피해야한다. 느리게 실행되는 서비스는 대부분 성능 문제가 있다.

## 폴백 프로세싱

회로 차단기 패턴의 장점은 서비스 호출에 실패했을때 이 실패를 가로채고 다은 대안을 선택할 수 있는 폴백 프로세싱을 사용할 수 있다는 점이다.

```java
@HystrixCommand(fallbackMethod = "buildFallbackLicenseList")
```

- **fallbackMethod**: 회로차단기가 작동될때 동작하는 메서드 이름을 지정 이 폴백 메서드는 @HystrixCommand를 사용한 클래스와 같이 있어야함



### 폴백 전략 사용시 주의사항

폴백 전략 사용 여부를 결정할 때 중요한것은 고객의 데이터 수명의 허용 정도와 문제가 있는 어플리케이션을 감추는 것이 얼마나 중요한지에 달려있다.

1. 폴백을 사용해서 타임아웃 예외를 잡아내고 로깅같은 처리만한다면 try~catch 만으로도 충분하다
2. 폴백메서드 역시 @HystrixCommand로 감싸질 수 있다. 폴백에서 또 다른 원격자원을 호출하는 경우에도 회로차단기가 동작할 수 있음을 명심하자.



## 벌크헤드 패턴

기본적인 자바의 서비스 호출 행위는 전체 자바 컨테이너에 대한 요청을 처리하는 스레드에서 이루어진다. 대규모 환경에서 한 서비스에서 발생한 성능 문제는 전체 자바 컨테이너 스레드 풀에게 영향을 줄 수 있다. 과부하게 계속되면 자바 컨테이너는 비정상정으로 종료할 것이다. 벌크헤드 패턴은 원격 자원 호출을 자신의 스레드 풀에 격리하므로 오작동 서비스를 억제하고 컨테이너의 비정상 종료를 방지한다.

히스트릭스는 스레드 풀을 사용해 원격 서비스에 대한 모든 요청을 위임하는데 이 스레드 풀은 REST 요청이라던지 데이터베이스요청같은 원격 서비스 호출을 처리할 10개의 스레드가 있는 스레드 풀이 존재한다. 만약 히스트릭스 스레드풀에서 어떤 원격 서비스 호출이 굉장히 오래걸리는 작업이있다면 이 스레드풀 역시 고갈되어 어플리케이션 전체에 악영향을 끼치게 될 것이다.

다행히도 히스트릭스는 서로 다른 원격 자원 호출 간에 벌크헤드를 생성하기에 용이한 메커니즘을 제공한다.

```java
@HystrixCommand(
        threadPoolKey = "licenseByOrgThreadPool",
        threadPoolProperties =
                {@HystrixProperty(name = "coreSize",value="30"),
                 @HystrixProperty(name="maxQueueSize", value="10")}
)
```

- **threadPoolKey**: 새로운 스레드풀을 설정하도록 key값 부여
- **threadPoolProperties**: 스레드풀 사용자 정의
- **coreSize**: 스레드 풀 크기를 설정 기본값 10
- **maxQueueSize**: 스레드들이 분주할때 스레드 풀의 앞 단에 백업할 큐를 value 값 만큼 만들어서 요청 수가 큐 크기를 초과하면 큐에 여유가 생길때 까지 스레드 풀에 대한 추가 요청은 모두 실패한다. 기본값 -1, -1일때는 큐를 사용하지 않는다

이 maxQueueSize 속성을 사용하는데 앞서 두가지 주의사항이 있다

1. maxQueueSize을 -1로 설정하면 유입된 호출을 유지하는데 자바의 SynchronousQueue가 사용된다 동기식 큐를 사용하면 본질적으로 스레드 풀에서 가용한 스레드 개수보다 더 많은 요청을 처리할 수 없다. 만약 1보다 큰 값으로 설정하면 히스트릭스는 자바의 LinkedBlockingQueue를 사용한다 LinkedBlockingQueue를 사용하면 모든 스레드가 요청을 처리하는데 분주하더라도 더 많은 요청을 큐에 넣을 수 있다.
2. maxQueueSize 속성은 스레드 풀이 처음 초기화 될 때만 설정할 수 있다. **queueSizeRejectionThreshold** 속성을 사용하면 히스트릭스는 큐 크기를 동적으로 변경할 수 있지만 maxQueueSize 가 0 이상일때만 가능하다

**스레드 풀의 적정 크기 공식은 다음과 같다 (넷플릭스에서 제공한 공식)**

**(서비스가 정상일 때 최고점에서 초당 요청 수 * 99 백분위 수 지연 시간(단위 : 초)) + 오버헤드를 대비한 소량의 추가 스레드**

만약 원격 자원 서비스는 정상인데 서비스 호출에 타임아웃이 걸린다면 스레드 풀을 조정해야한다.



## 히스트릭스 세부 설정

히스트릭스의 회로차단기 패턴은 호출 실패 횟수를 모니터링해서 필요 이상으로 실패할 때 원격 자원에 도달하기 전에 호출을 실패시켜 서비스로 들어오는 이후 호출을 자동으로 차단한다. 이렇게 하는 이유는 두가지다

1. 원격 자원에 성능 문제가 있는 경우 빨리 실패하게 하면 호출 어플리케이션이 호출 타임아웃을 기다릭는 것을 막아 호출 어플리케이션이나 서비스의 자원 고갈 문제와 비정상 종료될 위험을 크게 낮춘다.
2. 서비스 클라이언트의 빠른 실패로 호출을 막으면 힘겨워하는 서비스가 부하를 견디고 부하 때문에 완전히 비정상 종료되지 않는다.

### 히스트릭스의 회로차단기 차단 과정과 회복 과정

1. 문제 발생
2. 최소 요청수가 실패했는가? Yes
3. 에러 임계치에 도달했는가? Yes
4. 회로 차단기 차단
5. 원격 서비스 호출에 여전히 문제가 발생하는가? Yes 4번으로

No의 경우는 문제가 발생하지 않기 때문에 정상적인 호출이 이루어진다.

- 1~3 까지의 과정은 기본값으로 10초동안 판단하도록 되어있고 이 값은 설정 가능함
- 4~5 까지의 과정은 기본값으로 5초동안 판단하도록 되어있고 이 값 역시 설정 가능함
- 1~3 까지의 과정에서 10초동안 호출 회수를 추적한다. 만약 10초안에 지정해놓은 호출회수를 초과한다면 히스트릭스는 전체 실패 비율을 조사하기 시작한다. 
- 전체 실패 비율 기본값은 50%이고 이 값은 설정이 가능하다. 만약 이 값의 임계치를 넘긴다면 이후 요청하는 모든 호출은 실패한다.
- 만약 원격 호출이 10초동안 지정해놓은 횟수만큼 도달하지 않는다면 히스트릭스는 10초 뒤 호출 회수를 초기화시킨다.
- 만약 임계치를 넘겨서 회로차단기가 동작한다면 5번을 통해서 원격 서비스 호출이 정상적인지를 확인한다. 기본값은 5초로 설정되어 있고 이것 역시 설정 가능하다.

### 히스트릭스 설정

```java
@HystrixCommand(//fallbackMethod = "buildFallbackLicenseList",
        threadPoolKey = "licenseByOrgThreadPool",
        threadPoolProperties =
                {@HystrixProperty(name = "coreSize",value="30"),
                 @HystrixProperty(name="maxQueueSize", value="10")},
        commandProperties={
                 @HystrixProperty(name="circuitBreaker.requestVolumeThreshold", value="10"),
                 @HystrixProperty(name="circuitBreaker.errorThresholdPercentage", value="75"),
                 @HystrixProperty(name="circuitBreaker.sleepWindowInMilliseconds", value="7000"),
                 @HystrixProperty(name="metrics.rollingStats.timeInMilliseconds", value="15000"),
                 @HystrixProperty(name="metrics.rollingStats.numBuckets", value="5")}
)
```

- **circuitBreaker.requestVolumeThreshold**: 히스트릭스가 호출 차단을 고려하는데 필요한 시간 ex10초 시간대 동안 연속 호출 개수를 제어함 기본값 20
- **circuitBreaker.errorThresholdPercentage**: 히스트릭스가 호출 차단을 고려하고 연속 호출 개수가 지정한 수보다 높을때 전체 실패 비율 계산 기본값 50
- **circuitBreaker.sleepWindowInMilliseconds**: 히스트릭스가 서비스의 회복 상태를 확인할때까지 대기하는 시간 기본값 5초
- **metrics.rollingStats.timeInMilliseconds**: 히스트릭스가 서비스 호출을 모니터링하는 시간 기본값은 10초 
- **metrics.rollingStats.numBuckets**: 설정한 시간 간격 동안 통계를 수집할 횟수를 설정. 히스트릭스는 해당 시간 동안 버킷에 측정 지표를 수집하고 통계를 바탕으로 원격 자원 호출의 실패 여부를 결정한다. 설정한 버킷수는 **metrics.rollingStats.timeInMilliseconds** 값으로 균등하게 분할되어 위의 예로는 15초의 간격동안 3초간격으로 5개의 버킷에 통계 데이터를 수집한다.

확인할 통계 간격이 작고 유지하는 버킷수가 많은수록 대량 서비스에서 CPU및 메모리 사용량이 증가한다. 세분화된 버킷을 설정하고 싶은 유혹을 이겨내야한다.



### 히스트릭스 구성 재 검토

히스트릭스는 구성 기능이 뛰어나서 회로차단기, 폴백, 벌크헤드 패턴의 구현을 정밀하게 할 수 있다.

히스트릭스 환경을 구성할 때는 히스트릭스 세가지 구성 레벨을 꼭 기억해야한다.

- 어플리케이션 기본값
- 클래스 기본값
- 클래스 안에서 정의된 스레드 풀 레벨

<br>

- 클래스단위로 묶을때는 @DefaultProperties라는 어노테이션을 사용한다.

```java
@DefaultProperties(
  commandProperties = {
    @HystrixProperty(
      name="execution.isolation.thread.timeoㅕtInMilliseconds", value="10000"
    )
  }
)
class MyService{...}
```

MyService 안에 있는 모든 원격 자원 호출 모니터링 시간을 10초로 지정

히스트릭스 세부설정을 자주 바꾸어주어야한다면 Spring Cloud Config Server를 사용하는 것을 추천한다. 이걸 사용하면 어플리케이션의 인스턴스만 재시작해주면 된다.

## 스레드 컨텍스트와 히스트릭스

@HystrixCommand가 실행될 때 Thread 와 Semaphore라는 두가지 다른 격리 전략을 수행할 수 있다. 기본적으로 히스트릭스는 Thread 격리 전략을 수행한다. 호출을 시도한 부모 스레드와 컨텍스트를 공유하지 않는 격리된 스레드 풀에서 수행되는데 이런 전략은 히스트릭스가 자기 통제 하에서 원래 호출을 시도한 부모 스레드와 연관된 어떤 활동도 방해하지 않고 스레드 실행을 중단할 수 있다는 것을 의미한다.

Semaphore 기반 격리를 사용하면 히스트릭스는 새로운 스레드를 시작하지 않고 @HystrixCommand 어노테이션이 보호하는 분산 호출을 관리하며 타임아웃이 발생하면 부모 스레드를 중단시킨다.

톰캣처럼 동기식 컨테이너 서버 환경에서 부모 스레드를 중단하면 개발자가 예외 처리를 할 수 없는 예외가 발생한다. 이처럼 발생한 예외를 처리할 수 없거나 자원 정리 및 에러 처리를 수행할 수 없다면 개발자가 코드를 작성할 때 예기하지 않은 결과를 발생시킬 수 있다.

```javascript
@HystrixCommand(
  commandProperties = {
    @HystrixProperty(
      name="execution.isolation.strategy", value="SEMAPHORE"
    )
  }
)
```

기본적으로 Thread 전략을 추천하고 Thread 전략이 Semaphore전략보다 격리 수준이 더 높다 Semaphore는 Netty같은 비동기 I/O 컨테이너를 적용할때 고려해야한다.

### 





















