

# 스프링 마이크로서비스 코딩 공작소



스프링 마이크로서비스 코딩 공작소 책 내용 및 정리한 글



<br>



## MSA란 ?

- 2014년 즈음부터 시작한 마이크로서비스는 느슨히 결합된 작은 분산서비스
- 대형 어플리케이션을 관리하기 쉽고, 제한된 책임을 담받하는 컴포넌트로 분해할 수 있음
- 코드 베이스를 명확히 정의한 작은 조각으로 분리해서 대형 코드 베이스에서 발생하는 전통적인 복잡성 문제를 해결 가능함
- 어플리케이션 기능을 분해하고 분리해서 완전히 상호 독립적으로 구성해야함
- 어플리케이션 로직을 각자 책임이 명확한 작은 컴포넌트로 분해하고 이들을 조합해서 솔루션을 제공함

<br>

## MSA vs Monolithic

- 유연성 : MSA는 새로운 기능을 신속하게 제공하도록 분리된 서비스를 구성하고 재배치할 수 있음. 결합도를 낮추기때문에 코드단위, 복잡성 등이 낮아 배포 및 테스트 하는 시간도 줄음.
- 회복성: Monolithic에서의 장애는 전체 시스템 장애를 야기시키나 MSA의 경우 어플리케이션의 작은 부분에 국한되어 어플리케이션의 전체 장애로 확대되지  않음.
- 확장성: 분리된 서비스를 여러 서버에 수평벅으로 쉽게 분산할 수 있음. 기능 및 서비스를 적절히 확장 가능. 어플리케이션의 모든 로직이 얽혀 있는 Monolithic 어플리케이션은 한 부분이 병목점이 되더라도 전체를 확장해야함.



**MSA = 작고 단순하며 분리된 서비스 = 확장 가능하고 회복적이며 유연한 어플리케이션**

<br>

## MSA를 구축할때 고려해야할 점

- 적정 크기: 마이크로서비스가 과도한 책임을 맡지 않도록 어떻게 적절한 크기로 만들 수 있는가?
- 위치투명성: 마이크로서비스 어플리케이션에서 서비스의 다수 인스턴스가 재빨리 시작하고 종료될 때 서비스 호출에 대한 물리적 상세 정보를 관리할 수 있는 방법은 무엇인가?
- 회복성: 장애가 발생한 서비스를 우회하고 '빨리 실패'하는 방법을 사용해 어떻게 사용자를 보호하고 어플리케이션의 전반적 무결성을 유지할 것인가?
- 반복성: 새로운 서비스 인스턴스가 시작할 떄마다 운영 환경의 다른 서비스 인스턴스 구성과 코드베이스를 동일하게 만드는 방법은 무엇인가?
- 확장성: 비동기 프로세싱과 이벤트를 사용해 서비스 간 의존성을 최소화하고 마이크로서비스를 원만하게 확장할 수 있는 방법은 무엇인가?



<br>

## MSA 개발 패턴



### 1. 핵심 개발 패턴(core development patterns)

- 서비스 세분성: 비즈니스 영역을 마이크로 서비스로 분해해서 각 마이크로서비스가 적정 수준의 책임을 갖게하는 방법은 무엇인가? 서비스가 다른 비즈니스 문제 영역과 책임이 겹치도록 넘 굵게 나뉘면 시간이 지나면서 유지보수와 변경이 어려워진다. 서비스가 너무 잘게 나뉘면 어플리케이션의 전반적 복잡성이 증가되고 서비스는 데이터 저장소에 액세스하는 것 외에 아무런 로직도 없는 멍청한 데이터 추상화 계층으로 정락한다.
- 통신프로토콜: 개발자가 여러분의 서비스와 어떻게 통신하는가? 마이크로서비스와 데이터 교환을 위해 XML 이나 JSON 등 ..
- 인터페이스설계 : 개발자가 서비스 호출에 사용하는 실제 서비스 인터페이스를 설계하는 최선의 방법은 무엇인가? 서비스 의도를 전달하도록 서비스 url을 구조화하는 방법은 무엇인가? 서비스 버전관리는? 잘 설계된 마이크로서비스 인터페이스가 있다면 서비스를 직관적으로 사용할 수 있다.
- 서비스 간 이벤트 프로세싱: 서비스 간 하드 코딩된 의존성을 최소화하고 어플리케이션 회복성을 높이기 위해 마이크로 서비스를 분리하는 방법은 무엇인가?



### 2. 라우팅 패턴(routing patterns)

- 서비스 디스커버리: 클라이언트 어플리케이션이 서비스 위치를 하드 코딩하지 않고 찾을 수 있도록 마이크로서비스를 어떻게 탐색 가능하게 만들 수 있을까? 오작동하는 마이크로서비스 인스턴스를 가용 서비스 인스턴스 풀에서 어떻게 제거해야 할까?
- 서비스 라우팅: 보안 정책과 라우팅 규칙을 마이크로서비스 어플리케이션의 여러 서비스와 서비스 인스턴스에 차별 없이 적용하기 위해 모든 서비스에 대한 단일 진입점을 제공하는 방법은 무엇인가? 팀의 각 개발자가 서비스에 라우팅을 제공하기 위한 솔루션을 자체적으로 마련할 필요가 없도록 하는 방법은 무엇인가? 



### 3. 클라이언트 회복성 패턴(client resiliency patterns)

- 클라이언트 측 부하 분산: 카이크로서비스의 여러 인스턴스에 대한 호출이 정상 인스턴스에 분산하도록 서비스 인스턴스의 위치를 캐싱하는 방법은 무엇인가?

- 회로 차단기 패턴: 클라이언트가 고장 나거나 성능 문제가 있는 서비스를 계속 호출하지 않게 하는 방법은 무엇인가? 서비스가 느리게 실행되면 서비스를 호출하는 클라이언트의 리소스가 소모된다. 비정상 마이크로서비스 호출이 빨리 실패하면 호출 클라이언트가 신속하게 응답하고 적절한 조취를 취할수 있다.
- 폴백 패턴: 서비스 호출이 실패할 때 호출한 마이크로서비스가 아닌 다른 대체 수단을 사용해 서비스 클라이언트가 작업을 수행할 수 있게 하는 플러그인 매커니즘을 어떻게 제공할 것인가? 
- 벌크헤드 패턴: 마이크로서비스 어플리케이션은 작업을 수행하기 위해 여러 분산 자원을 사용한다. 오작동하는 서비스 호출 하나가 나머지 어플리케이션에 부정적인 영향을 미치지 않도록 이러한 호출을 구분하는 방법은 무엇인가? 

### 4. 보안 패턴(seucirty patterns)

- 인증: 서비스를 호출하는 서비스 클라이언트가 자신이라는 것을 어떻게 알 수 있는가?
- 인가: 마이크로서비스를 호출하는 서비스 클라이언트가 수행하려는 작업을 수행할 자격이 있는지 어떻게 알 수 있는가?
- 자격증명관리와 전파: 서비스 클라이언트가 한 트랜잭션과 관련된 여러 서비스 호출에서 자격증명을 항상 제시하지 않아도 될 방법은 무엇인가? 특히 OAuth와 JWT 같은 토큰 기반 보안 표준을 사용해 토큰을 얻는 방법 등 ..

### 5. 로깅 및 추적 패턴(logging and tracing patterns)

- 로그 상관관계: 단일 트랜잭션에 대해 여러 서비스 간 생성된 모든 로그를 함께 연결하는 방법은 무엇인가? 이 패턴을 이용해 상관관계 ID를 구현하는 방법을 살펴볼 것이다.
- 로그 수집: 이 패턴으로 서비스에서 생성된 모든 로그의 질의 가능한 단일 데이터베이스로 취합하는 방법을 살펴볼 것이다.
- 마이크로서비스 추적: 끝으로 트랜잭션과 연관된 모든 서비스에서 클라이언트 트랜잭션 흐름을 시각화하고 트랜잭션과 관련된 서비스의 성능 특성을 이해하는 방법을 살펴볼 것이다.



### 6. 빌드 및 배포패턴(build a deployment patterns)

- 빌드 및 배포 파이프라인: 조직의 모든 환경에서 원클릭 빌드와 배포를 강조하는 반복 가능한 빌드 및 배포 프로세스를 어떻게 만드는가?
- 코드형 인프라스트럭처: 소스 제어를 사용해 실행하고 관리할 수 있는 코드로 서비스 프로비저닝을 처리하는 방법은 무엇인가?
- 불변 서버: 마이크로서비스 이미지를 생성하고 배포한 후 변경하지 못하게 하려면 어떻게 해야하는가?
- 피닉스 서버: 서버가 오래 실행될수록 구성 편차가 발생할 가능성도 높아진다. 마이크로서비스를 실행하는 서버를 정기적으로 해체하고 불변 이미지를 재생성하려면 어떻게 해야 하는가?



<br>



## @EnableCircuitBreaker

스프링 마이크로서비스에 이 어플리케이션에서 넷플릭스 히스트릭스 라이브러리가 사용된다고 알려준다. 



## @EnableEurekaClient

@EnableEurekaClient 어노테이션은 마이크로서비스 자신을 유레카 서비스 디스커버리 에이전트에 등록하고 서비스 디스커버리를 사용해 코드에서 원격 REST 서비스의 엔드포인트를 검색할 것을 지정한다. 

Spring 의 기본 RestTemplate가 아니라 **수정된 RestTemplate**을 사용하도록 지정한다. 이 RestTemplate 클래스로 호출하려는 서비스의 논리적 서비스 ID를 전달할 수 있다.



```
ResponseEntity<Strping> restExchange = restTemplate.exchange(http://logical-service-id/name/{firstName}/{lastName})
```



내부적으로 RestTemplate는 유레카 서비스에 접속해 개 이상의 logical-service-id 서비스 인스턴스에 대한 물리적인 위치를 검색한다.



## @HystrixCommand

1. 이 어노테이션이 붙어있는 메서드는 메서드가 호출될 때 직접 호출되지 않고 히스트릭스가 관리하는 스레드 풀에 위임한다. 호출이 너무 오래걸리면(기본값 1초) 히스트릭스가 개입하고 호출을 중단시킨다. 이것이 바로 **회로차단기 패턴**의 구현이다. 
2. 히스트릭스가 관리하는 helloThreadPool 이라는 스레드 풀을 만드는것이다. 이 메서드에 대한 모든 호출은 이 스레드 풀에서만 발생하며 수행중인 다른 원격 서비스 호출과 격리된다.

<br>



## 비지니스 문제를 인식하고 마이크로 서비스 후보로 분해하기

1. 비지니스 문제를 기술하고 그 문제를 기술하는데 사용된 명사에 주목하라

문제를 기술하는 데 동일한 명사가 반복해서 사용되면 대개 핵심 비지니스 영역과 마이크로서비스로 만들 기회가 드러난다.

2. 동사에 주목하라

동사는 행위를 부각하고 문제가 되는 영역의 윤과을 자연스럽게 드러낸다.

3. 데이터 응집성을 찾아라

비지니스 문제를 각 부분으로 분해할 때 서로 연관성이 높은 데이터 부분들을 찾는다. 마이크로 서비스는 자기 데이터를 완전히 소유해야 한다.

<br>

## MSA 세분화시 고려사항

1. 큰 마이크로서비스에서 시작해 작게 리팩토링하는 것이 더 낫다

마이크로 서비스의 여정을 시작할 때 의욕이 지나쳐 모든 것을 마이크로서비스로 만들어 버리기 쉽다. 하지만 문제 영역을 한 번에 자은 서비스들로 분해하는 것은 마이크로서비스가 그저 단순한 데이터 서비스로 전락하기 때문에 너무 일찍 복잡함을 겪게 된다.

2. 서비스 간 교류하는 방식에 먼저 집중한다

이는 문제 영역에 대한 큰 단위의 인터페이스를 맏느는데 도움이 된다. 큰 것을 작게 리팩토링하는 것이 더 쉽다.

3. 문제 영역에 대한 이해가 깊어짐에 따라 서비스 책임도 계속 변한다.

새로운 어플리케이션 기능이 요구될 때 종종 마이크로서비스가 책임을 맡는다. 마이크로서비스는 단일 서비스에서 시작해 여러 서비스로 분화되며 성장하는데, 원래 서비스는 새로운 서비스들을 오케스트레이션하고 어플리케이션의 다른 부분에서 새 서비스들의 기능을 캡슐화한다.

<br>

## 안좋은 마이크로서비스의 징후



### 너무 큰 단위의 마이크로 서비스

- 책임이 너무 많은 서비스: 이 서비스에서 비즈니스 로직의 일반 흐름은 복잡하며 지나치게 다양한 종류의 비지니스 규칙을 시행하게 된다.
- 많은 테이블의 데이터를 관리하는 서비스: 마이크로서비스는 자기가 관리하는 데이터를 기록하는 시스템이다. 여러 테이블에 데티러르 저장하거나 직속 데이터베이스 외부의 테이블에 액세스하고 있다면 서비스가 너무 크다는 것을 암시한다. 이 책의 저자는 3~5개 이내의 테이블을 소유해야 한다는 지침을 세웠다 이보다 더 많다먼 서비스가 너무 많은 책임을 담당할 가능성이 높다 ... 와우
- 과다한 테스트 케이스: 시간이 지나면서 서비스와 크기와 책임이 늘어날 수 있다. 서비스가 적은 수의 테스트 케이스로 시작해 수백개의 단위 테스트와 통합 테스트 케이스로 늘어난다면 리팩토링이 필요할 것이다.



### 너무 작은 단위의 마이크로 서비스

- 한 문제 영역 부분에 속한 마이크로서비스가 토끼처럼 번식한다: 몯느 것이 마이크로서비스로 되면 작업 수행에 필요한 서비스 개수가 엄청나게 증가해서 서비스에서 비지니스 로직을 만드는 것이 복잡하고 어려워진다. 어플리케이션에 수십개의 마이크로서비스가 있고 각 서비스가 하나의 데이터베이스 테이블과 통신할 때 악취가 나곤 한다.
- 마이크로서비스가 지나치게 상호 의존적이다: 문제 영역의 한 부분에 이쓴ㄴ 마이크로서비스는 하나의 사용자 요청을 완료하기 위해 각 서비스가 서로 계속 호출한다.
- 마이크로서비스가 단순한 CRUD 집합이 된다: 마이크로서비스는 비지니스 로직의 표현이지 데이터 소스의 추상화 계층이 아니다. 마이크로서비스가 CRUD 관련 로직만 수행한다면 너무 잘게 나뉘어 있다는 의미다.



마이크로 서비스 아키텍쳐는 처음부터 올바르게 설계하기가 어렵기때문에 진화론적 사고 과정으로 개발해야한다.위에서 언급했듯이 **큰 것에서 작은 것 으로 나누기**의 좋은 예다. 

<br>



## 서비스 인터페이스 지침

개발자가 1~2개의 마이크로 서비스를 익혔다면 모든 서비스에 대한 동작 규칙을 습들할 수 있어야한다

- REST 철학을 수용하라: 서비스에 대한 REST 방식은 서비스 호출 프로토콜로 HTTP를 수용하고 HTTP 동사를 사용하는 것이 핵심이다. HTTP 동사를 기반으로 기본 행동 양식을 모델린하다.
- URI를 사용해 의도를 전달하라: 서비스의 엔드포인트로 사용되는 URI는 문제 영역에 존재하는 다양한 자원을 기술하고 자원 관계에 대한 기본 메커니즘을 제공해야 한다.
- 요청과 응답에 JSON을 사용하라
- HTTP 상태 코드로 결과를 전달하라: HTTP 프로토콜에는 서비스의 성광과 실패를 명시하는 풍부한 표준 응다봌드가 있다. 상태코드를 익히고 모든 서비스에 일관되게 사용하는 것이 매우 중요하다.



## 마이크로서비스를 사용하지 않아야 할 때

### 1.분산 시스템 구축의 복잡성

마이크로 서비스는 모놀리틱에 없던 복잡성을 가져온다. 높은 수준의 운영 성숙도도 필요로 하기 떄문에 고도로 분산된 어플리케이션의 성공에 필요한 자동화와 운영 작업에 투자할 의사가 없는 조직이라면 마이크로 서비스를 고려하지 않는 것이 좋다.

### 2. 서버 스프롤(server sprawl)

마이크로서비스의 유연성은 모든 서버를 운영하는데 드는 비용과 함께 따져 보아야 한다. 스프롤은 활용도가 낮은 여러 서버가 실제 작업량보다 더 많은 공간과 리소르를 차지하는 현상을 의미한다.

### 3. 어플리케이션 유형

마이크로서비스는 재사용성을 추구하며 높은 회복성과 확장성이 필요한 대규모 어플리케이션의 구축에 매우 유용하다. 소형 어플리케이션이나 소수 사용자를 위한 어플리케이션을 개발할 때 마이크로서비스와 같은 분산 모델로 구축한다면 배보다 배꼽이 더 클 수도 있다.

### 4. 데이터 변환과 일관성

서비스의 데이터 사용 패턴과 서비스 소비자가 어떻게 서비스를 사용하는지 고민해야한다. 마이크로서비스는 적은 수의 테이블을 둘러싸고 추상화하며 저장소에 단순한 질의 생성, 추가, 실행 등 '운영상의' 작업을 수행하는 메커니즘으로도 잘 동작한다. 어플리케이션이 여러 데이터소스에서 복잡한 데이터를 취합하고 변환해야 할 경우 마이크로서비스의 분산된 특성때문에 작업이 어려워진다. 마이크로 서비스에는 서비스 사이에 트랜잭션을 처리하는 표준이 없기때문에 필요하다면 직접 만들어야한다.  메시징을 사용해서 통신 가능하지만 메시징에는 지연시간이 발생하기때문에 어플리케이션은 최종 일관성을 유지해아한다.

<br>

## 마이크로서비스에서 JSON을 사용하는 이유

1. xml 기반의 SOAP와 같은 프로토콜과 비교할 때 JSON은 적은 텍스트로 데이트 표현이 가능하다
2. JSON은 가독성이 높고 사용하기 쉽다. 
3. JSON은 javscript에서 사용하는 기본 직렬화 프로토콜이다.

<br>

## 엔드포인트 작명 지침

1. **서비스가 제공하는 리소스를 알 수 있는 명확한 URL이름을 사용하라**: URL을 정의하는데 표준 형식을 사용하면 API의 직관성과 사용 편의성이 향상된다. 일관된 명명 규칙을 사용한다.
2. **리소스 간 관계를 알 수 있는 URL을 사용하라**: 마이크로서비스들이 가진 리소스 사이에서 부모 자식 관계가 생기는데, 외부에 부모 리소스 컨텍스트는 존재하지만 자식 리소스는 외부에 노출되지 않는 관계가 있다. 이러한 관계는 URL을 사용해 표현한다. 하지만 RUL이 지나치게 길어지거나 중첩되는 경향이 있다면 해당 마이크로서비스가 너무 많은 일을 하려는 것일지도 모른다.
3. **URL 버전 체계를 일찍 세워라**: RUL과 엔드포인트는 서비스 소유자와 서비스 소비자 간의 계약을 의미한다. 일반적 패턴 중 하나는 모든 엔드포인트 앞에 버전 번호를 붙이는 것이다. 일찍 버전 체계를 갖추고 준수하자. 소비자가 URL을 사용한 후 URL 버전 체계를 개량하는 것은 매우 어려운 일이다

<br>

## Twelve-Factor 마이크로서비스 어플리케이션 구축 지침

1. **코드베이스**: 모든 어플리케이션 코드와 서버 프로비저닝 정보는 버전관리 되어야한다. 각 마이크로 서비스는 소스 제어 시스템 안에 독립적인 코드 저장소를 가져야한다.
2. **의존성**: 어플리케이션이 사용하는 의존성을 메이븐 같은 빌드 도구를 이용해 명시적으로 선언해야 한다. 제3자의 jar의존성은 특정 버전 번호를 붙여 명시해 선언해야 한다. 따라서 동일 버전의 라이브러리를 사용해 항상 마이크로서비스를 빌드할 수 있다.
3. **구성**: 어플리케이션 구성(특히 환경별 구성)을 코드와 독립적으로 저장하자. 어플리케이션 구성운 절대로 소스코드와 동일한 저장소에 있으면 안된다.
4. **백엔드 서비스**: 마이크로서비스는 대게 네트워크를 거쳐 데이터베이스나 메시징 서비스와 통신한다. 그렇다면 언제든 데이터베이스 구현을 자체 관리형 서비스에서 외부업체 서비스로 교체할 수 있어야 한다.
5. **빌드, 릴리즈, 실행**: 배포할 어플리케이션의 빌드, 릴리즈, 실행 부분을 철처하게 분리하라. 코드가 빌드되면 개발자는 실행중에 코드를 변경할 수 없다. 모든 변경사항을 빌드 프로세스로 되돌려 재배포해야한다. 빌드된 서비스는 불변적이므로 변경할 수 없다.
6. **프로세스**: 마이크로서비스는 항상 무상태방식을 사용해야 한다. 서비스 인스턴스 손실에 의해 데이터가 손실될 것이라는 우려 없이 언제든 서비스를 강제 종료하거나 교체할 수 있다.
7. **포트 바인딩**: 마이크로서비스는 서비스용 런타임 엔진을 포함한(실행 파일에 패키징된 서비스를 포함한) 완전히 자체 완비형이다. 별도의 웹 또는 어플리케이션 서버 없이도 서비스는 실행되어야 한다. 서비스는 명령행에서 단독으로 시작하고 노출한 http포트를 통해 즉시 액세스 할 수 있어야 한다.
8. **동시성**: 확장해야 한다면 단일 서비스 안에서 스레드 모델에 의존하지 마라. 그 대신 더 많은 마이크로서비스를 시작하고 수평 확장하라. 마이크로서비스 안에서 스레드 사용을 배제하지는 않지만 확장을 위한 유일한 메커니즘으로 믿지 말라. 수직 대신 수평 확장하라
9. **폐기 가능**: 마이크로서비스는 폐기 가능하므로 요구에 따라 시작 및 중지할 수 있다. 시작 시간은 최소화하고 운영 체제에서 강제 종료 신호를 받으면 프로세스는 적절히 종료해야한다.
10. **개발 및 운영 환경 일치**: 서비스가 실행되는 모든 환경(개발자의 데스크톱 환경도 포함) 사이의 차이를 최소화하라 개발자는 서비스 개발을 위해 실제 서비스가 실행되는 동일한 인프라스트럭처를 로컬에 사용해야 한다. 이는 환경 간 서비스 배포가 수 주가 아닌 수 시간 안에 이루어져야 한다는 것을 의미한다. 코드가 커밋되자마자 테스트가 되고 가능한 신속하게 개발 환경에서 운영 환경으로 전파되어야 한다.
11. **로그**: 로그는 이벤트 스트림이다. 로그가 기록될 때 Splunk, Fluentd 같은 도구로 로그가 스트리밍되어야 한다. 이들 도구는 로그를 수집해 중앙에 기록한다. 마이크로서비스는 이러한 로깅 동작방식에 신경쓰지 않아야 하며, 개발자는 표준 출력으로 출력된 로그를 시각적으로 확인할 수 있어야 한다.
12. **관리 프로세스**: 개발자는 종종 담당 서비스에 대해 데이터 마ㅇ그레이션이나 변환처럼 관리 작업을 수행해야 한다. 이러한 작업은 임의로 수정되면 안되고 소스 코드 저장소에 유지 및 관리되는 스크립트에 의해 수행되어야 한다. 이 스크립트는 실행될 각 환경에 반복적으로 수행 가능하고 환경을 위해 변경되지 않아야 한다. 즉, 각 환경에 맞추어 스크립트를 수정하지 않는다.

<br>

## 마이크로서비스 역할 관점

1. **아키텍트**: 비지니스 문제의 실제 윤곽을 잡는다. 비지니스 문제 영역을 기술하고 이야기되는 시토리를 경청하고, 출현할 마이크로서비스 후보에 주시하자. 처음부터 잘게 나뉜 많은 서비스에서 시작하는 것보다 굵게 나뉜 마이크로서비스에서 시작해서 작은 서비스로 리팩토링하면 더 낫다는 것도 기억하자. 대부분 좋은 아키텍처와 마찬가지로 마이크로서비스 아키텍처도 창발적이며 사전에 세세히 계획되는 것은 아니다.

2. **소프트웨어 엔지니어**: 서비스가 작다는 사실이 좋은 설계 원칙을 포기하라는 것은 아니다. 서비스안의 각 계층이 개별 책임을 맡는 계층적 서비스를 구축하는데 집중한다. 코드 내 프레임워크를 만들려는 유호긍ㄹ 피하고 완전히 독립적인 마이크로서비스를 지향한다. 미슥한 프레임워크설계와 도입은 어플리케이션의 수명 주기 후반에 막대한 유지 보수 비용을 초래할 수 있다.
3. **데브옵스 엔지니어**: 서비스는 외부와 단절된 것이 아니다. 서비스 수명 주기를 일찍 수립하자. 데브옵스 관점에서 서비스 빌드와 배포를 자동화하는 방법뿐 아니라 서비스 상태를 모니터링하고 문제가 발생할 때 대응하는 방법도 집중해야 한다. 대개 서비스 운영은 비지니스 로직의 작성보다 더 많은 업무와 사전 숙고가 필요하다



# #3 스프링 클라우드 컨피그 서버로 구성 관리하기



## 구성관리 원칙

1. **분리**: 실제 물리적인 서비스의 배포와 서비스 구성 정보를 완전히 분리하고자 한다. 어플리케이션 구성 정보를 서비스 인스턴스와 함께 배포하면 안된다. 그 대신 시작하는 서비스에 환경변수로 전달하거나 중앙 저장소에서 읽어와 구성 정보를 전달해야 한다.
2. **추상화**: 서비스 인터페이스 뒷 단에 있는 구성 데이터의 접근 방식을 추상화한다. 서비스 저장소에 직접 액세스하는 코드를 작성하기(즉, 파일이나 jdbc를 사용해 데이터베이스에서 데이터를 읽기)보다 어플리케이션이 REST 기반의 JSON 서비스를 사용해 구성 데이터를 조회하게 만들어야 한다.
3. **중앙 집중화**: 클라우드 기반의 어플리케이션에는 말 그대로 수백개의 서비스가 존재할 수 있으므로 구성 정보를 보관하는 저장소 개수를 최소로 줄이는 것이 매우 중요하다. 어플리케이션의 구성 정보를 가능한 소수 저장소에 집중화 한다.
4. **견고성**: 어플리케이션 구성 정보를 배포된 서비스와 완전히 분리하고 중앙 집중화 하므로 어떤 솔루션을 사용하더라도 고가용성과 다중성을 구현할 수 있어야 한다.

<br>

## 구성관리 lifecycle

1. 마이크로서비스 인스턴스가 시작하면 서비스 엔드포인트를 호출해 동작중인 환경별 구성 정보를 읽어온다. 구성 관리 서비스에 연결할 정보는 마이크로서비스가 시작할 때 전달된다.
2. 실제 구성 정보는 저장소에 상주한다. 구성 데이터를 보관할 수 있는 구성 저장소 구현 방식이 다양하며, 소스 관리되는 파일이나 관계형 데이터베이스, 키-값 짝 데이터 저장소 같은 구현 방식을 택할 수 있다.
3. 실제로 어플리케이션 배포 방식과 독립적으로 어플리케이션의 구성 데이터를 관리한다. 대개 빌드 및 배포 파이프라인으로 구성 관리를 변경하며 변경된 구성은 버전 정보 태그를 달아 다른 환경에 배포될 수 있게 한다.
4. 구성 관리가 변경되면 어플리케이션 구성 데이터를 사용하는 서비스는 변경 통보를 받고 보유한 어플리케이션 데이터 사본을 갱신해야 한다.

<br>

## 구성관리 오픈소스 툴



| 프로젝트 이름               | 설명                                                         | 특성                                                         |
| :-------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Etcd                        | Go로 작성된 오픈소스 프로젝트, 서비스 검색과 key-value 관리에 사용되며 분산 컴퓨팅 모델용 Raft 프로토콜을 사용한다 | 1. 초고속이며 확장 가능<br />2. 분산 가능<br />3.명령줄 위주<br />4.사용과 설치가 쉬움 |
| 유레카                      | 넷플릭스가 만들었으며, 수많은 실전 테스트를 거쳤다. 서비스 검삭과 key-value 관리에 사용된다 | 1. 분산 key-value 저장소<br />2.유연하지만 설정하는데 공수가 든다.<br />3. 동적 클라이언트 갱신 기능을 제공한다 |
| 콘설                        | 하시코프가 만들었따. Etcd 및 유레카와 유사한 긴으을 제공하나 분산 컴퓨팅 모델에 따른 알고리즘을 사용한다. | 1.빠르다<br />2. DNS와 직접 통합해 네이티브 서비스 검색을 제공한다<br />3. 동적 클라이언트 갱신은 기본 기능으로 제공하지 않는다. |
| 주키퍼                      | 분산 잠금 기능을 제공하는 아파치 프로젝트로 key-value 데이터용 구성관리 솔루션으로 자주 사용된다 | 1. 가장 오래되고 실전 경험이 많은 솔루션<br />2. 가장 사용하기 복잡하다.<br />3. 구성 관리에 사용 가능하지만 이미 사용중일떄만 고려해야 한다. |
| 스프링 클라우드 컨피그 서버 | 다양한 백엔드와 함께 일반적인 구성관리 솔루션을 제공하는 오픈소스 프로젝트다. 깃, 유레카 및 콘설 같은 백엔드와 통합 가능하다 | 1.비분산 key-value 저장소<br />2. 스프링 및 스프링 기반이 아닌 서비스와 통합 가능하다.<br />3. 공유 파일과 시스템, 유레카, 콘설, 깃 등 구성 데이터를 저장하기 위한 다양한 백엔드 사용이 가능하다. |

<br>

## 스프링 클라우드 컨피그 서버의 특징

1. 스프링 클라우드 컨피그 서버는 쉽게 설치하고 사용할 수 있다.
2. 스프링 클라우드 컨피그는 스프링 부트와 긴밀히 통합되어 있다. 따라서 모든 어플리케이션의 구성 데이터를 사용이 간편한 어노테이션으로 읽어올 수 있다.
3. 스프링 클라우드 컨피그 서버는 구성 데이터를 저장할 수 있는 여러 백엔드를 지원한다. 유레카나 콘설같은 도구를 이미 사용하고 있다면 바로 스프링 클라우드 컨피그 서버에 연결할 수 있다.
4. 스프링 클라우드 컨피그 서버는 깃 소스제어 플랫폼과 직접 통합할 수 있다. 스프링 클라우드 컨피그를 깃과 통합하면 다른 솔루션의 추가 의존성을 제거하고 어플리케이션의 구성 데이터를 손쉽게 버전 관리할 수 있다.Etcd, 콘설, 유레카 같은 다른 도구는 자체 버전 관리 기능이 없으므로 필요하다면 직접 구축해야 한다. 이미 깃을 사용하고 있다면 스프링 클라우드 컨피그 서버는 매력적인 옵션이다.



## 스프링 클라우드 컨피그 서버의 대칭키 암호화



### 설치 및 적용

1. 암호화에 필요한 오라클 JCE jar 파일을 내려받고 설치한다.
2. 암호화 키를 설정한다.
3. 프로퍼티를 암호화 및 복호화한다.
4. 클라이언트측에서 암호화하도록 마이크로서비스를 구성한다.



다운로드 경로: 

https://www.oracle.com/java/technologies/javase-jce8-downloads.html

검색 키워드 : Java Cryptography Extensions



1. $JAVA_HME/jre/lib/security에 있는 local_policy.jar와 US_export_policy.jar 파일을 다른 위치로 백업한다
2. 오라클에서 내려받은 JCE.zip 안에 있는 local_policy.jar와 US_export_policy.jar \을 복사한다
3. 스프링 클라우드 컨피그가 암호화를 사용하도록 구성한다.

### 암호화 키 설정

1. 대칭 키의 길이는 12자 이상이어야하며 불규칙 문자열이 이상적

2. 대칭키를 분실하면 안된다. 암호화 키로 한번 암호화한 것은 그 키 없이는 복호화 할 수 없다.

   



# #4 서비스 위치 찾기

분산 아키텍처에서는 시스템의 물리적 위치를 찾아야함 공식적으로 서비스 디스커버리라는 명칭을 사용,

서비스 디스커버리는 어플리케이션에서 사용하는 모든 원격 서비스의 주소가 포함된 프로퍼티 파일을 관리하는 것처럼 단순하거나 UDDI 저장소처럼 정형화되고 복잡한 것일 수 있음

## 서비스 디스커버리의 중요성

1. 어플리케이션 팀은 서비스 디스커버리를 사용해 해당 환경에서 실해앟는 서비스 인스턴스 개수를 신속하게 수평 확장하거나 축소할 수 있다. 서비스 디스커버리를 통해 서비스의 물리적인 위치는 서비스 소비자에게 드러나지 않는다. 서비스 소비자는 실제 서비스 인스턴스의 물리적 위치를 모르기 때문에 서비스 풀에서 새로운 서비스 인스턴스의 추가나 삭제가 자유롭다. 이런 개념은 모놀리틱과 싱글테넌트 어플리케이션에 익숙한 개발팀에게 수평적인 확장하는 접근방식으로 사고의 전환 가능
2. 어플리케이션 회복성을 향상하는데 도움이 된다. 마이크로서비스 인스턴스가 비정상이거나 가용하지 않다면 대부분의 서비스 디스커버리 엔진은 내부의 가용 서비스 목록에서 해당 인스턴스를 제거시킨다. 서비스 디스커버리 엔진이 사용할 수 없는 서비스를 피해 라우팅하므로 다운된 서비스가 야기한 피해를 최소화 한다.

## 서비스 위치 찾기

서비스 디스커버리를 사용하지 않은 환경에서는 대개 DNS와 네트워크 로드 밸런서로 어플리케이션을 호출한다.

서비스요청자에게 요청을 받고 -> 로드밸런서는 사용자가 액세스하려는 경로를 기반으로 라우팅 테이블에서 물리적 주소 항목 찾기 -> 로드밸런서가 서버 목록에서 한개의 서버에 요청 전달

위 방법은 마이크로 서비스에서는 적합하지 못함

1. 단일 장애 지점: 로드 밸런서가 고가용성을 지원한다고 해도 여전히 전체 인프라의 단일 장애 지점임. 로드 밸런서가 다운되면 로드 밸런서에 의존하는 모든 어플리케이션도 다운됨. 로드 밸런서를 고가용하게 만들더라도 어플리케이션 인프라 안에서 집중화된 병목 지점이 될 가능성이 높음
2. 수평 확장의 제약성: 로드 밸런서 클러스터에 서비스를 모아 연결하므로 부하 분산 인프라를 여러 서버에 수평적으로 확장할 수 있는 능력이 제한됨 상용 로드밸런서는 다수의 중복성 모델과 라이선싱 비용이라는 두가지 요소에 제약을 받음.  본직적으로 하드웨어의 제약을 받음. 
3. 정적 관리: 전통적 로드 밸런서 대부분은 서비스를 신속히 등록하고 취소하도록 설계되지 않음. 중앙 집중식 데이터베이스를 사용해 경로 규칙을 저장하고 대개 공급업체의 독점적인 api를 사용해야만 새로운 경로를 저장할 수 있음
4. 복잡성: 로드 밸런서가 서비스에 대한 프록시 역할을 하므로 서비스 소비자에게 요청할때 물리적인 서비스에 매핑된 요청 정보가 있어야 함.  이 변환 계층은 서비스 매핑 규칙을 수동으로 정의 배포해야하므로 복잡성 증가. 

## 클라우드에서 서비스 디스커버리

### 서비스 디스커버리 메커니즘

1. 고가용성: 서비스 디스커버리는 서비스 검색 정보를 서비스 디스커버리 클러스터의 여러 노드가 공유하는 핫 클러스터링 환경을 지원해야함. 한 노드가 사용할 수 없게 되면 클러스터의 다른 노드가 인계를 받을수 있어야 함.
2. 피어 투 피어: 서비스 디스커버리 클러스터의 각 노드는 서비스 인스턴스의 상태를 공유함
3. 부하 분산: 서비스 디스커버리는 요청을 동적으로 부하 분산해서 서비스 디스커버리가 관리하는 모든 서비스 인스턴스에 분배해야함. 
4. 회복성: 서비스 디스커버리 클라이언트는 서비스 정보를 로컬에 캐시해야함. 로컬 캐싱 자체가 서비스 디스커버리 기능을 점진적으로 저하시킬 수 있는데 서비스 디스커버리 서비스가 가용하지 않을 때 어플러키에션이 로컬 캐시에 저장된 정보를 기반으로 서비스를 계속 찾을 수 있고 동작하게 해야함.
5. 장애 내성: 서비스 디스커버리는 서비스 인스턴스의 비정상을 탐지하고 가용 서비스 목록에서 인스턴스를 제거해야함. 그리고 이러한 서비스 장애를 감지하고 사람의 개입 없이 조치를 취해야함

### 서비스 디스커버리 아키텍처

1. 서비스 등록: 서비스를 서비스 디스커버리 에이전트에 어떻게 등록하는가?
2. 클라이언트가 서비스 주소 검색: 서비스 클라이언트가 어떻게 서비스 정보를 검색하는가?
3. 정보 공유: 서비스 정보를 노드간에 어떻게 공유하는가?
4. 상태 모니터링: 서비스가 자신의 상태 정보를 서비스 디스커버리 에이전트에 어떻게 전달하는가?

<br>

### 서비스 디스커버리 패턴 흐름

1. 서비스 인스턴스가 시작하면 서비스 디스커버리 인스턴스가 접근할 수 있는 자신의 물리적 위치와 경로, 포트를 등록함 서로 동일한 서비스 id를 등록함
2. 서비스는 일반적으로 개의 서비스 디스커버리 인스턴스에만 등록함
3. 서비스 디스커버리 구현체는 대부분 P2P모델을 사용해 서비스 인스턴스의 데이터를 클러스터에 있는 다른 노드에 전파함
4. 서비스 디스커버리 구현에 따라 전파 메커니즘을 하드코딩된 서비스 목록을 사용하거나 gossip, infection-style 프로토콜을 사용해 클러스터에서 발생된 변경을 다른노드가 발견할 수 있음
5. 서비스 인스턴스는 자기 상태를 서비스 디스커버리 서비스에 푸시하거나 그의 반대로 상태를 추출함. 정상 상태를 반환하지 못한 서비스는 가용한 서비스 인스턴스 풀에서 제거됨
6. 서비스가 서비스 디스커버리에 등록되면 그 서비스의 기능을 사용해야하는 어플리케이션이나 다른 서비스에서 사용할 준비가 된 것임
7. 클라이언트는 서비스 디스커버리 엔진에만 의존함



### 위에있는것 보다 더 강화된 서비스 디스커버리 패턴 (클라이언트 측 부하 분산)

1. 서비스 소비자가 요청한 모든 서비스 인스턴스를 위해 서비스 디스커버리 서비스에 접속한 후 데이터를 서비스 소비자 기기에 로컬 캐시함
2. 클라이언트가 서비스를 호출할 때마다 서비스 소비자는 캐시에서 위치 정보를 검색함. 일반적으로 클라이언트 측 캐싱은 라운드로빈 부하 분산 알고리즘처럼 단순한 알고리즘을 사용해 서비스 호출을 여러 인스턴스로 분산함
3. 클라이언트는 주기적으로 서비스 디스커버리 서비스에 접속해 서비스 인스턴스 캐시를 새로고침함. 클라이언트 캐시는 최종 일관성을 유지하지만 클라이언트가 목록을 새로고침하기 위해 서비스 디스커버리 인스턴스에 접속하고 서비스에 호출할 때 비정상적인 서비스 인스턴스를 호출할 위험성은 항상 존재함. 
4. 서비스를 호출하는 동안 서비스 호출이 실패하면 로컬에 있는 서비스 디스커버리 캐시가 무효화되고 서비스 디스커버리 클라이언트는 서비스 디스커버리 에이전트에 목록 새로고침을 시도함





## 스프링 유레카 설정

### application.yml

- eureka.client.registerWithEureka: 자신을 유레카 서비스에 등록하지 않도록 설정
- eureka.client.fetchRegistry: false로 설정시 유레카 서비스가 시작할때 레지스트리 정보를 로컬에 저장하지 않음. 스프링 부트 서비스로 된 유레카 클라이언트를 유레카에 등록할 경우 이 값을 바꿀 수 있음
- eureka.server.waitTimeInMsWhenSyncEmpty: 유레카는 기본적으로 모든 서비스가 등록할 기회를 갖도록 5분을 기다린 후 등록된 서비스 정보를 공유함. 로컬에서 테스트할때 시간단축용으로 사용



유레카는 등록된 서비스에서 10초 간격으로 연속 3회의 상태정보를 받아야하므로 등록된 개별 서비스를 보여주는데 30초가 걸림

## 스프링 유레카에 서비스 등록

bootstrap.yml 이 application.yml보다 먼저 인식함 장기적인 관점으로 봤을때 bootstrap.yml에 설정해야하는 파일은 bootstrap.yml에 분리하도록 지정하는게 좋음

서비스 인스턴스는 두개의 id를 가져야함

- 어플리케이션 ID: 서비스 인스턴스의 그룹화 ID
- 인스턴스 ID: 서비스 인스턴스의 고유 식별 ID
- eureka.instance.preferIpAddress: 서비스 호스트 이름이 아닌 IP 주소를 유레카에 등록하도록 지정

기본적으로 유레카는 호스트이름으로 접속하는 서비스를 등록함. 이 설정은 DNS가 지원된 호스트 이름을 할당하는 서버 기반 환경에서 잘 동작함. 그러나 컨테이너 기반의 배포에서 컨테이너는 DNS 엔트리가 없는 임의로 생성된 호스트 이름을 부여받아서 시작함. eureka.instance.preferIpAddress을 true로 설정해주어 IP 주소를 전달받는것으로 설정해야 어플리케이션의 호스트를 구해올 수 있음. 저자는 true를 추천함

## 서비스 디스커버리를 사용해 서비스 검색

- Discovery: 디스커버리 클라이언트와 표준 스프링 RestTemplate를 통한 서비스 호출
- Rest: 향상된 스프링 RestTemplate를 사용해 리본 기반의 서비스 호출
- Feign: 넷플릭스 Feign 클라이언트를 사용해서 리본을 통해 서비스 호출

### DisCoveryClient로 서비스 인스턴스 검색

- 리본과 등록된 서비스에 가장 저수준의 접근성 제공
- DisCoveryClient를 사용하면 리본 클라이언트와 해당 url에 등록된 모든 서비스에 대해 질의 가능
- @EnableDiscoveryClient를 이용해 DisCoveryClient와 리본 라이브러리를 사용 가능하도록 명시

**DisCoveryClient의 단점** 

1. 리본 클라이언트 측 부하 분산의 장점 X : 서비스의 호출 책임이 사용자에게 있음
2. 서비스 호출에 사용될 URL을 생성해야함
3. 서비스를 호출하는 더 좋은 방식이 존재함



### 리본 지원 스프링 RestTmeplate를 사용한 서비스 호출

DisCoveryClient를 사용하는것보다  더 일반적인 메커니즘

- @LoadBalanced를 사용해서 RestTemplate 객체를 미리 생성
- @EnableDiscoveryClient 같은 어노테이션은 불필요
- @LoadBalanced를 사용해서 RestTemplate 객체는 스프링 표준 RestTemplate와 유사하게 동작하지만 서비스의 물리적 위치를 사용하는 대신 호출하려는 서비스의 유레카 서비스 ID를 사용함
- ex : http://{applicationId}/{serviceUrl}/{serviceInstanceId}
- 리본은 RestTemplate를 클래스를 사용해 서비스 인스턴스에 대한 모든 요청을 라운드로빈 방식으로 부하 분산

### 넷플릭스 Feign 클라이언트로 서비스 호출

- @EnableFeignClients를 사용함
- java의 interface를 기반으로 사용함 다른코드보다 대체적으로 깔끔하고 추상화도 잘 되어있음
- Feign이 전부 알아서해줌 그냥 연결만 시켜주면됨
- 에러처리의 경우 4xx-5xx는 FeignException으로 반환됨
- 사용자 정의 Exception 클래스로 재 매핑하는 에러 디코더 클래스 작성 기능을 제공함

## 요약

- 서비스 디스커버리 패턴은 서비스의 물리적 위치를 추상화하는데 사용함
- 유레카 같은 서비스 디스커버리 엔진은 서비스 클라이언트에 영향을 주지 않고 해당 환경의 서비스 인스턴스를 원활하게 추가 삭제 가능
- 클라이언트 측 부하 분산을 사용하면 서비스를 호출하는 클라이언트에서 서비스의 물리적인 위치를 캐싱해더 나은 성능과 회복성을 제공 가능 
- 유레카는 네슾ㄹ릭스 프로젝트의 스프링 클라우드와 사용하면 쉽게 구축하고 구성할 수 있음
- 스프링 클라우드와 넷플릭스 유레카 그리고 서비스를 호출하는 넷플릭스 리본으로 다은 세가지 메커니즘 사용 가능
  1. DisCoveryClient
  2. 리본기반 RestTemplate
  3. 넷플릭스 Feign 클라이언트

# #5 나쁜 상황에 대비한 스프링 클라우드와 넷플릭스 히스트릭스의 클라이언트 회복성 패턴

서비스 하나가 충돌하면 쉽게 감지가 가능하지만 서비스가 느려질 경우 성능 저하를 감지하고 우회하는것은 다음 이유로 매우 어렵다

1. **서비스 저하는 간헐적으로 발생하고 확산될 수 있다**: 서비스 저하는 사소한 부분에서 갑자기 발생 가능함. 순식간에 어플리케이션 컨테이너가 스레드 풀을 모두 소진해 완전히 무너지기 전까지 장애 징후는 일부 사용자가 문제점을 불평하는 정도이기 때문
2. **원격 서비스 호출은 대개 동기식이며 오래 걸리는 호출을 중단하지 않는다**: 서비스 호출자에게는 호출이 영구 수행되는 것을 방지하는 타임아웃 개념이 없음. 어플리케이션 개발자는 서비스를 호출해 작업을 수행하고 서비스가 응답할 때까지 기다림
3. **어플리케이션은 대개 부분적인 저하가 아닌 원격 자원의 완전한 장애를 처리하도록 설계된다**: 서비스가 완전히 붕괴되지 않는 이상 서비스를 계속 호출하고 빠른 실패 확인이 불가함. 호출하는 서비스는 제대로 동작하지 않는 어플리케이션을 호출해서 일부 호출은 비정상적인 종료가 있을 수 있다. 

안전 장치가 없다면 제대로 동작하지 않는 서비스 하나가 여러 어플리케이션을 짧은 시간 동안 다운 시킬 수 있다.

클라우드와 마이크로서비스에 기반을 둔 어플리케이션은 사용자의 트랜잭션을 완료하는데 필요한 여러 인프라위에서 세밀하게 분산되어있기 때문에 이런 에러에 매우 취약하다



## 클라이언트 회복성 패턴이란? 

- 클라이언트의 회복성패턴은 원격 서비스가 에러를 던지거나 제대로 동작하지 못해 원격 자원의 접근이 실패할때 원격 자원을 호출하는 클라이언트 충돌을 막는데 초점이 맞추어져 있음
- 이 패턴의 목적은 클라이언트의 상향 전파되는 것을 막음
- 원격자원을 소비하는 클라이언트와 자원 사이에서 패턴을 구현

### **1. 클라이언트 측 부하 분산**

- 클라이언트가 넷플릭스 유레카 같은 서비스 디스커버리 에이전트를 이용해 서비스의 모든 인스턴스를 검색한 후 해당 서비스 인스턴스의 실제 위치를 캐싱함 (서비스 디스커버리 에이전트의 의존성을 어느정도 제거)
- 넷플릭스의 리본 라이브러리
- 서비스 클라이언트가 서비스 인스턴스를 호출해야할 때마다 서비스 클라이언트 측 로드 밸런서는 서비스 위치 풀에서 관리하는 서비스 위치를 하나씩 전달함
- 서비스 클라이언트 측 로드 밸런서는 서비스 클라이언트와 서비스 소비자 사이에 위치하므로 서비스 인스턴스가 에러를 전달하거나 불량 동작하는지 감시함
- 서비스 클라이언트측 로드 밸런서가 문제를 감지할 수 있다면 가용 서비스 위치 풀에서 문제가 된 서비스 인스턴스를 제거해 서비스 호출이 그 인스턴스로 전달되는 것을 막음

### 2. 회로 차단기

- 전기 회로의 차단기를 본 떠 만든 클라이언트 회복성 패턴, 전기 시스템에서 회로 차단기는 전기선에 유입된 과젼류를 감지하는데 회로 차단기가 문제를 감지하면 모든 전기 시스템과 연결된 접속을 차단하고 하위 컴포넌트가 과전류에 손상되지 않도록 보호하는 역할을 함
- 소프트웨어 회로 차단기는 원격 서비스 호출을 모니터링함. 호출이 필요한 만큼 실패하면 회로차단기가 활성화 되어 빨리 실패하게 만들고 고장난 원격 자원은 더이상 호출되지 않도록 차단함

### 3. 폴백

- 원격 서비스에 대한 호출이 실패할 때 예외를 발생시키지 않고 서비스 소비자가 대체 코드 경로를 실행해 다른 방법으로 작업 수행이 가능하도록 함
- 예를들어 사용자에게 이 사용자가 관심있는 물품을 추천하는 기능이 있다고 했을때 보통 마이크로서비스를 호출해 사용자의 과거 행위를 분석하고 특정사용자에게 맞춤화된 추천 목록을 전달함. 하지만 만약 이 서비스가 고장 난다면 폴백은 모든 사용자의 구매 정보를 기반으로 더 일반화된 선호 목록을 조회함 이 일반화된 선호 목록은 완전히 다른 서비스에서 추출됨

### 4. 벌크헤드

- 선박을 건조하는 개념에서 유래 벌크헤드 설계를 적용하면 배는 격벽이라는 완전히 격리된 수밀 구획으로 나뉨. 만약 선체에 구멍이 뚫리더라도 격벽으로 분리되어 있기 때문에 침수 구역을 제한하고 배 전체의 침수와 침몰 방지가 가능함
- 벌크헤드 패턴을 사용하면 원격 자원에 대한 호출을 자원별 스레드 풀로 분리하므로 특정 원격 자원의 호출이 느려져 전체 어플리케이션이 다운될 수 있는 위험을 줄일 수 있음
- 스레드 풀은 서비스를 위한 벌크헤드 역할을 하고 각 원격 자원은 분리되어 스레드 풀에 할당됨
- 한 서비스가 느리게 반응한다면 해당 서비스 호출을 위한 스레드풀은 포함되지만 격벽(스레드 풀)으로 구분된 다른 스레드들은 정상 작동함

## 클라이언트 회복성이 중요한 이유

여러개가 묶여있는 마이크로서비스에서 만약 한부분의 서비스가 매우 느리게 답할 경우 최초에 호출했던 서비스들의 스레드풀이 계속 중첩되고 결국엔 포화상태가 되어 전체 어플리케이션의 고장을 야기한다. 이런 경우 회로차단기를 구성했다면 느리게 답하는 한 부분의 특정 호출을 감지해서 스레드를 소진하지 않도록 빠르게 실패시키고 해당 원격자원을 사용하는 부분을 제거함으로써 그나마 나머지 엮이지 않은 부분까지는 제 기능을 하도록 구성할 수 있다. 

회로차단기로 타임아웃이 되었다면 회로 차단기는 발생한 실패 횟수를 추적하기 시작하고 특정 시간동안 특정 서비스 에러가 기대 이상 발생하면 회로 차단기는 회로를 차단시키고 다음 세가지 일이 일어난다

A에서 B를 호출하는것으로 가정하고 A와 B 사이에 회로차단기가 있다고 가정한다.

1. 서비스 A는 회로 차단기가 타임아웃하기 전에 문제가 있다는 것을 즉시 알아챈다.
2. 서비스 A는 완전히 실패하거나 대체 코드를(폴백) 사용하는 조치를 취하는 것 중에서 선택할 수있다.
3. 회로 차단기가 차단된 동안 서비스 A는 서비스 B를 호출하지 못하므로 서비스 B에 복구할 수 있는 여유가 생긴다. 이 기회를 이용해 서비스 B는 회복할 시간을 갖고 서비스 저하로 발생되는 연쇄 장애를 막을수 있다.

### 원격 호출 회로 차단 패턴의 핵심 기능

1. **빠른 실패**: 원격 서비스의 호출이 성능 저하를 겪으면 해당 호출은 빨리 실패함으로써 어플리케이션 전체를 다운시킬 수 있는 자원 고갈 이슈를 방지함. 대부분의 장애 상황에서 완전히 다운되는 것 보다 부분 다운되는게 더 나음
2. **원만한 실패**: 타임아웃과 빠른 실패 방법을 사용하는 회로 차단기 패턴으로 어플리케이션 개발자는 원만하게 실패하거나 사용 의도로 수행하는 대체 머커니즘을 찾을 수 있음(폴백같음)
3. **원활한 회복**: 회로차단기 패턴을 통해 요청 자원이 정상인지 주기적으로 확인하고 사랍의 개입 없이 자원 접근을 다시 허용 가능

## 히스트릭스

위에서 언급한 폴백, 회로차단기, 벌크헤드 패턴을 직접 구현하는건 정말정말 어려운일이지만 다행히도 넷플릭스가 마이크로서비스를 직접 사용하면서 실전에서 검증한 라이브러리인 히스트릭스를 제공함

## 히스트릭스를 사용한 회로 차단기 구현

@EnableCircuitBreak를 부트스트랩클래스에 추가해야함

히스트릭스는 두가지의 경우로 사용 가능함 

1. 서비스와 데이터베이스
2. 서비스와 서비스

@HystrixCommand 어노테이션을 사용해 히스트릭스 회로 차단기가 관리하는 자바 클래스 메서드라고 표시함 스프링이 @HystrixCommand을 만나면 메서드를 감싸는 프ㅡ록시를 동적으로 생성하고 원격 호출을 처리하기 위해 확보한 스레드가 있는 스레드풀로 해당 메서드에 대한 모든 호출을 관리함

### @HystrixCommand 사용자 설정

commandProperties 속성을 사용해서 사용자 정의 설정을 함

```java
 @HystrixCommand(
   commandProperties={
         @HystrixProperty(name="circuitBreaker.requestVolumeThreshold", value="10"),
         @HystrixProperty(name="circuitBreaker.errorThresholdPercentage", value="75"),
         @HystrixProperty(name="circuitBreaker.sleepWindowInMilliseconds", value="7000"),
         @HystrixProperty(name="metrics.rollingStats.timeInMilliseconds", value="15000"),
         @HystrixProperty(name="metrics.rollingStats.numBuckets", value="5")}
```

- **metrics.rollingStats.timeInMilliseconds**: 회로 차단기의 타임아웃 설정

분산 환경에서 너무 느리게 실행되는 서비스 문제에 대해 절대로 해결 할 수 없는 일이 아닌 경우 이 설정을 늘리는 유혹을 최대한 피해야한다. 느리게 실행되는 서비스는 대부분 성능 문제가 있다.

## 폴백 프로세싱

회로 차단기 패턴의 장점은 서비스 호출에 실패했을때 이 실패를 가로채고 다은 대안을 선택할 수 있는 폴백 프로세싱을 사용할 수 있다는 점이다.

```java
@HystrixCommand(fallbackMethod = "buildFallbackLicenseList")
```

- **fallbackMethod**: 회로차단기가 작동될때 동작하는 메서드 이름을 지정 이 폴백 메서드는 @HystrixCommand를 사용한 클래스와 같이 있어야함



### 폴백 전략 사용시 주의사항

폴백 전략 사용 여부를 결정할 때 중요한것은 고객의 데이터 수명의 허용 정도와 문제가 있는 어플리케이션을 감추는 것이 얼마나 중요한지에 달려있다.

1. 폴백을 사용해서 타임아웃 예외를 잡아내고 로깅같은 처리만한다면 try~catch 만으로도 충분하다
2. 폴백메서드 역시 @HystrixCommand로 감싸질 수 있다. 폴백에서 또 다른 원격자원을 호출하는 경우에도 회로차단기가 동작할 수 있음을 명심하자.



## 벌크헤드 패턴

기본적인 자바의 서비스 호출 행위는 전체 자바 컨테이너에 대한 요청을 처리하는 스레드에서 이루어진다. 대규모 환경에서 한 서비스에서 발생한 성능 문제는 전체 자바 컨테이너 스레드 풀에게 영향을 줄 수 있다. 과부하게 계속되면 자바 컨테이너는 비정상정으로 종료할 것이다. 벌크헤드 패턴은 원격 자원 호출을 자신의 스레드 풀에 격리하므로 오작동 서비스를 억제하고 컨테이너의 비정상 종료를 방지한다.

히스트릭스는 스레드 풀을 사용해 원격 서비스에 대한 모든 요청을 위임하는데 이 스레드 풀은 REST 요청이라던지 데이터베이스요청같은 원격 서비스 호출을 처리할 10개의 스레드가 있는 스레드 풀이 존재한다. 만약 히스트릭스 스레드풀에서 어떤 원격 서비스 호출이 굉장히 오래걸리는 작업이있다면 이 스레드풀 역시 고갈되어 어플리케이션 전체에 악영향을 끼치게 될 것이다.

다행히도 히스트릭스는 서로 다른 원격 자원 호출 간에 벌크헤드를 생성하기에 용이한 메커니즘을 제공한다.

```java
@HystrixCommand(
        threadPoolKey = "licenseByOrgThreadPool",
        threadPoolProperties =
                {@HystrixProperty(name = "coreSize",value="30"),
                 @HystrixProperty(name="maxQueueSize", value="10")}
)
```

- **threadPoolKey**: 새로운 스레드풀을 설정하도록 key값 부여
- **threadPoolProperties**: 스레드풀 사용자 정의
- **coreSize**: 스레드 풀 크기를 설정 기본값 10
- **maxQueueSize**: 스레드들이 분주할때 스레드 풀의 앞 단에 백업할 큐를 value 값 만큼 만들어서 요청 수가 큐 크기를 초과하면 큐에 여유가 생길때 까지 스레드 풀에 대한 추가 요청은 모두 실패한다. 기본값 -1, -1일때는 큐를 사용하지 않는다

이 maxQueueSize 속성을 사용하는데 앞서 두가지 주의사항이 있다

1. maxQueueSize을 -1로 설정하면 유입된 호출을 유지하는데 자바의 SynchronousQueue가 사용된다 동기식 큐를 사용하면 본질적으로 스레드 풀에서 가용한 스레드 개수보다 더 많은 요청을 처리할 수 없다. 만약 1보다 큰 값으로 설정하면 히스트릭스는 자바의 LinkedBlockingQueue를 사용한다 LinkedBlockingQueue를 사용하면 모든 스레드가 요청을 처리하는데 분주하더라도 더 많은 요청을 큐에 넣을 수 있다.
2. maxQueueSize 속성은 스레드 풀이 처음 초기화 될 때만 설정할 수 있다. **queueSizeRejectionThreshold** 속성을 사용하면 히스트릭스는 큐 크기를 동적으로 변경할 수 있지만 maxQueueSize 가 0 이상일때만 가능하다

**스레드 풀의 적정 크기 공식은 다음과 같다 (넷플릭스에서 제공한 공식)**

**(서비스가 정상일 때 최고점에서 초당 요청 수 * 99 백분위 수 지연 시간(단위 : 초)) + 오버헤드를 대비한 소량의 추가 스레드**

만약 원격 자원 서비스는 정상인데 서비스 호출에 타임아웃이 걸린다면 스레드 풀을 조정해야한다.



## 히스트릭스 세부 설정

히스트릭스의 회로차단기 패턴은 호출 실패 횟수를 모니터링해서 필요 이상으로 실패할 때 원격 자원에 도달하기 전에 호출을 실패시켜 서비스로 들어오는 이후 호출을 자동으로 차단한다. 이렇게 하는 이유는 두가지다

1. 원격 자원에 성능 문제가 있는 경우 빨리 실패하게 하면 호출 어플리케이션이 호출 타임아웃을 기다릭는 것을 막아 호출 어플리케이션이나 서비스의 자원 고갈 문제와 비정상 종료될 위험을 크게 낮춘다.
2. 서비스 클라이언트의 빠른 실패로 호출을 막으면 힘겨워하는 서비스가 부하를 견디고 부하 때문에 완전히 비정상 종료되지 않는다.

### 히스트릭스의 회로차단기 차단 과정과 회복 과정

1. 문제 발생
2. 최소 요청수가 실패했는가? Yes
3. 에러 임계치에 도달했는가? Yes
4. 회로 차단기 차단
5. 원격 서비스 호출에 여전히 문제가 발생하는가? Yes 4번으로

No의 경우는 문제가 발생하지 않기 때문에 정상적인 호출이 이루어진다.

- 1~3 까지의 과정은 기본값으로 10초동안 판단하도록 되어있고 이 값은 설정 가능함
- 4~5 까지의 과정은 기본값으로 5초동안 판단하도록 되어있고 이 값 역시 설정 가능함
- 1~3 까지의 과정에서 10초동안 호출 회수를 추적한다. 만약 10초안에 지정해놓은 호출회수를 초과한다면 히스트릭스는 전체 실패 비율을 조사하기 시작한다. 
- 전체 실패 비율 기본값은 50%이고 이 값은 설정이 가능하다. 만약 이 값의 임계치를 넘긴다면 이후 요청하는 모든 호출은 실패한다.
- 만약 원격 호출이 10초동안 지정해놓은 횟수만큼 도달하지 않는다면 히스트릭스는 10초 뒤 호출 회수를 초기화시킨다.
- 만약 임계치를 넘겨서 회로차단기가 동작한다면 5번을 통해서 원격 서비스 호출이 정상적인지를 확인한다. 기본값은 5초로 설정되어 있고 이것 역시 설정 가능하다.

### 히스트릭스 설정

```java
@HystrixCommand(//fallbackMethod = "buildFallbackLicenseList",
        threadPoolKey = "licenseByOrgThreadPool",
        threadPoolProperties =
                {@HystrixProperty(name = "coreSize",value="30"),
                 @HystrixProperty(name="maxQueueSize", value="10")},
        commandProperties={
                 @HystrixProperty(name="circuitBreaker.requestVolumeThreshold", value="10"),
                 @HystrixProperty(name="circuitBreaker.errorThresholdPercentage", value="75"),
                 @HystrixProperty(name="circuitBreaker.sleepWindowInMilliseconds", value="7000"),
                 @HystrixProperty(name="metrics.rollingStats.timeInMilliseconds", value="15000"),
                 @HystrixProperty(name="metrics.rollingStats.numBuckets", value="5")}
)
```

- **circuitBreaker.requestVolumeThreshold**: 히스트릭스가 회로 차단기의 차단 여부를 검토하는데 필요한 최소 요청 수 기본값 20
- **circuitBreaker.errorThresholdPercentage**: 히스트릭스가 호출 차단을 고려하고 연속 호출 개수가 지정한 수보다 높을때 전체 실패 비율 계산 기본값 50
- **circuitBreaker.sleepWindowInMilliseconds**: 히스트릭스가 서비스의 회복 상태를 확인할때까지 대기하는 시간 기본값 5초
- **metrics.rollingStats.timeInMilliseconds**: 히스트릭스가 서비스 호출을 모니터링하는 시간 기본값은 10초 
- **metrics.rollingStats.numBuckets**: 설정한 시간 간격 동안 통계를 수집할 횟수를 설정. 히스트릭스는 해당 시간 동안 버킷에 측정 지표를 수집하고 통계를 바탕으로 원격 자원 호출의 실패 여부를 결정한다. 설정한 버킷수는 **metrics.rollingStats.timeInMilliseconds** 값으로 균등하게 분할되어 위의 예로는 15초의 간격동안 3초간격으로 5개의 버킷에 통계 데이터를 수집한다.

확인할 통계 간격이 작고 유지하는 버킷수가 많은수록 대량 서비스에서 CPU및 메모리 사용량이 증가한다. 세분화된 버킷을 설정하고 싶은 유혹을 이겨내야한다.



### 히스트릭스 구성 재 검토

히스트릭스는 구성 기능이 뛰어나서 회로차단기, 폴백, 벌크헤드 패턴의 구현을 정밀하게 할 수 있다.

히스트릭스 환경을 구성할 때는 히스트릭스 세가지 구성 레벨을 꼭 기억해야한다.

- 어플리케이션 기본값
- 클래스 기본값
- 클래스 안에서 정의된 스레드 풀 레벨

<br>

- 클래스단위로 묶을때는 @DefaultProperties라는 어노테이션을 사용한다.

```java
@DefaultProperties(
  commandProperties = {
    @HystrixProperty(
      name="execution.isolation.thread.timeoㅕtInMilliseconds", value="10000"
    )
  }
)
class MyService{...}
```

MyService 안에 있는 모든 원격 자원 호출 모니터링 시간을 10초로 지정

히스트릭스 세부설정을 자주 바꾸어주어야한다면 Spring Cloud Config Server를 사용하는 것을 추천한다. 이걸 사용하면 어플리케이션의 인스턴스만 재시작해주면 된다.

## 스레드 컨텍스트와 히스트릭스

@HystrixCommand가 실행될 때 Thread 와 Semaphore라는 두가지 다른 격리 전략을 수행할 수 있다. 기본적으로 히스트릭스는 Thread 격리 전략을 수행한다. 호출을 시도한 부모 스레드와 컨텍스트를 공유하지 않는 격리된 스레드 풀에서 수행되는데 이런 전략은 히스트릭스가 자기 통제 하에서 원래 호출을 시도한 부모 스레드와 연관된 어떤 활동도 방해하지 않고 스레드 실행을 중단할 수 있다는 것을 의미한다.

Semaphore 기반 격리를 사용하면 히스트릭스는 새로운 스레드를 시작하지 않고 @HystrixCommand 어노테이션이 보호하는 분산 호출을 관리하며 타임아웃이 발생하면 부모 스레드를 중단시킨다.

톰캣처럼 동기식 컨테이너 서버 환경에서 부모 스레드를 중단하면 개발자가 예외 처리를 할 수 없는 예외가 발생한다. 이처럼 발생한 예외를 처리할 수 없거나 자원 정리 및 에러 처리를 수행할 수 없다면 개발자가 코드를 작성할 때 예기하지 않은 결과를 발생시킬 수 있다.

```javascript
@HystrixCommand(
  commandProperties = {
    @HystrixProperty(
      name="execution.isolation.strategy", value="SEMAPHORE"
    )
  }
)
```

기본적으로 Thread 전략을 추천하고 Thread 전략이 Semaphore전략보다 격리 수준이 더 높다 Semaphore는 Netty같은 비동기 I/O 컨테이너를 적용할때 고려해야한다.

## ThreadLocal과 히스트릭스

기본적으로 히스트릭스는 부모 스레드의 컨텍스트를 히스트릭스 명령이 관리하는 스레드에 전파하지 않는다. 예를들면 Spring Securty에서 ThreadLocal 전략으로 UserContext 객체를 저장해서 하나의 컨텍스트에서 항상 UserContext 객체를 꺼내올 수 있지만 히스트릭스로 @HystrixCommand로 감싸진 부분은 전파되지 않는다.

다행히 히스트릭스와 스프링클라우드는 부모 스레드의 컨텍스트를 히스트릭스 스레드 풀이 관리하는 스레드에 전달하는 메커니즘을 제공한다. 이 메커니즘을 **HystrixConcurrencyStrategy**라고한다

## HystrixConcurrencyStrategy 동작

히스트릭스를 사용하면 히스트릭스 호출을 감싸는 병행성 전략을 사용자 정의하고 부모 스레드의 컨텍스트를 히스트릭스 명령이 관리하는 스레드에 주입할 수 있다. 사용자 정의된 HystrixConcurrencyStrategy를 구현하려면 다음 작업을 수행해야 한다.

1. 히스트릭스 병행성 전략 클래스를 사용자 저으이하기
2. 히스트릭스 명령에 UserContext를 주입하도록 자바 Callable 클래스 정의하기
3. 히스트릭스 병행성 전략을 사용자 정의하기 위해 스프링 클라우드 구성하기

예제는 생략



히스트릭스는 하나의 HystrixConcurrencyStrategy만 허용한다. 만약 스레드 수준의 격리 방식으로 히스트릭스를 사용할 때는 반드시 이런 작업이 필요하다.

## 요약

- 마이크로서비스에 기반을 둔 어플리케이션처럼 고도로 분산된 어플리케이션을 설계할 때는 클라이언트 회복성이 고려되어야 한다.
- 심각한 서비스 장애는 탐지 탐지하고 처리하기 쉽다.
- 성능이 나쁜 서비스 하나가 호출을 완료할 때까지 호출 클라이언트를 대기시키므로 연쇄적인 자원 고갈을 유발한다.
- 세 가지 핵심적인 클라이언트 회복성 패턴은 회로 차단기와 폴백, 벌크헤드 패턴이다
- 회로 차단기 패턴은 느리게 실행되고 성능 저하된 시스템 호출을 종료해 빨리 실패시키고 자원 고갈을 방지한다
- 폴백 패턴을 사용하면 개발자가 원격 서비스 호출이 실패하거나 호출에 대한 회로 차단기가 실패할때 대체할 코드 경로를 정의할 수 있다.
- 벌크헤드 패턴은 원격 호출을 서로 격리하고 원격 서비스 호출을 자체 스레드 풀로 분리한다. 일련의 서비스 호출이 실패할 때 어플리케이션 컨테이너의 모든 자원이 고갈되어서는 안된다.
- 스프링 클라우드와 넷플릭스 히스트릭스 라이브러리는 회로 차단기와 폴백, 벌크헤드 패턴에 대한 구현을 제공한다.
- 히스트릭스 라이브러리는 구성 기능이 뛰어나며, 어플리케이션 전역과 클래스, 스레드 풀 레벨로 설정할 수 있다.
- 히스트릭스는 Thread와 Semaphore 격리 모델을 지원한다.
- 히스트릭스의 기보 ㄴ격리 모델인 Thread 모델은 히스트릭스로 보호된 호출을 완벽히 격리해서 부모 스레드 컨텍스트를 히스트릭스가 관리하는 스레드에 전파하지 않는다.
- 히스트릭스의 또 다른 격리 모델인 Semephore 모델은 히스트릭스 호출을 위해 별도의 스레드를 사요하지 않는다. 이 모델은 더 효율적이지만 히스트릭스가 호출을 중단할 때 서비스가 예상하지 않은 동작도 유발할 수 있다.
- 히스트릭스를 사용하면 사용자가 정의한 HystrixConcurrencyStrategy를 구현해 부모 스레드 컨텍스트를 히스트릭스가 관리하는 스레드에 주입할 수 있다.

# #6 스프링 클라우드와 주울로 서비스 라우팅

마이크로서비스에서 보안과 로깅, 사용자 추적 등의 기능이 필요한 시점이 오는데 다음 세가지 이유를 봐보자.

1. 구축중인 각 서비스에 이러한 기능을 일관되게 구현하기 어렵다. 개발자는 비지니스 로직에 집중하기 때문에 로깅, 추적 기능의 구현을 잊어버리기 쉽다.
2. 이런 기능을 적절하게 구현하기 어렵다. 마이크로서비스에 대한 보안 기능들을 구현중인 각 서비스에 설정하고 구성하기 어려울 수 있다. 
3. 서비스 간 복잡한 의존성을 만든다. 전체 서비스가 공유하는 공통 프레임워크에 더 많은 기능을 추가할수록 서비스의 재컴파일과 재배포 없이 공통 코드의 동작 변경이나 추가를 하는 일이 더 어려워진다. 

이런 문제들을 해결하려면 특정 서비스에서 이러한 횡단 관심사들을 추상화하고 독립적인 위치에서 어플리케이션의 모든 마이크로서비스 호출에 대한 필터와 라우터 역할을 해야한다. 이러한 횡단 관심사를 **서비스 게이트웨이**라고 한다. 서비스 클라이언트가 서비스를 직접 호출하지 않고 단일한 정책 시행 지점 역할을 하는 서비스 게이트웨이로 모든 호출을 경유시켜 최종 목적지로 라우팅한다.

- 하나의 URL 뒤에 모든 서비스를 배치하고 서비스 디스커버리를 이용해 모든 호출을 실제 서비스 인스턴스로 매핑한다.
- 서비스 게이트웨이를 경유하는 모든 서비스 호출에 상관관계 ID를 삽입한다.
- 호출할 때 생성된 상관관계 ID를 HTTP응답에 삽입하고 클라이언트에 회신한다.
- 대중이 사용중인 것과 다른 조직 서비스 인스턴스 엔드포인트로 라우팅하는 동적 라우팅 메커니즘을 구축한다.



## 서비스 게이트웨이란?

지금까지는 포스트맨을 통해서 개별 서비스를 직접 호출하거나 유레카 같은 서비스 디스커버리 엔진을 이용해 프로그램 방식으로 호출했다. 서비스 게이트웨이는 서비스 클라이언트와 호출될 서비스 사이에서 중개 역할을 한다. 서비스 게이트웨이가 구축되면 서비스 클라이언트는 개별 서비스의 URL을 직접 호출하지 않고 서비스 게이트웨이로 모든 호출을 보낸다.

### 서비스 게이트웨이에서 구현할 수 있는 횡단 관심사

- **정적 라우팅**: 서비스 게이트웨이는 단일 서비스와 URL과 API 경로로 모든 서비스를 호출하게 한다. 개발자는 모든 서비스에 대하 하나의 서비스 엔드포인트만 알면 되므로 개발이 간단해진다.
- **동적 라우팅**: 서비스 게이트웨이는 유입되는 서비스 요청을 조사하고 요청 데이터를 기반으로 서비스 호출자 대상에 따라 지능형 라우팅을 수행할 수 있다. 예를 들어 베타 프로그램에 참여하는 고객의 서비스 호출은 모두 다른 코드 버전이 수행되는 특정 서비스 클러스터로 라우팅 될 수 있다.
- **인증과 인가**: 모든 서비스 호출은 서비스 게이트웨이로 라우팅되므로 서비스 게이트웨이는 서비스 호출자가 자신을 인증하고 서비스를 호출할 권할 여부를 확인할 수 있는 최적의 장소이다.
- **측정 지표 수집과 로깅**: 서비스 게이트웨이를 사용하면 서비스 호출이 서비스 게이트웨이를 통과할 때 측정 지표와 로그 정보를 수집할 수 있다. 규격화된 로깅을 보장하기 위해 사용자 요청에서 주요 정보가 누락되지 않았는지 확인하는 데도 사용된다. 이는 각 서비스에서 측정 지표를 수집할 필요가 없다는 것이 아니라 서비스 게이트웨이를 사용하면 서비스가 호출된 횟수와 응답 시간처럼 많은 기본 측정 지표를 한곳에서 수집할 수 있다는 의미다.

**서비스 게이트웨이는 병목지점 이다?**

- 로드 밸런서는 서비스 그룹 앞에 있을때 유용하다. 이때 여러 서비스 게이트웨이 인스턴스 앞에 로드 밸런서를 두는 것은 적절한 설계이고 서비스 게이트웨이를 확장할 수 있다. 하지만 모든 서비스 인스턴스 앞에 로드밸런서를 두는 것은 병목점이 될 수 있어 좋은 생각은 아니다.
- 작성하는 서비스 게이트웨이를 코드를 최대한 가볍고 무상태로 유지하자.



## 스프링 클라우드와 넷플릭스 주울

스프링 클라우드는 넷플릭스의 오픈 소스 프로젝트인 주울을 통합한다. 주울은 다음의 서비스 게이트웨이 기능을 제공한다.



- **어플리케이션의 모든 서비스 경로를 단일 URL로 매핑**: 주울의 매핑이 단일 URL로만 제한되는 것은 아니다 주울에서는 여러 경로 항목을 정의해 경로 매핑을 매우 세분화할 수 있다. 하지만 주울의 가장 일반적인 사용 사례는 모든 서비스 호출이 통과하는 단일 진입점을 구축하는 것이다.
- **게이트웨이로 유입되는 요청을 검사하고 대응할 수 있는 필터 작성**: 이러한 필터를 사용하면 코드에 정책시행지점을 주입해서 모든 서비스 호출에서 광범위한 작업을 일관된 방식으로 수행할 수 있다.



@EnableZuulProxy 또는 @EnableZuulServer를 사용한다.

@EnableZuulServer를 사용한다면 주울 리버스 프록시 필터를 로드하지 않은 주울 서버나 넥플릭스 유레카를 사용하는 주울 서버를 생성할 수 있다. @EnableZuulServer는 자체 라우팅 서비스를 만들고 내장된 주울기능도 사용하지 않을때 선택할 수 있다. 유레카가 아닌 서비스 디스커버리 엔진(consul 같은 .. )과 통합할 경우가 해당된다.



## 주울에서 경로 구성

주울은 리버스 프록시다. 리버스 프록시는 자원에 접근하려는 클라이언트와 자원 사이에 위차한 중개 서버다.  마이크로 서비스 아키텍처에서 주울은 ㅡㄹ라이언트에서 받은 마이크로서비스 호출을 하위 서비스에 전달한다. 주울이 하위 클라이언트와 통신하려면 유입되는 호출을 어떻게 하위 경로로 매핑할지 알아야 한다. 주울은 다음 세가지 메커니즘을 제공한다.

1. 서비스 디스커버리를 이용한 자동 경로 매핑
2. 서비스 디스커버리를 이용한 수동 경로 매핑
3. 정적 URL을 이용한 수동 경로 매핑



### 서비스 디스커버리를 이용한 자동 경로 매핑

만약 자동으로 호출하고 싶은 하위 서비스가 있다면 http://127.0.0.1:5555/{application-name}/{some-url} 로 호출이 가능하다. 127.0.0.1:5555로 주울 서버에 접근하고 다음 경로에 호출하려는 서비스 이름과 호출하려는 주소를 차례대로 적으면 된다. 이런 구성은 단일 엔드포인트를 구성할 수 있다는 장점을 제공할 뿐만 아니라 주울 수정 없이도 인스턴스를 추가하고 제거할 수 있는 장점이 있다. 예를 들어 유레카에 새로운 서비스를 추가하면 주울은 자동으로 이 서비스 인스턴스에 라우팅하는데 서비스 엔드포인트의 실제 물리적 위치를 유레카와 소통하고 있어서 가능하다.

주울 서버가 관리하는 경로를 보고싶다면 application.yml에 해당 기능을 설정하고 주울서비스에 /actuator/routes 로 확인할 수 있다이곳을 호출하면 서비스의 모든 매핑 목록을 반환한다. 



### 서비스 디스커버리를 이용한 수동 경로 매핑

주울을 사용하면 유레카에 등록된 applicationid 에 의존하지 않고 명시적으로 매핑 경로를 정의할 수 있기 때문에 더욱 세분화 할 수 있다. 위에서 설명한 {application-name}을 직접 설정 가능하다.

**application.yml**

```properties
zuul:
  ignored-services: 'someservice'
  prefix: api
  routes:
    someservice: /mycustomname/**
```

만약 유레카에 등록된 applicationid가 someservice라면 위 설정으로 /mycustomname/ 인 새로운 경로에 매핑 가능하다.

자동 경로 매핑을 사용했을때 요청할 수 있는 하위 인스턴스가 없을때 /actuator/routes로 요청하면 인스턴스 목록을 반환하지 않지만 수동 경로 매핑을 사용했을때는 하위 인스턴스 존재 여부에 상관 없이 항상 반환한다.

만약 ignored-services 에 정의 할 서비스가 많다면 `,`를 사용하면 되고 모두 적용하려면 `*`를 사용하면 된다.

만약 /api/{service-name}/ 과 같은 prefix 기능을 원한다면 zuul.prefix 설정을 통해 설정할 수 있다.



### 정적 URL을 이용한 수동 경로 매핑

만약 유레카로 관리하지 않는 서비스를 라우팅하는데도 주울을 사용할 수 있다. 주울은 고정 URL에 직접 라우팅하도록 설정할 수 있다.

```properties
zuul:
  routes:
    licensestatic:
      path: /licensestatic/**
      serviceId: licensestatic
ribbon:
  eureka:
    enabled: false
licensestatic:
  ribbon:
    listOfServers: http://licenseservice-static1:8081, http://licenseservice-static1:8082
```



### 경로 구성을 동적으로 로딩

스프링 클라우드 컨피그 서버를 쓴다는 가정하에 경로 구성을 동적으로 로딩할 수 있다. 스프링 클로우드로 주울 서버의 컨피그 정보를 설정하고 수정사항이 있을때마다 git에 커밋 후 주울의 POST /actuator/refresh를 호출하면 경로 구성을 동적으로 설정할 수 있다.



### 주울과 서비스 타임아웃

주울은 히스트릭스와 리본 라이브러리를 사용해 오래 수행되는 서비스 호출이 서비스 게이트웨이의 성능에 영향을 미치지 않도록 한다. 기본적으로 1초 이상이 걸리면 호출을 종료하고 HTTP 500 에러를 반환한다. 주울로 실행중인 모든 서비스에 대해 히스트릭스 타임아웃을 설정하려면 hystrix.command.default.excution.isolation.thread.timeoutInMilliseconds 로 설정하면 된다. 특정 서비스에 대해 별도로 설정하고 싶다면 hystrix.command.**someservice**.excution.isolation.thread.timeoutInMilliseconds 로 설정하면 된다. 이 설정 말고 만약 5초 이상 걸리는 서비스가 있다면 리본의 타임아웃 설정도 해야한다. {service-name}.ribbon.ReadTimeout 프로퍼티를 사용해 리본의 타임아웃을 재정의할 수 있다. 



## 주울의 진정한 힘! 필터

주울의 진정한 능력은 게이트웨이를 통과하는 모든 서비스 호출에 대해 사용자 정의 로직을 작성할 떄 드러난다. 이러한 사용자 정의 로직 대부분은 모든 서비스에 대한 보안과 로깅 및 추적 (횡단관심사)처럼 일관된 어플리케이션 정책을 시행하는데 사용된다. 주울은 J2EE 서블릿 필턴나 스프링의 애스펙트와 유사한 방식으로 사용되어 다양한 동작을 가로챈다. 주울은 다음 세가지 필터 타입을 지원한다.

- **사전 필터(pre-filter)**: 주울에서 목표 대상에 대한 실제 요청이 발생하기 전에 호출된다. 일반적으로 사전 필터는 서비스의 일관된 메시지 형식을 확인하는 작업을 수행하거나 서비스를 이용하는 사용자가 인증 및 인가 되었는지 확인하는 게이트키퍼 역할을 한다.
- **사후 필터(post filter)**: 대상 서비스를 호출하고 응답을 클라이언트로 전송한 후 호출된다. 일반적으로 사후 필터는 대상 서비스의 응답을 로깅하거나 에러 처리, 민감한 정보에 대한 응답을 감시하는 목적으로 구현된다.
- **경로 필터(route filter)**: 대상 서비스가 호출되기 전에 호출을 가로채는데 사용된다. 일반적으로 경로 필터는 일정 수준의 동적 라우팅 필요 여부를 결정하는데 사용된다. 예를 들어 동일 서비스의 다른 두 버전을 라우팅할 수 있는 경로 단위 필터를 사용해 작은 호출 비율만 새 버전의 서비스로 라우팅 할 수 있다. 이렇게 하면 모든 사용자가 새로운 서비스를 이용하지 않고도 소수 사용자에게 새로운 기능을 노출할 수 있따.... 개쩌네 ? 경로 필터는 주울 외부의 서비스로도 동적 라우팅할 수 있다.

서비스 클라이언트가 서비스 게이트웨이로 호출하면 다음 과정이 일어난다.

1. 요청이 주울 게이트웨이에 유입되면 정해진 사전 필터가 호출된다. 사전 필터는 HTTP 요청이 실제 서비스에 도달하기 전에 요청을 검사하고 수정한다. 사전 필터는 사용자를 다른 엔드포인트나 서비스로 향하게 할 수 없다.
2. 주울은 유입된 요청에 대해 사전 필터를 실행한 후 정해진 경로 필터를 실행한다. 경로 필터는 서비스가 향하는 목적지를 변경할 수 있다.
3. 경로 필터는 주울 서버가 전송하도록 구성된 경로가 아닌 다른 경로로 서비스 호출을 리다이렉션 하는 것도 가능하다. 하지만 주울의 경로 필터는 HTTP 리다이렉션 대신 유입된 HTTP요청을 종료한 후 원래 호출자를 대신해 그 경로로 호출한다. 이것은 경로로 필터가 동적 경로 호출을 완전히 소유해야하므로 HTTP 리다이렉션할 수 없다는 것을 의미한다.
4. 경로 필터가 호출자를 새로운 경로로 동적 리다이렉션하지 않는다면 주울 서버는 원래 대상 서비스의 경로로 보낸다.
5. 대상 서비스가 호출되었다면 주울의 사후 필터가 호출된다. 사후 필터는 호출된 서비스의 응답을 검사하고 수정할 수 있다.



## 상관관계 ID를 생성하는 주울의 사전 필터 작성

주울에서 필터를 구현할때는 ZuulFilter를 상속받아서 filterType(), filterOrder(), shouldFilter(), run() 등 4개의 메서드를 재정의해야 한다. 일반적으로 스프링 MVC에서 RequestContext는 org.springframework.web.servletsupport.RequestContext 타입이지만 주울에서는 특정 주울 정보에 액세스하기 위해 일부 메서드가 추가된 특별한 com.netflix.zuul.context.RequestContext를 제공한다. 

주울을 관통하는 모든 마이크로서비스 호출에 상관관계 ID 추가가 보장되도록 어플리케이션을 작성하면 다음 작업이 가능하다.

- 호출되는 마이크로서비스에서 상관관계 ID를 손쉽게 접근한다.
- 마이크로서비스가 호출하는 하위 서비스 호출에도 상관관계 ID를 전파한다.

이제 상관관계 ID가 각 서비스에 전달되므로 호출과 연관된 모든 서비스를 관통하는 트랜잭션을 추적할 수 있다.



## 상관관계 ID를 전달받는 사후 필터 작성

주울 사후필터는 사전필터로 데이터를 캡처하는 것과 연계할때 측정 지표를 수집하고 사용자 트랜잭션과 일관된 모든 로깅을 완료할 최적의 장소다. 마이크로서비스 사이 전달된 상솬관계 ID를 사용자에게 다시 전달해 이러한 이점을 얻을 수 있다. 



## 요약

- 마이크로서비스는 서비스 게이트웨이 구축을 단순화한다.
- 주울 서비스는 넷플릭스 유레카 서버와 통합하고 유레카에 등록된 서비스를 자동으로 주울 경로에 매핑한다.
- 주울은 관리하는 모든 경로 앞에 /api같은 prefix를 서비스 경로에 쉽게 추가할 수 있다.
- 주울을 사용하면 수동으로 경로 매핑을 정의할 수 있다.
- 이러한 경로 매핑은 어플리케이션 구성 파일에 정의한다.
- 스프링 클라우드 컨피그 서버를 사용해 주울 서버를 재시작하지 않고 경로 매핑을 동적으로 다시 로드할 수 있다.
- 주울의 히스트릭스와 리본 타님아웃을 전체 및 개별서비스 수준으로 사용자 정의할 수 있다.
- 주울 필터로 사용자 정의 비지니스 로직을 구현할 수 있다. 주울은 3개의 주울필터인 사전필터와 사후필터 경로필터를 제공한다.
- 주울의 사전 필터는 상관관계ID를 생성해 주울로 연결되는 모든 서비스에 주입한다.
- 주울의 사후 필터는 서비스 클라이언트에 대한 모든 HTTP 서비스 응답에 상관관계 ID를 삽입한다.
- 사용자 정의된 주울의 경로 필터는 유레카 서비스 ID 에 기반을 둔 동적 라우팅을 수행해 동일 서비스의 다른 버전에 대해 A/B 테스팅을 수행할 수 있다.



# #7 마이크로서비스의 보안

**※7장은 책을 직접 보는걸 추천합니다**



안전한 어플리케이션은 다음과 같은 여러 보호 계층을 포함한다.

- 사용자를 적절히 통제해 사용자 본인 여부를 확인하고 수행하려는 작업에 대한 권한 여부를 검증할 수 있다.
- 서비스가 실행되는 인프라스트럭처를 꾸준히 패치하고 최신 상태로 유지해 취약점의 위험을 최소화한다.
- 서비스는 명확히 정의된 포트로만 접근하고 소수의 인가된 서버만 접근할 수 있도록 네트워크 접근을 통제한다.

스프링 클라우드의 보안과 OAuth2 표준을 사용한다. OAuth2는 토큰 기반의 보안 프레임워크로 사용자가 제 3자 서비스에서 자신을 인증할 수 있다. 사용자는 인증에 성공하면 요청을 보낼 때마다  제시할 토큰을 전달받는다. 그런 다음 인증 서비스에서 토큰의 유효성을 확인할 수 있다. OAuth2의 주요 목표는 사용자 요청을 수행하기 위해 여러 서비스를 호출할 때 이 요청을 처리할 서비스에 일일이 자격 증명을 제시하지 않고도 사용자를 인증하는 것이다. 스프링 부트와 스프링 클라우드 모두 OAhth2 서비스의 기본 구현을 제공하기 때문에 OAuth2 보안을 매우 쉽게 서비스에 통합할 수 있다.

OAuth2의 진정한 힘은 어플리케이션 개발자가 쉽게 외부 클라우드 공급자와 통합할 수 있고, 사용자의 자격 증명을 외부업체의 서비스에 계쏙 전달하지 않고도 그 서비스에서 사용자 인증과 인가를 수행할 수 있다는 것이다.

## OAuth2 소개

OAuth2는 토큰 기반의 보안 인증과 인가 프레임워크로 다음 4개의 컴포넌트로 구성된다.

1. 보호 자원: 보호하려는 자원이며 적절한 권한을 부여받은 인증된 사용자만 액세스할 수 있다.
2. 자원 소유자: 서비스를 호출할 수 있는 어플리케이션 및 서비스에 접근할 수 있는 사용자, 그리고 서비스에서 수행할 수 있는 작접을 정의한다. 자원 소유자가 등록한 어플리케이션은 식별 가능한 어플리케이션 이름과 시크릿 키를 받는다. 어플리케이션 이름과 시크릿 키는 OAuth2 토큰을 인증할 떄 전달되는 자격 증명의 일부다.
3. 어플리케이션: 사용자를 대신해 서비스를 호출할 어플리케이션이다. 즉 사용자는 서비스를 직접 호출하지 않는 대신 어플리케이션에 의존해 작업을 수행한다.
4. OAuth2 인증 서버: 어플리케이션과 소비되는 서비스 사이의 중개자다. OAuth2 서버를 사용하면 어플리케이션이 사용자 대신 호출하는 모든 서비스에 사용자의 자격 증명을 전달하지 않고도 사용자 자신을 인증할 수 있다.



보호 자원은 OAuth2 서버에 접속해 토큰 유효성을 확인하고 사용자가 지정한 역할을 조회할 수 있다. 역할은 연관된 사용자를 함께 그룹으로 묶고 사용자 그룹이 액세스할 수 있는 자원을 정의한다. OAuth2를 사용하면 그랜트라는 인증 체계를 이용해 다양한 시나리오에서 REST기반 서비스를 보호할 수 있다. OAuth2 명세에는 다음 네가지 그랜트 타입이 있다.

1. 패스워드
2. 클라이언트 자격 증명
3. 인가 코드
4. 암시적

이책에서는 다음내용만 다룬다

- 단순한 OAuth2  그랜트 타입을 선택해 마이크로서비스 서비스가 OAuth2를 사용할 수 있는 방법을 논의한다.
- 자바스크립트 웹 토큰을 사용해 더 견고한 OAuth2 솔루션을 제공하고, OAuth2 토큰 정보를 인코딩하는 표준을 수립한다.
- 마이크로서비스를 구축할 떄 고려해야할 추가 보안 사항을 살펴본다.



OAuth2 패스워드 그랜트 타입의 구현

- 스프링 클라우드 기반의 OAuth2 인증 서비스를 설정한다.
- OAuth2 서비스와 사용자 신원을 인증 및 인가할 수 있도록 인가된 어플리케이션 역할을 하는 가짜 어플리케이션을 등록한다
- OAuth2 패스워드 그랜트 타입을 사용해 서비스를 보호한다. 가짜 어플리케이션은 POSTMAN으로 대체한다.
- 인증된 사용자만 호출할 수 있도록 다른 서비스들을 보호한다.



## OAuth2 인증 서버 설정하기



OAuth2 인증 서비스는 사용자 자격 증명을 인증하고 토큰을 발행하며 인증 서비스가 보호하는 서비스에 사용자가 접근할 때마다 자기가 발급한 OAuth2 토큰인지, 만료되지 않았는지 확인한다. 

- **spring-cloud-security** 모듈과 **spring-security-oauth2** 모듈을 추가해준다.
- @EnableAuthorizationServer를 부트스트랩 클래스에 반영한다. 
- 별도의 엔드포인트를 만든다 이 엔드포인트는 OAuth2로 보호되는 서비스에 접근하려고 할 때 사용할 것이다. 이 엔드포인트는 보호 서비스로 호출되어 OAuth2 액세스 토큰의 유효성을 검증하고 보호 서비스에 접근하는 할다오딘 사용자 역할을 조회한다.

@EnableAuthorizationServer은 스프링 클라우드에 서비스가 OAuth2 서비스로 사용되며 OAuth2 인증 및 인가 과정에서 사용될 여러 REST 기반 엔드포인트를 추가할 것이라고 알린다.



## 클라이언트 어플리케이션을 OAuth2 서비스에 등록하기

### 인증과 인가

- 인증: 자격 증명을 제공해 자신이 누구인지 증명하는 행위
- 인가: 사용자가 수행하려는 작업의 허용 여부를 결정



AuthorizationServerConfigurerAdapter 을 확장해서 구현한다. AuthorizationServerConfigurerAdapter 클래스는 스프링 시큐리티의 핵심부로 핵심 인증 및 인가 기능을 수행하는 기본 메커니즘을 제공한다. 









# #8 스프링 클라우드 스트림을 사용한 이벤트 기반 아키텍처

스프링 클라우드 프로젝트의 하위 프로젝트인 스프링 클라우드 스트림을 사용하면 메시지 기반 솔루션을 손쉽게 구축할 수 있다. EDA는 Event Driven Archtecture이다. 이런 EDA는 메시지를 사용해 상태 변화를 표현하는 이벤트로 통신 개념이다.

## 동기식 요청 응답 방식으로 상태 전달

1. 사용자가 서비스 A를 호출
2. 서비스 A는 먼저 B를 호출하기 이전에 B에서 얻어올 데이터를 레디스 캐시에서 획득
3. 레디스에 A가 원하는 데이터가 없다면 B는 A의 호출을 실행함
4. B의 데이터가 변경됨
5. B는 A에게 캐시를 무효화하라고 알려주거나 캐시와 직접 통신함

이런 방법에서 오는 단점이 몇가지 있다. 1~3 까지는 괜찮은데 4부터가 문제다

1. A와 B는 강하게 결합되어있음
2. 결합은 두 서비스 사이에 깨지기 쉬운 성질이 생김.
3. 레디스가 만약 B와도 통신한다면 다른 서비스가 소유하는 데이터 저장소와 통신하게 되므로 마이크로 서비스 환경에서 절대 금기해야한다. 비록 B에 속한 데이터라도 A는 그 데이터를 특정 컨텍스트에 사용, 변환한 비지니스 데이터로 만들 수 있기 때문이다.

## 메세지 기반의 아키텍처

이런 단점은 A와 B사이에 큐서버를 도입해서 완화 할 수 있다.

1. B서비스가 상태 변화를 전달할 때 큐에 메세지를 발행한다.
2. A는 B의 메시지 발행 여부를 큐에서 모니터링하고 필요할때는 레디스 캐시 데이터를 무효화한다.

이런식의 메세지큐를 도입한 아키텍쳐는 다음 네가지의 이점을 제공한다.

1. **느슨한 결합**: 마이크로서비스 어플리케이션들은 수많은 작은 서비스로 분산 되어 상호작용하기때문에  서비스사이에 강한 의존성을 만든다. 이 의존성을 완전히 제거할 수는 없지만 서비스가 소유한 데이터를 직접 관리하는 엔드포인트만 노출함으로 의존성을 최소화 할 수 있다. 메시징을 도입하면 두 서비스가 서로 알지 못하므로 결합되지 않는다. B서비스는 단순히 발행만하고 A서비스는 단순히 수신만하기 때문이다.
2. **내구성**: 큐가 존재하기 때문에 서비스 소비자가 다운되어도 메시지 전달을 보장할 수 있으며 서비스간의 직접적인 통신이 없기 때문에 구독자가 가동중이 아니더라도 메세지를 계속 발행할 수 있다.
3. **확장성**: 메세지가 큐에 저장되므로 발신자는 메세지의 소비를 기다릴 필요가 없고 소비자 역시 많은 발행이 있을 경우 수평적으로 확장하여 성능을 향상시킬 수 있다. 
4. **유연성**: 발신자는 누가 메세지를 소비하는지 알 수 없다. 즉 원래 발신 서비스에 영향을 주지 않고 새로운 메세지 소비자를 쉽게 추가할 수 있다. 이것은 기존 서비스를 건드리지 않고 새로운 기능을 어플리케이션에 추가할 수 있는 매우 강력한 개념이다. 

## 메세지 아키텍처의 단점

1. **메세지 처리의 의미론**: 마이크로서비스 기반의 어플리케이션에서 메세징을 사용하려면 메세지 큐에 대한 이해가 높아야한다. 시스템 설계단계에서 메세지가 실패하면 에러처리 등등의 내용은 깊이 생각해야할 내용이다.
2. **메세지 가시성**: 마이크로서비스에서 메세지 사용은 종종 동기식 서비스 호출과 서비스 내 비동기 처리가 합쳐진것을 의미한다. 메세지의 비동기적 특성으로 메세지가 발행되거나 소비될 때 메세지의 수신과 처리가 아주 근접한 곳에서 이루어질 필요는 없다. 웹 서비스의 호출과 메세징을 경유하는 사용자 트랜잭션을 추적하기 위해 상관관계 ID를 사용하는 것도 어플리케이션에서 발생하는 일을 이해하고 디버깅하는데 매우 중요하다. 
3. **메세지 코레오그래피**: 메시징 기반 어플리케이션은 코드가 더이상 단순한 요청 응답 모델을 사용해 선형적으로 처리되지 않기 때문에 어플리케이션 비지니스 로직을 추론하는 것은 더 어려워진다. 메시징 기반 어플리케이션의 디버깅은 사용자 크랜잭션이 순서가 바뀌고 다른 시점에 실행될 수 있는 다양한 서비스의 로그를 꼼꼼히 살펴보는 것이다.



## 스프링 클라우드 스트림 소개

스프링 클라우드를 사용하면 스프링 기반 마이크로서비스에 메시징을 쉽게 통합할 수 있다. 스프링 클라우드 스트림 프로젝트는 어플리케이션에 메세지 발행자와 소비자를 쉽게 구축할 수 있는 어노테이션 기반의 프레임워크이다. 스프링 클라우드 스트림은 메시징 플랫폼의 구현 세부 사항을 추상화해서 여러 메시지 플랫폼이 스프링 클라우드 스트림과 통합 될 수 있다. 



### 스프링 클라우드 스트림 아키텍쳐

스프링 클라우드 스트림에서 메세지를 발행하고 소비하는 데 다음 4가지 컴포넌트가 관련되어 있다.



서비스 A에서

비지니스 로직 -> 소스 -> 채널 -> 바인더 -> 스프링 클라우드 스트림 -> MQ 로 향하고

서비스 B에서는 반대로

MQ -> 스프링 클라우드 스트림 -> 바인더 -> 채널 -> 싱크 -> 비지니스 로직 순서대로 간다.



1. **소스**: 서비스가 메세지를 발행할 준비가 되면 소스를 사용해 메세지를 발행한다. 소스는 발행될 메세지를 표현하는 POJO를 전달받는 스프링의 어노테이션 인터페이스이다. 소스는 메세지를 받아 직렬화하고 메세지를 채널로 발행한다. 
2. **채널**: 채널은 메세지 생산자와 소비자가 메세지를 발행하거나 소비한 후 메세지를 보관할 큐를 추상화한 것이다. 채널 이름은 항상 대상 큐의 이름과 간련이 있지만 코드에서는 큐 이름을 직접 사용하지 않고 채널 이름을 사용한다. 따라서 채널이 읽거나 쓰는 큐를 전환하려면 어플리케이션 코드가 아닌 구성 정보를 변경한다.
3. **바인더**: 바인더는 스프링 클라우드 스트림 프레임워크의 일부인 스프링 코드로 특정 메세지 플랫폼과 통신한다. 스프링 클라우드 스트림의 바인더를 사용하면 메세지를 발행하고 소비하기 위해 플랫폼 마다 별도의 라이브러리와 api를 제공하지 않고도 메시징을 사용할 수 있다.
4. **싱크**: 스프링 클라우드 스트림에서 서비스는 싱크를 사용해 큐에서 메시지를 받는다. 싱크는 들어오는 메세지를 위해 채널을 수신 대기하고, 메세지를 다시 POJO로 직렬화한다. 이 과정에서 스프링 서비스의 비지니스 로직이 메세지를 처리할 수 있다.





## 스프링 클라우드 스트림 적용하기 (발행자)

spring-cloud-stream 모듈과 spring-cloud-start-stream-kafka 모듈을 추가한다. 부트스트래핑 클래스에서 @EnableBinding 어노테이션을 추가해서 해당 서비스를 메세지 브로커에 바인딩하도록 스프링 클라우드 스트림을 설정한다. @EnableBinding(Source.class)를 사용하면 해당 서비스가 Source 클래스에 정의된 채널들을 이용해 메세지 브로커와 통신하게 된다. 스프링 클라우드 스트림은 메세지 브로커와 통신할 수 있는 기본 채널이 있다.



특정 메세지 토픽에 대한 모든 통신은 채널이라는 스프링 클라우드 스트림 구조로 발생한다. 채널은 자바 인터페이스로 표현되며 Source 인터페이스를 사용한다. 이 인터페이스는 output() 이라는 단일 메서드를 노출하는데 서비스가 단일 채널에만 발행해야 할 때 사용하기에 편리한 인터페이스 이다. output() 메서드는 MessageChannel 클래스 ㅏ입을 반환하는데 이 클래스는 메세지 브로커에 메세지를 보내는 방법을 정의한다.



```
spring:
  cloud:
    stream:
      bindings:
        output:
            destination:  orgChangeTopic
            content-type: application/json
      kafka:
        binder:
          zkNodes: localhost
          brokers: localhost
```



spring.cloud.stream.bindings.ouput은 위에서 설명한 source.output() 채널을 통신하려는 메세지 브로커에 매핑한다. 스프링 클라우드 스트림은 json 및 xml 아파치 재단의 아브로 포맷을 포함한 다양한 포맷으로 메세지를 직렬화 할 수 있다.

spring.cloud.stream.kafka 구성 프로퍼티는 스프링 클라우드 스트림이 서비스를 카프카에 바인딩하도록 설정한다. 함께 실행되는 아파치 주키퍼도 마찬가지로 설정한다.

메세지 큐에는 되도록 변경된 데이터 사본을 넣는것은 비추한다. 되도록 pk만 넣도록 설정하고 변경된 내용이 필요하면 다시 통신하도록 구성하는게 좋다. 



## 스프링 클라우드 스트림 적용하기 (구독자)

spring-cloud-stream 모듈과 spring-cloud-start-stream-kafka 모듈을 추가한다. 부트스트래핑 클래스에서 @EnableBinding 어노테이션을 추가한다. 이 서비스는 소비자가 될 것이므로 @EnableBinding(Sink.class)를 적용한다. 스프링 클라우드 스트림에 기본 스프링 Sink 인터페이스를 사용해 메세지 브로커를 바인딩하라고 지정하는 것이다. Sink인터페이스는 input()메서드를 제공한다. 



```
spring:
  cloud:
    stream:
      bindings:
        inboundOrgChanges:
          destination: orgChangeTopic
          content-type: application/json
          group: licensingGroup
      kafka:
        binder:
          zkNodes: localhost
          brokers: localhost

#        input:
#          destination: orgChangeTopic
#          content-type: application/json
#          group: licensingGroup
```

프로퍼티 설정으로



```
//    @StreamListener(Sink.INPUT)
//    public void loggerSink(OrganizationChangeModel orgChange) {
//        logger.debug("Received an event for organization id {}", orgChange.getOrganizationId());
//    }
```

@StreamListener(Sink.INPUT) 을 통해 카프카의 메세지를 수신할때마다 로직을 수행한다.

group 프로퍼티는 메세지를 소비할 소비자 그룹의 이름을 정의한다. 소비자 그룹 개념은 동일한 메세지 큐를 수신하는 여러 서비스 모두 많은 인스턴스를 가지고 있다. 각각의 고유 서비스가 메세지 복사본을 처리하길 원하지만 서비스 인스턴스 그룹 안에서는 한 서비스 인스턴스만 메세지를 사용하고 처리해야 한다. group  프로퍼티는 서비스가 속한 소비자 그룹을 식별한다. 모든 서비스 인스턴스가 동일한 그룹 이름을 가지고 있는 한 스프링 클라우드 스트림과 하부 메시지 브로커는 해당 그룹에 속한 인스턴스에 메세지 복사본 하나만 소비할 것을 보장한다. 



## 스프링 클라우드 스트림 사용 사례: 분산 캐싱

레디스와 마이크로서비스를 적절하게 사용해서 오는 장점 세가지

1. **일반적으로 보유한 데이터 조회 성능을 향상시킨다**: 캐싱은 자주 사용되는 데이터를 고정하는 경향이 있어 레디스로 캐싱해 DB에 대한 읽기를 줄임으로써 성능 향상 가능
2. **데이터를 가진 db 테이블의 부하 및 비용을 줄인다**: db 액세스의 비용 문제를 레디스 서버를 통해 절감시킨다.
3. **회복성을 높여주는 데이터 저장소에 성능 문제가 있을때 적절히 성능을 저하시킬 수 있다**: 레디스 같은 캐시를 사용해 서비스 성능을 정상적으로 저하시킬 수 있다. 캐싱 솔루션은 캐시에 보관하는 데이터 용량에 따라 데이터 저장소를 히트할 떄 발생하는 에러 수를 줄인다.







## 요약

1. 메세징을 사용한 비동기식 통신은 마이크로서비스 아키텍처의 중요한 부분이다.
2. 어플리케이션에서 메시징을 ㅈ사용하면 서비스를 확장하고 결함에 더 잘 견디게 만들 수 있다.
3. 스프링 클라우드 스트림은 간단한 어노테이션을 사용하고 하부 메시지 플랫폼별 세부 정보를 추상화해 메시지 발행과 소비스를 단순화한다.
4. 스프링 클라우드 스트림의 메시지 소스는 어노테이션된 자바 메서드로 메시지 브로커 큐에 메시지를 발행한다.
5. 스프링 클라우드 스트림의 메시지 싱크는 어노테이션된 자바 메서드로 메시지 브로커에서 메시지를 수신한다.
6. 레디스는 데이터베이와 캐시로 사용될 수 있는 키- 값 저장소이다.



