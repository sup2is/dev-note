# github

- https://github.com/javacafe-project



# #1 검색 시스템 이해하기

## 검색 시스템의 이해

### 검색 시스템이란?

- 검색엔진은 광활한 웹에서 정보를 수집해 검색 결과를 제공하는 프로그램
- 검색시스템은 대용량 데이터를 기반으로 신뢰성 있는 검색 결과를 제공하기 위해 검색엔진을 기반으로 구축된 시스템
- 검색서비스는 검색 시스템을 활용해 검색 결과를 서비스로 제공
- 서비스 > 시스템 > 엔진
- 엘라는 검색엔진

### 검색 시스템의 구성 요소

#### 수집기

- 웹사이트, 블로그, 카페 등 웹에서 필요한 정보를 수집하는 프로그램 

#### 스토리지

- 데이터베이스에서 데이터를 저장하는 물리적인 저장소

#### 색인기

- 검색엔진이 사용자의 질의와 일치하는 정보를 찾으려면 수집되 ㄴ데이터를 검색 가능한 구조로 가공하고 저장해야 함 그 역할을 하는 것이 색인기임
- 다양한 형태소 분석기를 조합해 정보에서 의미가 있는 용어를 추출하고 검색에 유리한 역색인 구조로 데이터를 저장함

#### 검색기

- 사용자의 질의를 입력받아 색인기에서 저장한 역색인 구조에서 일치하는 문서를 찾아 결과로 반환함. 질의와 문서가 일치하는지도 유사도 기반의 검색 순위 알고리즘으로 판단함
- 형태소 분석기에 따라 검색 품질이 달라짐

### rdb와의 차이점

- rdb는 sql문을 이용해 원하는 정보의 검색이 가능한데 텍스트 매칭을 통한 단순한 검색만 가능함 텍스트를 여러 개의 동의어나 유의어를 활용한 검색은 불가능함
- 검색엔진은 rdb에서 불가능한 비정형 데이터를 색인하고 검색할 수 있음
- 형태소 분석을 통해 사람이 구사하는 자연어으 ㅣ처리가 가능해지고 역색인 구조를 바탕으로 빠른 검색 속도를 보장함



| 엘라      | rdb          |
| --------- | ------------ |
| 샤드      | 파티션       |
| 타입      | 테이블       |
| 문서      | 행           |
| 필드      | 열           |
| 매핑      | 스키마       |
| query dsl | sql          |
| 인덱스    | 데이터베이스 |



| 엘라 HTTP 메서드 | 기능               | RDB 문법       |
| ---------------- | ------------------ | -------------- |
| GET              | 조회               | SELECT         |
| PUT              | 생성               | INSERT         |
| POST             | 업데이트 조회      | UPDATE, SELECT |
| DELETE           | 삭제               | DELETE         |
| HEAD             | 인덱스의 정보 확인 | -              |



- curl -X(메섣) http://host:port/(인덱스)/(타입)/(문서 id) -d '{json data}' 가 간단한 요청 구조
- 기존 rdb에서 대소문자 검색이 불가능하던것을 엘라는 가능하게 해줌
- 비정형 데이터도 검색이 가능한것이 강점
- rdb는 스키마를 미리 정의해야만 데이터 저장과 조회가 가능하지만 엘라는 구조화되지 않은 데이터까지 스스로 분석해 자동으로 필드를 생성하고 저장함

## 검색 시스템과 엘라스틱 서치

- nosql의 한 종류
- 분산처리를 통해 실시간에 준하는 빠른 검색이 가능함
- 비정형 데이터도 검색 가능
- 전문 검색과 구조 검색 모두 지원
- 기본적으로 검색 엔진이지만 mongo나 hbase처럼 대용량 스토리지로도 활용 가능

### 엘라가 강력한 이유

- 오픈소스: 루씬을 기반으로 개발된 오픈소스 검색 엔진
- 전문 검색: 고차원적인 전문 검색이 가능함 전문검색이란 내용 전체를 색인해서 특정 단어가 포함된 문서를 검색하는것을 말함 기존 rdb는 전문 검색에 적합하지 않지만 엘라는 다양한 기능별 언어별 플러그인을 조합해 빠르게 검색 가능
- 통계 분석: 비정형 로그 데이터를 수집하고 한곳에 모아 통계 분석 가능 엘라와 키바나를 연결하면 실시간으로 쌓이는 로그를 시각화하고 분석 가능
- 스키마리스: 다양한 형태의 문서도 자동으로 색인하고 검색 가능
- restful api: 엘라는 api를 제공하고 json 타입을 사용함 <- 이기종 플랫폼 지원 가능
- 멀티 테넌시: 서로 상이한 인덱스일지라도 검색할 필드명만 같으면 여러개의 인덱스를 한번에 조회 가능
- document-oriented: 여러 계층형 데이터를 json 형식의 구조화된 문서로 인덱스에 저장 가능 & 쉽게 쿼리 가능
- 역색인: 루씬 기반의 검색 엔진. mongo, 칸산드라는 역색인 지원 x 역색인은 매우 큰 장점
- 확장성과 가용성: 샤드로 분리하고 인덱스를 만들때마다 샤드의 수를 조절 가능함

### 엘라스틱서치의 약점

- 실시간이 아니고 일반적으로 색인된 데이터는 1초 뒤에나 검색이 가능함 내부적으로 커밋과 플러시 같은 복잡한 과정을 거치기 때문에 실시간이 아님
- 트랜잭션과 롤백 기능이 없음 최악의 경우 데이터 손실의 위험이 있음
- 데이터의 업데이트를 제공하지 않음 업데이트가 필요하다면 지우고 다시 씀 <- 비용이 크지만 불변이라는 이득도 있어서 큰 단점은 아님



## 실습 환경 구축

- 도커로 설치함

```
version: '3'
services:
  elasticsearch:
    image: elasticsearch:6.5.0
    container_name: elasticsearch
    volumes:
      - /root/elastic/search_example:/usr/share/elasticsearch/search_example
      - /root/elastic/agg_example:/usr/share/elasticsearch/agg_example
      - /root/elastic/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
    ports:
      - 9200:9200
      - 9300:9300
    environment:
      discovery.type: single-node
```



```

curl localhost:9200

{
  "name" : "DcYVRdz",
  "cluster_name" : "docker-cluster",
  "cluster_uuid" : "DsypX1ktQcCIUW7xCrR8FA",
  "version" : {
    "number" : "6.5.0",
    "build_flavor" : "default",
    "build_type" : "tar",
    "build_hash" : "816e6f6",
    "build_date" : "2018-11-09T18:58:36.352602Z",
    "build_snapshot" : false,
    "lucene_version" : "7.5.0",
    "minimum_wire_compatibility_version" : "5.6.0",
    "minimum_index_compatibility_version" : "5.0.0"
  },
  "tagline" : "You Know, for Search"
}
```

- cluster_name은 엘라스틱서치 클러스터를 구분하는 중요한 속성임 
- 주요 설정 항목
  - cluster.name : 클러스터 명
  - node.name : 노드 명 설정안하면 알아서 생성
  - path.data : 인덱스 경로 설정안하면 엘라스틱 서치 하위의 data 디렉터리에 인덱스 생성
  - path.logs : 로그 기본경로는 /path/to/logs
  - path.repo : 엘라스틱서치 인덱스를 백업하기 위한 스냅숏의 겨올 지정
  - network.host : 특정 ip만 접근하도록 허용 가능 
  - http.port : 서버에 접근할 수 있는 포트 기본값은 9200
  - transport.tcp.port : 클라이언트가 접근할 수 있는 tcp 기본값은 9300
  - discovery.zen.ping.unicast.hosts: 노드가 여러개인 경우 유니캐스트로 활성화된 다른 서버를 찾음 
  - discovery.zen.minimum_master_nodes: 마스터 노드의 선출이 되는 노드의 수를 지정함.
  - node.master: 마스터 노드로 동작 여부 지정
  - node.data: 데이터 노드로 동작 여부 지정



```
 curl -XPUT 'http://localhost:9200/_snapshot/javacafe' -d '{"type":"fs", "settings": {"location": "/usr/share/elasticsearch/search_example", "compress":true}}' --header "content-Type:application/json"
 
  curl -XPUT 'http://localhost:9200/_snapshot/javacafe' -d '{"type":"fs", "settings": {"location": "/usr/share/elasticsearch/agg_example", "compress":true}}' --header "content-Type:application/json"
  
  
 curl -XGET 'http://localhost:9200/_snapshot/javacafe/_all' | python -m json.tool
 
 curl -XGET 'http://localhost:9200/_snapshot/apache-web-log/_all' | python -m json.tool
  
```

### 키바나 설치

- docker-compose로 함



```
version: '3'
services:
  elasticsearch:
    image: elasticsearch:6.5.0
    container_name: elasticsearch
    volumes:
      - /root/elastic/search_example:/usr/share/elasticsearch/search_example
      - /root/elastic/agg_example:/usr/share/elasticsearch/agg_example
      - /root/elastic/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
      #- /root/elastic/ela_data:/usr/share/elasticsearch/data
    ports:
      - 9200:9200
      - 9300:9300
    environment:
      discovery.type: single-node
  kibana:
    image: kibana:6.5.0
    container_name: kibana
    links:
      - elasticsearch
    volumes:
      - /root/elastic/kibana.yml:/usr/share/kibana/config/kibana.yml
      #- /root/elastic/kibana_data:/usr/share/kibana/data
    ports:
      - 80:5601
```



```
GET _search
{
  "query": {
    "match_all": {}
  }
}
```

- GET : 요청 전달 방식. post,put,delete 지정 가능 curl -X 옵션에 해당함
- _serach는 검색 쿼리를 의미함. _search 앞부분에 인덱스를 명시해서 해당 인덱스로만 범위를 한정 검색 수행 가능 여기서는 어떠한 인덱스도 지정하지 않음. size가 기본값 10이기때문에 10개 문서만 반환하므로  빠르게 반환함 
- 쿼리 본문에 해당하며 모든 문서를 검색함 curl 명령에서 -d 옵션에 해당함

```
#put으로 데이터 넣기

PUT movie-kibana_execute/_doc/1
{
  "message":"helloworld"
}


GET movie-kibana_execute/_search
{
  "query" : {
    "match_all" : {}
  }
}
```





# #2 엘라스틱 서치 살펴보기

## 엘라스틱서치를 구성하는 개념

### 기본용어

- **인덱스**
  - 데이터 저장 공간.
  - 하나의 인덱스는 하나의 타입만 가지며 하나의 물리적인 노드에 여러개의 논리적인 인덱스 설정 가능
  - 검색시 인덱스 이름으로 문서 데이터를 검색하고 여러 개의 인덱스를 동시에 검색도 가능
  - 엘라를 분산환경으로 구축하면 하나의 인덱스가 여러 노드에 분산 저장되어 관리
  - 엘라는 인덱스 생성시 기본적으로 5ㄱ개의 프라이머리 샤드와 1개의 레플리카 샤드 세트를 생성함
  - 인덱스의 이름은 모두 소문자, 작업은 restful api로 수행 가능
  - 만약 인덱스가 없는 상태에서 데이터가 추가된다면 데이터를 이용해 인덱스가 자동 생성됨
- **샤드**
  - 색인된 문서는 하나의 인덱스에 담김. 인덱스 내부에 색인된 데이터는 물리적인 공간에 여러 개의 파티션으로 나뉘어 구성됨 이 파티션을 엘라스틱서치에서는 샤드라고 부름 엘라에서는 다수의 샤드로 문서를 분산 저장하고 있어 데이터 손실 위험을 최소화 할 수 있음
- **타입**
  - 타입은 인덱스의 논리적인 구조를 의미, 인덱스 속성에 따라 분류하기도 함
  - 엘라 6.0이하 버전에서는 하나의 인덱스에 여러 타입을 설정 가능햇지만 인덱스당 하나의 타입만 사용 가능
- **문서**
  - 문서는 엘라에서 데이터가 저장되는 최소단위
  - 기본적으로 json 포맷
  - rdb와 비교하면 테이블의 행이 엘라스틱서치의 문서에 해당한다고 볼 수 있음
  - 하나의 문서는 다수의 필도로 구성되어 있고각 필드는 데이터의 형태에 따라 용도에 맞는 데이터 타입을 정의해야함
  - 문서는 중첩 구조를 지원하기 때문에 이를 이용해서 문서안에 문서 지정 가능
- **필드**
  - 문서를 구성하기 위한 속성
  - 일반적으로 컬럼과 비교할 수 있으나 컬럼이 정적인 데이터 타입에 반해 필드는 좀 더 동적인 데이터 타입이라고 할 수 있음
  - 하나의 필드는 목적에 따라 다수의 데이터 타입을 가질 수 있음 영화 정보를 담아둔 무서에 제목필드가 있다고 가정해보면 영화 제목을 검색할 때 매칭 검색을 하거나 초성을 이용한 검색이 모두 지원되도록 제목 필드는 2개의 데이터 타입을 가져야함
- **매핑**
  - 매핑은 문서의 필드와 필드의 속성을 정의하고 그에 따른 색인 방법을 정의하는 프로세스임
  - 인덱스의 매핑 정보에는 여러가지 데이터 타입을 지정할 수 있지만 필드명은 중복해서 사용 불가

### 노드의 종류

- 클러스터는 물리적인 노드 인스턴스들의 모임
- 클러스터는 모든 노드의 검색과 색인 작업을 관장하는 논리적인 개념
- 엘라는 다수의 서버로 분산해서 처리하는것이 가능하기 때문에 대용량 데이터 처리 가능
- 분산처리를 위해서는 다양한 형태의 노드들을 조합해서 클러스터를 구성해야 함
- 기본적으로 마스터 노드가 전체적인 클러스터를 고나리하고 데이터 노드가 실제 데이터를 관리함
- 마스터노드
  - 클러스터 관리
  - 노드 추가와 제거 같은 클러스터의 전반적인 관리
- 데이터 노드
  - 실질적인 데이터 저장
  - 검색과 통계 같은 데이터 관련 작업
- 코디네이팅 노드
  - 사용자의 요청만 받아서 처리
  - 클러스터 관련 요청은 마스터 노드에 전달하고 데이터 관련 요청은 데이터 노드에 전달함
- 인제스트 노드
  - 문서으 ㅣ전처리 작업을 담당함
  - 인덱스 생성 전 문서의 형식을 다양하게 변경 가능
- **마스터노드**
  - 마스터 노드는 인덱스 생성 삭제 등 클러스터와 관련된 전반적인 작업을 담당함
  - 네트워크 속도가 빠르고 지연이 없는 노드를 마스터 노드로 선성해야 함
  - 다수의 노드를 마스터로 설정할 수 있지만 결과적으로 하나의 노드만이 마스터로 선출되어 동작

```
note.master: true
node.data: false
node.ingest: false
search.remote.connect: false
```
- **데이터노드**
  - 문서가 실제로 저장되는 노드
  - 데이터가 실제로 분산 저장되는 물리적 공간인 샤드가 배치되는 노드
  - 색인 작업은 cpu, 메모리, tmxhflwl rkxdms zjavbxld flthtmfmf aksgdl thahgkrl Eoansdp flthtm ahslxjflddl vlfdygka
  - eaktmxj shemdhk qnsflgotj rntjdgksmsrp whgdma
  - tordlsgkf anstjrk wjrdmaus gkaRpgoeh anrhks

```
node.master: false
node.data: true
node.ingrst: false
search.remote.connect: false
```

- **코디네이팅 노드**
  - 데이터 노드, 마스터 노드, 인제스트 노드의 역할을 하지 않고 들어온 요청을 단순히 라운드로빈 방식으로 분산시켜주는 도구 

```
node.master: false
node.data: false
node.ingrst: false
search.remote.connect: false
```

- **인제스트 노드**
  - 색인에 앞서 데이터를 전처리하기 위한 노드.
  - 데이터 포맷을 변경하기 위해 스크립트로 전처리 파이프라인을 구성하고 실행할 수 있음

```
node.master: false
node.data: false
node.ingrst: true
search.remote.connect: false
```

### 클러스터, 노드, 샤드

- 책 확인





## 엘라스틱서치에서 제공하는 주요 api

- api종류 : 엘라는 rest 방식의 api를 제공하고 json 기반으로 통신함
  - 인덱스 관리 api
  - 문서 관리 api
  - 검색 api
  - 집계 api
- 문서를 색인하기 위해서는 기본적으로 인덱스라는 그릇을 생성해야 함.
- 인덱스를 통해 입력되는 문서의 필드를 정의하고 각 필드에 알맞은 데이터 타입을 지정할 수 있음.
- 엘라에서 색인은 index, 매핑 정의 공간을 의미할 경우 indices라는 단어로 표현함
- 엘라는 사용 편의성을 위해 스키마리스라는 기능을 제공함 이 기능은 기본적으로 문서를 색인하기 위해서는 인덱스를 생성하는 과정이 필요한데 인덱스를 생성하는 과정 없이 문서를 추가하더라도 문서가 색인되도록 지원하는 일종의 편의 기능임 엘라는 최초 문서가 색인될 때 인덱스의 존재 여부를 확인하고 만약 인덱스가 존재핮 ㅣ않는다면 문서를 분석해서 문서가 색인될 수 잇게 인덱스를 자동으로 생성함
- **스키마리스 기능은 가급적이면 사용하지 말것** 특수한 상황에서만 제한적으로 사용해야 함
- 원하는 결과를 얻기 위해서라도 스키마리스 방식의 사용을 지양하고 반드시 인덱스를 직접 정의해서 사용하는 습관을 들이는 것이 좋음
- 스키마리스를 사용하고 싶지 않더라도 실수로 인덱스를 설정하지 않고 데이터를 색인하면 엔덱스를 자동생성함  노드 설정 파일에서 action.auth-create_index를 false로 하면 됨

### 인덱스 관리 api

- **인덱스 생성**
  - 인덱스를 생성할때 매핑이라는 세부 설정을 이용할 수 있는데 매핑은 문서와 문서에 포함된 필드, 필드타입 등을 세세하게 지정하는 것이 가능한 설정 방식임. 인덱스 생성시 이러한 매핑 정보를 추가할 수 있음
  - 한가지 주의할 점은 한번 생성된 매핑 정보는 변경할 수 없음 만약 잘못 생성했거나 변경해야 하는 경우엔 데이터 삭제 후 다시 색인해야 함

```
PUT /movie
{
  "settings": {
    "number_of_shards": 3,
    "number_of_replicas": 2
  },
  "mappings": {
    "_doc": {
      "properties": {
        "movieCd": {"type": "integer"},
        "movieNm": {"type": "text"},
        "movieNmEn": {"type": "text"},
        "prdtYear": {"type": "integer"},
        "openDt": {"type": "date"},
        "typeNm": {"type": "keyword"},
        "prdtStatNm": {"type": "keyword"},
        "nationAlt": {"type": "keyword"},
        "genreAlt": {"type": "keyword"},
        "repNationNm": {"type": "keyword"},
        "repGenreNm": {"type": "keyword"}
      }
    }
  }
}






{
  "acknowledged" : true,
  "shards_acknowledged" : true,
  "index" : "movie"
}

```

- 단순 문자열은 keyword, 형태소 분석을 원할 경우 text 타입을 사용

```
#인덱스 삭제

DELETE /movie

{
  "acknowledged" : true
}

```

- 인덱스 삭제는 복구 불가함 신중하게 할 것

### 문서 관리 api

- index api: 한건의 문서를 색인
- get api: 한 건의 문서를 조회
- delete pai: 한 건의 문서를 삭제
- update api: 한 건의 문서를 업데이트
- single document api
- multi get api: 다수의 문서를 조회
- bulk api: 대량의 문서를 색인
- delete by query api: 다수의 문서를 삭제
- update by query api: 다수의 문서를 업데이트
- reindex api: 인덱스의 문서를 다시 색인함

- **문서 생성**

```
POST /movie/_doc/2
{
    "movieCd": "2",
    "movieNm": "살아남은 아이2",
    "movieNmEn": "Last Child2",
    "prdtYear": "2017",
    "openDt": "2017-10-20",
    "typeNm": "장편",
    "prdtStatNm": "기타",
    "nationAlt": "한국",
    "genreAlt": "드라마,가족",
    "repNationNm": "한국",
    "repGenreNm": "드라마"
}


{
  "_index" : "movie",
  "_type" : "_doc",
  "_id" : "2",
  "_version" : 1,
  "result" : "created",
  "_shards" : {
    "total" : 3,
    "successful" : 1,
    "failed" : 0
  },
  "_seq_no" : 0,
  "_primary_term" : 1
}


```

- **문서 조회**

```
GET /movie/_doc/2

{
  "_index" : "movie",
  "_type" : "_doc",
  "_id" : "2",
  "_version" : 1,
  "found" : true,
  "_source" : {
    "movieCd" : "2",
    "movieNm" : "살아남은 아이2",
    "movieNmEn" : "Last Child2",
    "prdtYear" : "2017",
    "openDt" : "2017-10-20",
    "typeNm" : "장편",
    "prdtStatNm" : "기타",
    "nationAlt" : "한국",
    "genreAlt" : "드라마,가족",
    "repNationNm" : "한국",
    "repGenreNm" : "드라마"
  }
}

```

- **문서 삭제**

```
DELETE /movie/_doc/1

{
  "_index" : "movie",
  "_type" : "_doc",
  "_id" : "1",
  "_version" : 7,
  "result" : "deleted",
  "_shards" : {
    "total" : 3,
    "successful" : 1,
    "failed" : 0
  },
  "_seq_no" : 6,
  "_primary_term" : 1
}

```

- id를 지정하지 않고 문서를 생성

```
POST /movie/_doc
{
    "movieCd": "2",
    "movieNm": "살아남은 아이2",
    "movieNmEn": "Last Child2",
    "prdtYear": "2017",
    "openDt": "2017-10-20",
    "typeNm": "장편",
    "prdtStatNm": "기타",
    "nationAlt": "한국",
    "genreAlt": "드라마,가족",
    "repNationNm": "한국",
    "repGenreNm": "드라마"
}



{
  "_index" : "movie",
  "_type" : "_doc",
  "_id" : "rKkruHMB3Tk3JWb8I_0z",
  "_version" : 1,
  "result" : "created",
  "_shards" : {
    "total" : 3,
    "successful" : 1,
    "failed" : 0
  },
  "_seq_no" : 1,
  "_primary_term" : 1
}

```

- id를 uuid 값 무작위로 생성함 비추천 색인된 문서의 _id 값은 업데이트를 고려해서 데이터베이스 테이블의 식별 값과 맞춰준ㄴ 것이 중요함

### 검색 api

- 엘라 검색 api의 사용 방식은 두가지임
  1. http uri 형태의 파라미터를 uri에 추가해 검색하는 방법
  2. rest api 방식인 querydsl을 사용해 요청 본문에 질의 내용을 추가해 검색하는 방법
- 2번을 자주 사용함

```
# 1, 2 둘다 섞은 형태

GET /movie/_doc/_search?q=prdtYear:2017&pretty=true
{
  "sort" : {
    "movieCd" : {
      "order" : "asc"
    }
  }
}
```

- querydsl을 사용하면 가독성이 높고 json 형식으로 다양한 표현이 가능해짐
- 통계를 위한 집계는 등 복잫ㅂ한 쿼리는 querydsl을 사용하는 것이 좋음

- **uri 방식의 검색 질의**
  - 문서 id는 _id 값을 사용해 문서를 조회하는 방식

```
GET /movie/_doc/rKkruHMB3Tk3JWb8I_0z



{
  "_index" : "movie",
  "_type" : "_doc",
  "_id" : "rKkruHMB3Tk3JWb8I_0z",
  "_version" : 1,
  "found" : true,
  "_source" : {
    "movieCd" : "2",
    "movieNm" : "살아남은 아이2",
    "movieNmEn" : "Last Child2",
    "prdtYear" : "2017",
    "openDt" : "2017-10-20",
    "typeNm" : "장편",
    "prdtStatNm" : "기타",
    "nationAlt" : "한국",
    "genreAlt" : "드라마,가족",
    "repNationNm" : "한국",
    "repGenreNm" : "드라마"
  }
}


#p 파라미터 사용

POST /movie/_search?q=장편
POST /movie/_search?q=typeNm:장편

```

- **Request Body 방식의 검색 질의**
  - uri 검색 질의는 여러 필드를 각기 다른 검색어로 질의하는 것이 어려움 쿼리의 조건이 복잡하고 길어지기 때문임 이럴 때는 json 방식으로 질의하면 됨

```
POST /movie/_search
{
  "query": {
    "term" : {"typeNm": "장편"}
  }
}


{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 3,
    "successful" : 3,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 0.18232156,
    "hits" : [
      {
        "_index" : "movie",
        "_type" : "_doc",
        "_id" : "2",
        "_score" : 0.18232156,
        "_source" : {
          "movieCd" : "2",
          "movieNm" : "살아남은 아이2",
          "movieNmEn" : "Last Child2",
          "prdtYear" : "2017",
          "openDt" : "2017-10-20",
          "typeNm" : "장편",
          "prdtStatNm" : "기타",
          "nationAlt" : "한국",
          "genreAlt" : "드라마,가족",
          "repNationNm" : "한국",
          "repGenreNm" : "드라마"
        }
      },
      {
        "_index" : "movie",
        "_type" : "_doc",
        "_id" : "rKkruHMB3Tk3JWb8I_0z",
        "_score" : 0.18232156,
        "_source" : {
          "movieCd" : "2",
          "movieNm" : "살아남은 아이2",
          "movieNmEn" : "Last Child2",
          "prdtYear" : "2017",
          "openDt" : "2017-10-20",
          "typeNm" : "장편",
          "prdtStatNm" : "기타",
          "nationAlt" : "한국",
          "genreAlt" : "드라마,가족",
          "repNationNm" : "한국",
          "repGenreNm" : "드라마"
        }
      }
    ]
  }
}


```



- size : 몇 개의 결과 를반환할지 결정 기본값 10
- from : 어느 위치부터 반호나할지 결정 0부터 시작하면 상위 0 기본값 0
- _source : 특정 필드만 결과로 반환하고 싶을 때
- sort :  특정 필드를 기준으로 정렬 asc, desc
- query : 검색될 조건 설정
- filter : 검색 결과 중 특정한 ㄱ밧을 다시 보여줌, 결과 내에서 재검색할때 사용하는 기능중 하나 필터를 사용하면 자동으로 score 값이 정렬되지 않음

### 집계 api

- 엘라는 과거에 루씬의 패싯 API를 사용했었음 

- **데이터 집계**

```
POST /movie/_search?size=0
{
  "aggs": {
    "genre": {
      "terms": {
        "field":"genreAlt"
      }
    }
  }
}



{
  "took" : 3,
  "timed_out" : false,
  "_shards" : {
    "total" : 3,
    "successful" : 3,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : 2,
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "genre" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 0,
      "buckets" : [
        {
          "key" : "드라마,가족",
          "doc_count" : 2
        }
      ]
    }
  }
}

```

- 버킷이라는 구조 안에 그룹된 데이터가 포함됨
- 엘라가 강력한 이유는 버킷 안에 다른 버킷의 결과를 추가할 수 있음 중첩 조합 가능

```
POST /movie/_search?size=0
{
  "aggs": {
    "genre": {
      "terms": {
        "field":"genreAlt"
      },
      "aggs": {
        "nation": {
          "terms": {
            "field": "nationAlt"
          }
        }
      }
    }
  }
}

```

## 데이터 집계 타입

- 집계는 현재 4가지 api 로 제공
  - 버킷집계 : 집계중 가장 많이사용 문서의 필드를 기준으로 버킷을 집계
  - 메트릭 집계: 문서에서 추출된 값을 가지고 sum, max, min, avg를 계산함
  - 매트릭스 집계 : 행렬의 값을 합하거나 곱함
  - 파이프라인 집계: 버킷에서 도출된 결과 문서를 다른 필드의 값으로 재분류함. 즉 다른 집계에 의해 성성된 출력 결과를 다시한번 집계함 집계가 패싯보다 강력한 이유가 여기에 있음







# #3 데이터 모델링

- 색인할 때 문서의 데이터 유형에 따라 필드에 적절한 데이터 타입을 지정해야 함 이러한 과정을 매핑이라고 하며 매핑은 색인될 문서의 데이터 모델리이라고도 할 수 있음 사전에 매핑을 설정하면 지정된 데이터 타입으로 색인되지만 매핑을 설정해두지 않으면 엘라가 자동으로 필드를 생성하고 필드 타입까지 결정함 <- 운영에서 에러날수도 있음

## 매핑 api 이해하기

- 매핑은 색인 시 데이터가 어디에 어떻게 저장될지를 결정하는 설정임
- 인덱스에 추가되는 각 데이터 타입을 구체적으로 정의하는 일
- 문서에 존재하는 필드의 속성을 정의할 때 각 필드 속성에는 데이터 타입과 메타데이터가 포함됨 이를 통해 색인 과정에서 문서가 어떻게 역색인으로 변환되는지를 상세하게 정의 가능
- rdb에서 테이블의 컬럼 정보를 정의하는 것 만큼 엘라에서도 데이터의 타입을 정의하는 것이 매우 중요함
- 스키마리스는 웬만하면 사용하지 말 것
- 한번 생성된 매핑의 타입은 변경할 수 없음
- 매핑 정보를 설정할 때 고려사항
  - 문자열을 분석할 것인가?
  - _source에 어떤 필드를 정의할 것인가?
  - 날짜 필드를 가지는 필드는 무엇인가?
  - 매핑에 정의되지 않고 유입되는 필드는 어떻게 처리할 것인가?
- 실무에서는 동적 매핑을 거의 사용하지 않음

### 매핑 인덱스 만들기



```
PUT movie_search
{
  "settings": {
    "number_of_shards": 5,
    "number_of_replicas": 1
  },
  "mappings": {
    "_doc": {
      "properties": {
        "movieCd": {
          "type": "keyword"
        },
        "movieNm": {
          "type": "text",
          "analyzer": "standard"
        },
        "movieNmEn": {
          "type": "text",
          "analyzer": "standard"
        },
        "prdtYear": {
          "type": "integer"
        },
        "openDt": {
          "type": "integer"
        },
        "typeNm": {
          "type": "keyword"
        },
        "prdtStatNm": {
          "type": "keyword"
        },
        "nationAlt": {
          "type": "keyword"
        },
        "genreAlt": {
          "type": "keyword"
        },
        "repNationNm": {
          "type": "keyword"
        },
        "repGenreNm": {
          "type": "keyword"
        },
        "companies": {
          "properties": {
            "companyCd": {
              "type": "keyword"
            },
            "companyNm": {
              "type": "keyword"
            }
          }
        },
        "directors": {
          "properties": {
            "peopleNm": {
              "type": "keyword"
            }
          }
        }
      }
    }
  }
}


{
  "acknowledged" : true,
  "shards_acknowledged" : true,
  "index" : "movie_search"
}


```

### 매핑 확인

- 이미 만들어진 매핑은 _mapping api를 통해 확인 가능

```
GET movie_search/_mapping



{
  "movie_search" : {
    "mappings" : {
      "_doc" : {
        "properties" : {
          "companies" : {
            "properties" : {
              "companyCd" : {
                "type" : "keyword"
              },
              "companyNm" : {
                "type" : "keyword"
              }
            }
          },
          "directors" : {
            "properties" : {
              "peopleNm" : {
                "type" : "keyword"
              }
            }
          },
          "genreAlt" : {
            "type" : "keyword"
          },
          "movieCd" : {
            "type" : "keyword"
          },
          "movieNm" : {
            "type" : "text",
            "analyzer" : "standard"
          },
          "movieNmEn" : {
            "type" : "text",
            "analyzer" : "standard"
          },
          "nationAlt" : {
            "type" : "keyword"
          },
          "openDt" : {
            "type" : "integer"
          },
          "prdtStatNm" : {
            "type" : "keyword"
          },
          "prdtYear" : {
            "type" : "integer"
          },
          "repGenreNm" : {
            "type" : "keyword"
          },
          "repNationNm" : {
            "type" : "keyword"
          },
          "typeNm" : {
            "type" : "keyword"
          }
        }
      }
    }
  }
}

```

### 매핑 파라미터

- **analyzer**
  - 해당 필드의 데이터를 형태소 분석하겠다는 의미의 파라미터. 색인과 검색 시 지정한 분석기로 형태소 분석을 수행함. text데이터 타입의 필드는 anlyzer 매핑 파라미터를 기본적으로 사용해야 함 별도의 분석기를 지정하지 않으면 Standard Anlyzer로 형태소 분석을 수행
- **normalizer**
  - term query에 분석기를 사용하기 위해 사용됨. 예를 들어 keyowrd 데이터 타입의 경우 원문을 기준으로 문서가 색인되기 때문에 caef, Cafe는 서로 다른 문서로 인섹됨 하지만 해당 유형을 normalizer를 통해 분석기에 asciifolding과 같은 필터를 사용하면 같은 데이터로 인식시킬 수 있음
- **boost**
  - 필드에 가중치를 부여함. 가중체아 따라 유사도 점수가 달라지기 때문에 boost 서렂ㅇ 시 검색 결과의 노출 순서에 영향을 줌. 만약 색인 시점에 boost 설정을 하게 된다면 재색인 하지 않는 이상 가중치를 변경할 수 없기 때문에 주의해서 사용해야 함 가급적 검색 시점에서만 사용하는 것을 권장 <- 최근 제거@@ 7버전인듯
- **coerce**
  - 색인시 자동 변환을 허용할지 여부를 설정하는 파라미터. 예를 들어 "10"과 같은 숫자 형태의 문자열이 integer 타입의 필드에 들어온다면 자동으로 형변환함 하지만 coerce설정이 미사용이면 색인 실패
- **copy_to**
  - 매핑 파라미터를 추가한 필드의 값을 지정한 필드로 복사함. keyword 타입의 필드에 text 타입 필드로 copy_to를 지정하면 text타입을 사용해 형태소 분석 가능
  - 필드를 묶어서 사용하는것도 가능
- **fielddata**
  - 엘라가 힙공간에 생성하는 메모리 캐시임 거의 사용하지 않고 최근엔 doc_values라는 새로운 형태의 캐시를 제공하고 있음 text 타입 필드를 제외한 모든 필드는 기본적으로 doc_values 캐시를 사용함
  - 사용해야 할 경우도 있음
- **doc_values**
  - 엘라에서 사용하는 기본 캐시. text타입을 제외한 모든 타입에서 기본적으로 사용함
  - 루씬을 기반으로 하는 캐시 방식. 
  - 과거에는 캐시를 모두 메모리에 올렸으나 힙 사용에 대한 부담을 없애고 운영체제의 파일 시스템 캐시를 통해 디스크에 있는 데이터에 빠르게 접근 가능함
  - 비활성화 시킬 수 있으나 다시 하려면 재색인해야함
- **dynamic**
  - 매핑에 필드를 추가할 때 동저긍로 생성할지 생성하지 않을지를 결정함 동적 생성 필드의 처리 방법으로 다음의 세가지 설정중 하나를 지정할  수 있음
  - true : 세로 추가되는 필드를 매핑에 추가함
  - false : 새로 추가되는 필드를 무시함. 검색 불가지만 _source에는 표시
  - stirct : 새로운 필드가 ㅊ감지되면 예외가 발생하고 문서 자체가 색인되지 않음 새로 유입되는 필드는 명시적으로 매핑에 추가해야함
- **enabled**
  - 검색 결과에는 포함하지만 색인하고 싶지 않을때 ex: 메타 성격의 데이터
  - 일반적인 게시판이라면 제목과 요약글만 색인하고 날짜와 사용자 id는 색인하지 않음
  - enable을 false로 지정하면 _source에선 검색이되지만 색인은 X
- **format**
  - 엘라에서는 날짜 시간을 문자열로 표기함 포매팅할 수 있음
  - 자세한건 찾아볼것
- **ignore_above**
  - 필드에 저장되는 문자열이 지정한 크기를 넘어서면 빈 값으로 색인함.
  - 지정한 크기가 넘어가면 빈값이되니 주의할것
- **ignore_malformed**
  - 엘라에서는 잘못된 데이터 타입을 색인하려고 하면 예외가 발생하고 문서 전체가 색인되지 않음 이 매핑파라미터를 사용하면 해당 필드만 무시하고 문서는 색인 가능함
- **index**
  - 필드값을 색인할지 결정함 기본값은 true이고 false로 변경하며 ㄴ해당 필드를 색인하지 않음
  - enabled랑 뭐가다름 ?
- **fields**
  - 다중 필드를 설정할 수 있는 옵션. 필드 안에 또 다른 필드의 정보를 추가할 수 있어 같은 string 값을 각가 다른 분석기로 처리하도록 설정할 수 있음
- **norms**
  - 문서의 _score 값 계산에 필요한 정규화 인수를 사용할지 여부를 설정함 기본값은 true
  - _score 계산이 필요 없거나 단순 필터링 용도로 사용하는 필드는 비활성화해서 디스크 공간 절약 가능\
- **null_value**
  - 엘라에서 색인 시 문서에 필드가 없거나 필드의 값이 null이면 색인 시 필드를 생성하지 않음
  - 이 경우 null_value를 설정하면 문서의 값이 null이라도 필드를 생성하고 그에 해당하는 값으로 저장함
- **position_increment_gap**
  - 배열 형태의 데이터를 색인할 때 검색의 정확도를 높이기 위해 제공하는 옵션, 필드 데이터 중 단어와 단어 사이의 간격을 허용할지를 설정함.
  - 검색 시 단어와 단어 사이의 간격을 기준으로 일치하는 문서를 찾는데 필요함 예를들어 데이터가 ["aaa bbb", "ccc ddd"]일때 bbb ccc 으로 검색해도 검색 가능
- **properties**
  - 오브젝트 타입이나 중첩 타입의 스키마를 정의할 때 사용되는 옵션으로 필드 타입을 매핑함
  - 오브젝트 필드 및 중첩 필드네는 properties라는 서브 필드가 있음
  - 이 properties는 object나 nested를 포함한 모든 데이터 타입이 될 수 있음
- **search_analyzer**
  - 일반적으로 색인과 검색 시 같은 분석기를 사용함. 만약 다른 분석기를 사용하고 싶은 경우 search_analzer를 설정해서 검색 시 사용할 분석기를 별도로 지정할 수 있음
- **similarity**
  - 유사도 측정 알고리즘을 지정함 유사도 측정 방식을 기본 알고리즘인 BM25에서 다른 알고리즘으로 변경 가능
- **store**
  - 필드의 값을 저장해 검색 결과에 값을 포함하기 위한 매핑 파라미터. 기본적으로 엘라는 _source에 색인된 문서가 저장됨. 하지만 store 매핑 파라미터를 사용하면 해당 필드를 자체적으로 저장할 수 있음. 예를 들어 10개의 필드가 존재하고 해당 필드에 데이터를 매핑한 상태라면 _source를 로드해서 해당 필드를 찾는 것 보다 사용할 각 필드만 로드해서 사용하는 편이 효율적임
  - 많이 쓰면 디스크 비용 높음
- **term_vector**
  - 루씬에서 분석된 용어의 정보를 포함할지 여부를 결정하는 파라미터
  - no, yes, with_positions, with_offsets, with_position_offsets

## 메타 필드

- 엘라에서 생성한 문서에 제공하는 특별한 필드

### _index 메타필드

- 해당 문서가 속한 인덱스의 이름을 담고 있음 

```
#집계로 카운트 정보 가져오기

# 요청
POST moive_search/_search
{
 "size":0,
  "aggs": {
    "indices": {
      "terms": {
        "field": "_index",
        "size": 10
      }
    }
  }
}

# 결과
{
  "took": 13,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": 29507,
    "max_score": 0,
    "hits": [

    ]
  },
  "aggregations": {
    "indices": {
      "doc_count_error_upper_bound": 0,
      "sum_other_doc_count": 0,
      "buckets": [
        {
          "key": "moive_search",
          "doc_count": 29507
        }
      ]
    }
  }
}

```

### _type 메타 필드

- 해당 문서가 속한 매핑의 타입 정보를 담고 있음 이를 이용해 해당 인덱스 내부에서 타입 별로 몇개의 문서가 있는지 확인 가능

```
# 요청
POST movie_search/_search
{
 "size":0,
  "aggs": {
    "indices": {
      "terms": {
        "field": "_type",
        "size": 10
      }
    }
  }
}

# 결과
{
    "took": 15,
    "timed_out": false,
    "_shards": {
        "total": 5,
        "successful": 5,
        "skipped": 0,
        "failed": 0
    },
    "hits": {
        "total": 63069,
        "max_score": 0,
        "hits": []
    },
    "aggregations": {
        "indices": {
            "doc_count_error_upper_bound": 0,
            "sum_other_doc_count": 0,
            "buckets": [
                {
                    "key": "_doc",
                    "doc_count": 63069
                }
            ]
        }
    }
}
```



### _id 메타 필드

- 문서를 식별하는 유일한 키 값 

```
# 요청
POST movie_search/_search
{
 "size":0,
  "aggs": {
    "indices": {
      "terms": {
        "field": "_id",
        "size": 10
      }
    }
  }
}

# 결과
{
    "took": 5,
    "timed_out": false,
    "_shards": {
        "total": 5,
        "successful": 5,
        "skipped": 0,
        "failed": 0
    },
    "hits": {
        "total": 63069,
        "max_score": 0,
        "hits": []
    },
    "aggregations": {
        "indices": {
            "doc_count_error_upper_bound": 5,
            "sum_other_doc_count": 63059,
            "buckets": [
                {
                    "key": "-D3JqmkBjjM-ebDb-U-P",
                    "doc_count": 1
                },
                {
                    "key": "-D3JqmkBjjM-ebDb-U2P",
                    "doc_count": 1
                },
                {
                    "key": "-D3JqmkBjjM-ebDb-U6P",
                    "doc_count": 1
                },
                {
                    "key": "-D3JqmkBjjM-ebDb-UmO",
                    "doc_count": 1
                },
                {
                    "key": "-D3JqmkBjjM-ebDb-UqP",
                    "doc_count": 1
                },
                {
                    "key": "-D3JqmkBjjM-ebDb-UuP",
                    "doc_count": 1
                },
                {
                    "key": "-D3JqmkBjjM-ebDb-UyP",
                    "doc_count": 1
                },
                {
                    "key": "-D3JqmkBjjM-ebDb-V-P",
                    "doc_count": 1
                },
                {
                    "key": "-D3JqmkBjjM-ebDb-V2P",
                    "doc_count": 1
                },
                {
                    "key": "-D3JqmkBjjM-ebDb-V6P",
                    "doc_count": 1
                }
            ]
        }
    }
}
```

### _uid 메타 필드

- #태그를 사용해 _type과 _id 값을 조합해서 사용하는 특수한 필드 내부적으로만 사용되서 검색은 안됨

```
# 요청
POST movie_search/_search
{
 "size":0,
  "aggs": {
    "indices": {
      "terms": {
        "field": "_uid",
        "size": 10
      }
    }
  }
}

# 결과
{
  "took": 182,
  "timed_out": false,
  "_shards": {
    "total": 5,
    "successful": 5,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": 29507,
    "max_score": 0,
    "hits": [

    ]
  },
  "aggregations": {
    "indices": {
      "doc_count_error_upper_bound": 5,
      "sum_other_doc_count": 29497,
      "buckets": [
        {
          "key": "_doc#20173732",
          "doc_count": 1
        }
      ]
    }
  }
}
```

### _source 메타 필드

- 문서의 원본 데이터를 제공함 내부에는 색인시 전달된 원본 json 문서의 본문이 저장돼 있음
- _reindex api나 스크립트를 사용해 값을 계산할때 해당 메타 필드를 활용 가능

```
# 요청
PUT reindex_movie
{}

# 결과
{
  "acknowledged": true,
  "shards_acknowledged": true,
  "index": "reindex_movie"
}





# 요청
POST _reindex
{
  "source": {
    "index": "movie_search",
    "query": {
      "match": {
        "movieCd": "20173732"
      }
    }
  },
  "dest": {
    "index": "reindex_movie"
  },
  "script": {
    "source": "ctx._source.prdtYear++"
  }
}
```

### _all 메타 필드

- 색인세 사용된 모든 필드의 정보를 가진 메타 필드
- 모든 필드의 내용이 하나의 텍스트로 합쳐져서 제공됨 특정 필드가 아닌 문서 전체 필들에서 특정 키워드를 검색한다면 _all 메타 필드를 사용하면 됨
- 6 이상부터 폐기됨 copy_to로 대체할 것

### _routing 메타 필드

- 특정 문서를 특정 샤드에 저장하기 위해 사용자가 지정하는 메타 필드

## 필드 데이터 타입

- 매핑 설정을 위해서 엘라스틱서치에서 제공하는 데이터 타입으로 어떻나 종류가 있는지 정확하게 이해하는 것이 중요함
- 이를 바탕으로 데이터의 종류와 형태에 따라 데이터타입을 선택적으로 사용해야 함 
- 필드에는 다음과 같은데이터 타입을 지정할 수 있음
  - keyword, text 같은 문자열 데이터 타입
  - date, long, double, integer, boolean, ip 같은 일반적인 데이터 타입
  - 객체 또는 중첩문과 같은 JSON 계층의 특성의 데이터 타입
  - geo_point, geo_shape 같은 특수한 데이터 타입 

### keyword 데이터 타입

- keyword 데이터 타입은 말 그대로 키워드 형태로 사용할 데이터에 적합한 데이터 타입임. keyword 타입을 사용할 경우 별도의 분석기를 거치지 않고 원문 그대로 색인하기 때문에 특정 코드나 키워드 등 정형화된 콘텐츠에 주로 사용됨.
- 엘라 일부 기능은 형태소 분석을 하지 않아야만 사요이 가능한데 이경우에도 keyword 데이터 타입이 사용됨
- 주로 검색시 필터링되는 항목, 정렬이 필요한 항목, 집계해야하는 항목에서 많이 사용하고 반드시 사용해야함
- 만약 elastic search라는 문자열이 keyword 타입으로 설정되면 elastic이나 search라는 질의로 절대 검색 안됨
- keyword 데이터 타입의 주요 파라미터
  - boost: 필드의 가중치로 검색 결과 정렬에 영향을 줌 1.0에서 1보다 크면 점수가 높게 오르고 낮으면 점수가 낮게 오름. 이를 이용해 검색에 사용된 키워드와 문서 간의 유사도 스코어 값을 계산할 때 필드의 가중치 값을 얼마나 더 줄것인지를 판단함
  - doc_values: 필드를 메모리에 로드해 캐시에 사용 기본값 true
  - index: 해당 필드를 검색에 사용할지를 설정함 기본값은 true
  - null_value: 기본적으로 엘라는 데이터 값이 없으면 필드를 생성하지 않음 데이터의 값이 없는 경우 null 필들의 값을 대체할 지설정
  - store: 필드 값을 필드와 별도로 _source에 저장하고 검색 가능하게 할지를 설정 기본값은 false

### Text 데이터 타입

- Text 데이터 타입을 이용하면 색인 시 지정된 분석기가 칼럼의 데이터를 문자열 데이터로 인식하고 이를 분석함.
- 별도의 분석기를 정의하지 않았으면 Standard Analyzer를 사용
- 영화의 제목이나 영화의 설명글과 같이 문장 형태의 데이터에 사용하기 적합함
- Text 데이터 타입은 전문 검색이 가능하다는 점이 가장 큰 특징임
- 전체 텍스트가 토큰화되어 생성되며 특정 단어를 검색하는 것이 가능해짐
- 정렬이나 집계 연산을 사용해야 할 때가 있는데 Text와 Keyword 타입을 동시에 갖도록 멀티 필드로 설정할 수 있음
- Text 데이터 타입의 주요 파라미터
  - boost: 필드의 가중치로 검색 결과 정렬에 영향을 줌 1.0에서 1보다 크면 점수가 높게 오르고 낮으면 점수가 낮게 오름. 이를 이용해 검색에 사용된 키워드와 문서 간의 유사도 스코어 값을 계산할 때 필드의 가중치 값을 얼마나 더 줄것인지를 판단함
  - fielddata: 정렬 집계 스크립트 등에서 메모리에 저장된 필드 데이터를 사용할지를 설정함 기본값 false
  - index: 해당 필드를 검색에 사용할지를 설정함 기본값은 true
  - norms: 유사도 점수를 산정할 때 필드 길이를 고려할지를 결정함 기본값 true
  - store: 필드 값을 필드와 별도로 _source에 저장하고 검색 가능하게 할지를 설정 기본값은 false
  - search_analyzer: 검색에 사용할 형태소 분석기를 사용
  - similarity: 유사도 점수를 구하는 알고리즘을 선택함 기본값은 BM25
  - term_vector: Anlyzed 필드에 텀벡터를 저장할지를 결정함 기본값은 no

### Array 데이터 타입

- 2차원 데이터가 필요할 경우 Array 데이터 타입을 사용하면 됨
- [{}, {}] , [1,2], ["one", "two"] 형태로 사용하고 내부 데이터는 모두 같은 타입이어야함
- 엘라에서는 매핑 설정 시 Array 타입을 명시적으로 정의하지 않음 모든 필드가 기본적으로 다수의 값을 가질 수 있기 때문임
- 정의된 인덱스 피르에 단순히 배열 값을 입력하면 자동으로 Array 형태로 저장됨 만약 필드가 동적으로 추가된다면 배열의 첫번째 값이 필드의 데이터 타입을 결졍하며 이후 데이터는 모두 같은 타입어야 색인할때 오류가 발생하지 않음

### Numeric 데이터 타입 

- 엘라에서 숫자 데이터 타입은 여러가지 종류가 제공됨 
- long, integer, short, byte, double, float, half_float <- 일반적인 범위를 갖고 있고 필요하면 검색

### Date 데이터 타입

- Date 타입은 Json 포멧에서 문자열로 처리됨
- 날짜는 다양하게 표현될 수 있기 때문에 올바루게 구문 분석될 수 있게 날짜 문자열 형식을 명시적으로 설정해야함 만약 별도의 형식을 지정하지 않을 경우 기본 형식인 "yyyy-MM-ddTHH:mm:ssZ" 로 설정됨

### Range 데이터 타입

- Range 데이터 타입은 범위가 있는 데이터를 저장할 때 사용하는 데이터 타입임
- 10 ~ 20 이라면 시작과 끝만 정의하면 됨
- integer, float, long, double, date, ipv4 ipv6 범위도 지정 가능함

### Boolean 데이터 타입

- 참 또는 거짓 문자열로 표현 가능

### Geo-Point 데이터 타입

- 위도 경도 등 위치 정보를 담은 데이터를 저장할때 사용
- 위치 기반 쿼리를 이용해 반경 내 쿼리, 위치 기반 집계, 위치별 정렬 등을 사용할 수 있기 때문에 위치 기반 데이터를 색인하고 검색하는데 매우 유용함

### ip데이터 타입

- ip 주소와 같은 데이터를 저장하는데 사용함
- ipv4 ipv6 가능

### Object 데이터 타입

- json 포멧의 문서는 내부 객체를 계층적으로 포함할 수 있음
- 문서의 필드는 복잡한형태의 또 다른 문서를 포함하는 것도 가능함

### Nested 데이터 타입

- Nested 데이터 타입은 Object 객체 배열을 독립적으로 색인하고 질의하는 형태의 데이터 타입
- 데이터가 배열 형태로 저장되면 한 필드 내의 검색은 기본적으로 OR 조건으로 검색됨. 
- 이런 문제를 해결하기 위해 nested 데이터 타입이 고안됨
- 검색할 때 일치하는 문서만 정확하게 출력 가능

## 엘라스틱 서치 분석기

- 엘라는 루씬 기반으로 구축된 텍스트 기반 검색 엔진임
- 인덱스를 잘 분석하고 분석기를 구성해야함

### 텍스트 분석 개요

- 텍스트 분석을 이해하려면 루씬이 제공하는 분석기를 이해해야함
- 텍스트 분석을 할 때 별도의 분석기를 지정하지 않으면 기본적으로 Standard Anlyzer가 사용됨

### 역색인 구조

- 어떤 책을 읽을때 특정 단어를 알고 있지만 해당 단어가 등장하는 페이지를 알지 못할 때 책의 마지막 부분에 나열된 목록을 보게됨
- 루씬도 비슷함 루씬의 색인은 역색인이라는 특수한 방식으로 구조화 되어 있음
- 역색인 구조
  - 모든 문서가 가지는 단어의 고유 단어 목록
  - 해당 단어가 어떤 문서에 속해 있는지에 대한 정보
  - 전체 문서에 각 단어가 몇개 들어있는지에 대한 정보
  - 하나의 문서에 단어가 몇 번씩 출현했는지에 대한 빈도
- Ela와 ela를 동시에 가장 검색하기 좋은 간단한 방법은 소문자로 바꿔서 인덱스하는것임 원본에는 변함 없음

### 분석기 구조

- 기본 프로세스
  - 문장을 특정한 규칙에 의해 수정한다
  - 수정한 문장을 개별 토큰으로 분리한다
  - 개별 토큰을 특정한 규칙에 의해 변경한다.
- Character filter
  - 문장을 분석하기 전에 입력 텍스트에 대해 특정한 단어를 변경하거나 html과 같은 태그를 제거하는 역할을 하는 필터
- Tokenizer filter
  - 분석기를 구성할 때 하나만 사용할 수 있고 텍스트를 어떻게 나눌 것인지를 정의함 상황에 맞는 적절한 Tokenizer를 사용하면 됨
- Token Filter
  - 토큰화된 단어를 하나씩 필터링해서 사용자가 원하는 토큰으로 변환함 예를 들어 불필요한단어를 제거하거나 동의어 사전을 만들어 단어를 추가하거나 영문 단어를 소문자로 변환하는 등의 작업을 수행 가능
- 전체 프로세스
  - Character Filter -> Tokenizer filter -> TokenFilter <--> 사전 <---> TokenFilter -> 사전



* 분석기는 데이터의 특성에 따라 원하는 분석 결과를 미리 예상해보고 해당 결과를 얻기 의한 옵션을 적용해 설정해야함



### 분석기 사용법

- 엘라는 루씬에 존재하는 기본 분석기를 별도의 정의 없이 사용할 수 있게 미리 정의해서 제공함
- standard는 루씬의 Standard Anlyzer를 의미함

**분석기를 이용한 분석**

- 엘라는 형태소가 어떻게 분석되는지 확인할 수 있는 _analyze api를 제공함
- 미리 정의된 분석기의 경우 쉽게 테스트 가능

**필드를 이용한 분석**

- 인덱스를 설정할때 분석기를 직접설정 가능

**색인과 검색 시 분석기를 각각 설정**

- 분석기는 색인할때 사용되는 Index Analyzer와 검색할때 사용되는 Search Anlyzer로 구분해서 구성 가능

```
# 요청
PUT movie_analyzer
{
   "settings":{
      "index": {
        "number_of_shards": 5,
        "number_of_replicas": 1
      },
      "analysis":{
         "analyzer":{
            "movie_lower_test_analyzer":{
               "type":"custom",
               "tokenizer":"standard",
               "filter":[
                  "lowercase"
               ]
            },
            "movie_stop_test_analyzer":{
               "type":"custom",
               "tokenizer":"standard",
               "filter":[
                  "lowercase",
                  "english_stop"
               ]
            }
         },
         "filter":{
            "english_stop":{
               "type":"stop",
               "stopwords":"_english_"
            }
         }
      }
   },
   "mappings":{
      "_doc":{
         "properties":{
            "title": {
               "type":"text",
               "analyzer":"movie_lower_test_analyzer",
               "search_analyzer":"movie_stop_test_analyzer"
            }
         }
      }
   }
}
```

- 위 경우 기본적으로 StandardTokenizer를 사용하고 분리된 토큰을 소문자로 변경함
- movie_stop_test_analyzer의 경우 추가적으로 불용어를 처리한다는 점만 다름
- 색임시점과 검색시점에 다른 분석기를 사용하도록 함
- 















