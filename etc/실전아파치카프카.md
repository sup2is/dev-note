http://www.hanbit.co.kr/src/10280





# #1 아파치 카프카 개요

## 아파치 카프카

- 아파치 카프카는 여러 대의 분산 서버에서 대량의 데이터를 처리하는 분산 메시징 시스템
- 메시지를 받고, 받은 메시지를 다른 시스템이나 장치에 보내기 위해 사용
- 카프카의 실현 4가지
  1. **확장성:** 여러 서버로 확장 구성할 수 있기 때문에 데이터 양에 따라 시스템 확장 가능
  2. **영속성:** 수신한 데이터를 '디스크에 유지'할 수 있기 때문에 언제라도 데이터를 읽을 수 있음
  3. **유연성:** '연계할 수 있는 제품이 많기' 때문에 제품이나 시스템을 연결하는 허브 역할을함
  4. **신뢰성:** ''메시지 전달 보증''을 하므로 데이터 분시을 걱정하지 않아도 됨

- 카프카 자체가 높은 처리량으로 데이터를 실시간 처리하는 처리 성능에 초점을 두었음

## 카프카 탄생 배경

### 링크드인의 시스템 요구사항

- 카프카는 2011년 링크드인에서 출발, 웹사이트에서 생성되는 로그를 처리하여 웹사이트 활동을 추적하는 것을 목적으로 개발
- 링크드인의 실현 목표
  1. **높은 처리량으로 실시간 처리**
     - 전 세계 사용자의 방대한 엑세스 데이터를 처리하기 위해 우수한 처리량 필수
  2. **임의의 타임이에 데이터를 읽기**
     - 실시간처리 뿐만아니라 기존에 있는 데이터를 처리할 버퍼 역할 포함
  3. **다양한 제품과 시스템에 쉽게 연동**
     - 이용 목적에 따라 DB, 데이터웨어하우스, 하둡 등의 다른 제품과의 쉬운 연결
  4. **메시지를 잃지 않음**
     - 취급하는 메시지가 방대하더라도 메시지를 잃어서는 안됨.

### 카프카 이전 제품

#### 메시지큐

- IBM WebSphere MQ, RabbitMQ, ActiveMQ 등이 있음
- 링크드인에서 요구하는 사항과 일치하지 않는 사항들
  - **강력한 전달 보증이 오버스펙:** IBM WebSphere MQ는 메시지 단위로 트랜잭션을 지원하는 기능이 있음 이런 기능은 하나의 메시지가 정확하게 한번만 전송되는 것을 보장할 수 있지만 링크드인에서 다루는 로그의 성질을 고려하면 오버스펙이였음. 높은 처리량이 우선이여서 배제
  - **스케일 아웃이 용이한 제품이 아님:** 대량의 메시지를 처리하는데 1대의 서버로만 대응하는 것은 한계가 있음 기존 제품들도 클러스터 구성이 가능한것이 있었지만 실제로는 가용성을 위한 중복 구성에 주안점을 두고 있었음. 단순히 처리량을 높이는 목적으로 스케일 아웃 기능을 한 제품은 없었음
  - **메시지가 대량으로 쌓이는 것을 예상하지 않음:** 기존의 메시지 큐는 즉시 이용되는것을 예상했지만 장시간 대량을 축척하는 것을 예상하지 않았음 링크드인에서는 실시간뿐만아니라 대량의 배치처리도 가정하고 있었으므로 시존 메시지 큐로 감당 불가능

#### 로그 수집 시스템

- 페이스북의 Scribe, 클라우데라의 Flume
- 각 프론트엔드 서버가 중계용 서버에 로그를 전달하고 거기서 로그를 수집하여 데이터베이스와 분산파일 시스템 HDFS(Hadoop Distributed File System)에 축적함
- 링크드인에서 요구하는 사항과 일치하지 않는 사항들
  - **HDFS로 데이터 축적과 배치 처리만 고려함**: 링크드인에서도 하둡을 사용했지만 데이터 웨어하우스를 이용한 데이터 분석도 실시하고 있어서 배제, 또한 데이터를 배치 뿐만아니라 실시간으로 처리하고자 하는 요구가 있었음
  - **알기 쉬운 API가 없음:** 카프카 이전의 제품은 미들웨어 내의 구현 사양을 모르면 사용하기 힘들다는 지적이 있었음 데이터 송신 수신에 있어서 이용하기 쉬운 API가 필요했음
  - **수신하는 쪽이 임의로 메시지를 수신하기 어려움:** 기존제품들은 송신시스템에 push에 의존했지만 수신시스템이 pull 하는 방식이 오히려 더 사용하기 쉽다고 생각함

## 카프카로 링크드인 요구 사항 실현하기

- 요구사항
  1. 높은 처리량으로 실시간 처리를 한다
  2. 임의의 타이밍에 데이터를 읽는다
  3. 다양한 제품과 시스템에 쉽게 연동한다
  4. 메시지를 잃지 않는다.
- 실현 수단
  1. 메시징 모델과 스케일 아웃형 아키텍처
  2. 디스크로의 데이터 영속화
  3. 이해하기 쉬운 api 제공
  4. 전달 보증

### 메시징 모델과 스케일 아웃

- 여기에 해당하는 요구사항
  1. 높은 처리량으로 실시간 처리를 한다
  2. 임의의 타이밍에 데이터를 읽는다
  3. 다양한 제품과 시스템에 쉽게 연동한다

- 이러한 요구사항을 해걸하기 위해 카프카는 메시징 모델을 채용함
- 일반적으로 메시징 모델은 다음 세가지 요소로 구성됨
  1. **Producer:** 메시지 생산자
  2. **Broker:** 메시지 수집/전달 역할
  3. **Consumer:** 메시지 소비자

- Producer -> Broker -> Consumer

#### 큐잉 모델

- 브로커 안에 큐를 준비해 프로듀서의 메시지가 큐에 담기고 컨슈머가 큐에서 메시지를 추출함
- 하나의 큐에 대해 컨슈머가 여러개 존재하는 것을 생각할 수 있음
- 메시지는 하나의 컨슈머에게만 보장함
- 큐에서 여러개의 컨슈머가 메시지를 추출할 수 있어 컨슈머에 의한 병렬 처리 가능
- 하나의 메시지는 여러 컨슈머 중 어느 하나에서 처리함

#### 펍/섭 메시징 모델

- 이 모델에서는 프로듀서를 퍼블리셔, 컨슈머를 서브스크라이버 라고함
- 역시 브로커의 개념이 있음
- 퍼블리셔는 누가 그 메시지를 수신하는지 알 수 없고 브로커에 있는 토픽이라고 불리는 카테고리 안에 메시지를 등록함
- 퍼블리셔는 브로커에게 메시지를 보내기만하고 그 메시지를 이용하는건 신경쓰지 않음
- 퍼블리셔의 메세지는 브로커 내의 토픽이라 불리는 부분에 등록함
- 서브스크라이버는 여러개 존재하는 토픽 중 하나를 선택하여 메시지를 받음 여러 서비스크라이버가 동일한 토픽을 구독하기로 결정했다면 이 여러 서브스크라이버는 동일한 메시지를 받게됨 큐잉모델과 다른점

#### 프로듀서/컨슈머 사이에 브로커를 끼우는 장점

- **프로듀서/컨슈머 모두 접속처를 하나로 할 수 있다:** 프로듀서는 메시지를 단순히 브로커로만 보내면되고 컨슈머는 브로커에서 수신만 하면됨.
- **프로듀서/컨슈머 증감에 대응할 수 있음:** 서로의 존재를 모르기때문에 증감에 유연하게 동작 가능

#### 큐잉모델과 펍/섭 메시징 모델

- 큐잉모델과 펍/섭 메시징모델의 가장 큰 차이는 메시지를 소비하는 형식인데 펍/섭 모델의 경우 전부 다 같은 메시지를 소비하기 때문에 병렬로 동작하는 복수의 서브스크라이버에게 메시지를 전달하는 장점이 있지만 처리 능력을 높이는 효과는 없음

### 카프카 메시징 모델

- 카프카는 큐잉모델과 펍/섭모델로 구성되어 있고 이 모델을 실현하기 위해 컨슈머 그룹 이라는 개념을 도입해서 컨슈머를 확장 구성할 수 있도록 설계했음
- 브로커가 1대라면 병목이 될 수 있기때문에 브로커 역시 복수 구성이 가능함

### 디스크로의 데이터 영속화

- 여기에 해당하는 요구사항
  1. 임의의 타이밍에 데이터를 읽는다
  2. 메시지를 잃지 않는다.

- 카프카의 영속화는 디스크에서 이루어짐. 카프카는 **디스크에 영속화함에도 불구하고 높은 처리량을 제공**한다는 특징이 있음
- 카프카는 들어오는 데이터를 받아 한 묶음으로 장기간 영속화 시킬 수 있어서 **스토리지 시스템**으로도 간주할 수 있음



### 이해하기 쉬운 API 제공

- 여기에 해당하는 요구사항
  1. 다양한 제품과 시스템에 쉽게 연동
- 카프카는 프로듀서와 컨슈머를 쉽게 접속할 수 있도록 **Connect API**를 제공함 이 api를 이용하여 각종 외부 시스템과 접속하고 API를 기반으로 카프카에 접속하기 위한 프레임워크로 Kafka Connect도 제공함
- 데이터베이스, 키 밸류 스토어, 검색인덱스 등의 다양한 커넥터가 존재함
- Kafka streams로 입출력에 사용하는 스트림 처리 에플리케이션을 쉽게 구축 가능함

### 전달 보증

- 여기에 해당하는 요구사항
  1. 메시지를 잃지 않음
- 카프카의 전달 보증 수준

| 종류          | 개요                    | 재전송 유무 | 중복 삭제 유무 | 비고                                                         |
| ------------- | ----------------------- | ----------- | -------------- | ------------------------------------------------------------ |
| At Most Once  | 1회는 전달을 시도해본다 | X           | X              | 메시지는 중복되지 않지만 상실될 수 있다                      |
| At Least Once | 적어도 1회는 전달한다   | O           | X              | 메시지가 중복될 가능성은 있지만 상실되지 않는다.             |
| Exactly Once  | 1회만 전달한다.         | O           | O              | 중복되거나 상실되지도 않고 확실하게 메시지가 도달하지만 성능이 나오기가 힘들다. |

- 기존 MQ는 Exactly Once 수준을 주 목적으로 하는 경우가 많았음
- **At Least Once:** At Least Once를 실현하기 위해 Ack와 오프셋 커밋이라는 개념을 도입함 Ack는 브로커가 메시지를 수신했을 때 프로듀서에게 수신 완료 했다는 응답을 뜻함 이것으로 메시지 상실을 판단, 컨슈머가 어디까지 메세지를 받았는지 관리하기 위한 오프셋이 있고 이 전달 범위 보증의 구조를 오프셋 커밋이라고함. 이 오프셋 커밋을 통해서 메시지 재전송시에 어디서부터 재전송하면 되는지 판단 가능
- **Exactly Once:** 구체적으로 쌍방간의 실현이 모두 필요함 프로듀서와 브로커의 상호교환 사이, 컨슈머와 브로커의 상호 교환 사이 자세한 설명은 그림 



## 카프카의 확산

- 2011년 0.7 버전 출시 이후 2018년 7월 2.0 출시, 지속적인 활발한 오픈소스
- 링크드인, 야후, 넷플릭스, 시스코, 골드만삭스, 트위터, 우버, 마이크로소프트 등 굉장히 많은 기업에서 이미 카프카를 사용하고 있음



# #2 카프카 기초

## 메시지 송수신의 기본

카프카의 주요 구성 요소

- **브로커:** 데이터를 수신 전달하는 서비스
- **메시지:** 카프카에서 다루는 데이터의 최소 단위, 키밸류 형식
- **프로듀서:** 데이터의 생산자이며 브로커에게 메시지를 보내는 애플리케이션
- **컨슈머:** 브로커에서 메시지를 취득하는 애플리케이션
- **토픽:** 메시지의 종류 별로 관리하는 스토리지, 브로커에 배치되어 관리.  단일 카프카 클러스터에서 여러 종류의 메시지를 중개함



## 시스템 구성

**브로커**

-  하나의 서버 또는 인스턴스당 하나의 데몬 프로세스로 동작하여 메시지 수신/전달 요청을 받아들임
- 여러대의 클러스터로 구성할 수 있으며 브로커를 추가함으로써 수신/전달의 처리량 향상 스케일 아웃이 가능
- 브로커에서 받은 데이터는 전부 영속화가 이루어져 장기간 데이터 보존 가능

**프로듀서 API/ 컨슈머 API**

- 프로듀서 컨슈머를 구현하는 기능은 라이브러리로 제공됨 그것들을 각각 프로듀서 API, 컨슈머 API라고 칭하고 각각 API는 자바로 제공됨

**프로듀서**

- 프로듀서는 프로듀서 API를 이용하여 브로커에 데이터를 송신하기 위해 구현된 애플리케이션
- 프로듀서 기능을 내장하거나 서드 파티 플러그인 제휴를 통해 제공하는 다양한 오픈소스 소프트웨어가 있음 (Apache Log4j, Logstash ...)

**컨슈머**

- 컨슈머 API를 이용하여 메시지를 취득하도록 구현된 애플리케이션
- 프로듀서와 마찬가지로 다양한 오픈소스 소프트웨어가 존재 (Apache Spark, Apache Samza ...)

**주키퍼**

- 카프카의 브로커에 있어 분산 처리를 위한 관리 도구로 아파치 주키퍼가 필요함
- 주키퍼는 하둡 등 병렬 분산 처리용 OSS에 있어서 설정 관리, 이름 관리, 동기화를 위한 잠금 관리를 하는 구조로 자주 사용됨

**카프카 클라이언트**

- 토픽 작성 등 카프카의 동작 및 운영 상에 필요한 조작을 실행하는 서버, 브로커랑 다름

**카프카 클러스터**

- 카프카의 여러대의 브로커 서버, 주키퍼 서버로 이루어진 클러스터링의 메시지 중계 기능과 메시지 송수신을 위한 라이브러리 그룹으로 구성됨. 브로커, 주키퍼에 의해 구성된 클러스터 시스템을 카프카 클러스터라고 정의



## 분산 메시징을 위한 구조

**파티션**

- 토픽에 대한 대량의 메시지 입출력을 지원하기 위해 브로커상의 데이터를 읽고 쓰는 것은 파티션이라는 단위로 분할되어 있음
- 토픽을 구성하는 파티션은 브로커 클러스터 안에 분산 배치되어 프로듀서에서의 메시지 수신, 컨슈머로의 배달을 분산해서 실시함으로써 하나의 토픽에 대한 대규모 데이터 수신과 전달을 지원함
- 이런 파티션 정보는 브로커에 저장되기 때문에 실제 구현에서 파티션을 의식할 필요가 없음

**컨슈머 그룹**

- 카프카는 컨슈머에서 분산 스트림 처리도 고려해 설계되었기때문에 단일 애플리케이션 안에서 여러 컨슈머가 단일 토픽이나 여러 파티션에서 메시지를 취득하는 방법으로 컨슈머 그룹이라는 개념이 존재함
- 카프카 클러스터 전체에서 글로벌 ID를 컨슈머 그룹 전체에서 공유하고 여러 컨슈머는 자신이 속한 컨슈머 그룹을 식별해서 읽어들일 파티션을 분류하고 재시도를 제어함

**오프셋**

- 각 파티션에서 수신한 메시지에는 각각 일련번호가 부여되어 있어 파티션 단위로 메시지 위치를 나타내는 오프셋이라는 관리 정보를 이용해 컨슈머가 취득하는 메시지의 범위 및 재시도를 제어함 
- 제어에 사용되는 오프셋
  - Log-End-Offset(LEO) : 파티션 데이터의 끝을 나타냄
  - Current Offset: 컨슈머가 어디까지 메시지를 읽었는가를 나타냄
  - Commit Offset: 컨슈머가 어디까지 커밋했는지를 나타냄

### 메시지 송수신

- 카프가는 어느정도 메시지를 축적하여 배치 처리로 송수신하는 기능 또한 제공함

**프로듀서의 메시지 송신**

- 프로듀서가 토픽의 파티션에 메시지를 송신할 때 버퍼 기능처럼 프로듀서의 메모리를 이용하여 일정량을 축적 후 송신 가능
- 데이터 송신은 지정한 크기까지 메시지가 축적되거나 지정한 대기시간에 도달하는 것 중 하나를 트리거로 전송
- 기본설정은 1개씩 송신하지만 위에 설명한 배치처리로 성능향상 가능

**컨슈머의 메시지 취득**

- 컨슈머는 취득 대상의 토픽과 파티션에 대해 Current Offset으로 나타나는 위치에서 취득한 메시지 부터 브로커에서 보관하는 최신 메시지까지 모아서 요청 및 취득을 실시함
- 컨슈머도 한번에 1개 또는 5개 등의 처리를 할 수 있음
- 프로듀서 컨슈머 모두 메시지를 모아서 처리할 수 있지만 충분한 고려와 설계가 필요함

### 컨슈머의 롤백

- 컨슈머는 메시지를 취득하고 오프셋을 지속적으로 업데이트하고 Offset commit을 통해 컨슈머 처리 실패, 고장시 롤 백 메시지 재취득을 실현함
- 컨슈머가 메시지를 소비하고 Offset Commit을 하기 이전에 장애가 나서 Offset Commit에 실패하여도 Offset Commit을 통해 메시지 상실을 방지할 수 있음

### 메시지 전송시 파티셔닝

- 프로듀서에서 송신하는 메시지를 어떻게 파티션으로 보낼지 결정하는 파티셔닝 기능이 제공됨
- **Key의 해시값을 사용한 송신: **메시지의 Key를 명시적으로 지정함으로써 Key에 따라 송신처 파티션을 결정하는 로직이됨 동일한 Key를 가지는 메시지는 동일한 ID를 가진 파티션에 송신됨
- **라운드 로빈에 의한 송신:** 메시지 Key를 지정하지 않고 Null로 한 경우 여러 파티션으로의 메시지 송신을 라운드 로빈 방식으로 실행함
- 파티셔닝을 이용하는 경우 데이터 편차에 따른 파티션의 편향에도 주의해야함



## 데이터의 견고성을 높이는 복제 구조

- 카프카는 메시지를 중계함과 동시에 서버가 고장 났을때 수신한 메시지를 잃지 안힉 위해서 복제 구조를 갖추고 있음
- 파티션은 단일 또는 여러개의 레플리카로 구성되고 여러 레플리카 중 한개는 Leader이며 나머지는 Follower임

### 레플리카의 동기 상태

- Leader 레플리카의 복제 상태를 유지하고 있는 레플리카는 In-Sync Replica로 분류됨 ISR 로 표기
- 모든 레플리카가 ISR로 되어 있지 않은 파티션을 Under Replicated Partitions라고 함
- 복제 수와는 독립적으로 최소 ISR 수 설정이 가능
- ISR 들은 비동기로 계속적으로 복제를 유지함

### 복제 완료 최신 오프셋(High Watermark)

- 복제 사용시 High Watermark 라는 개념이 있는데 이는 복제가 완료된 오프셋이고 컨슈머는 High Watermark 까지 기록된 메시지를 취득 가능함

### 프로듀서의 메시지 도달 보증 수준

- 브로커에서 프로듀서로 메시지 송신 여부를 나타내는 Ack 를 어느 타이밍에 송신할 것인지를 제어하는 것은 성능과 내장애성에 큰 영향을 줌 ack는 3종류로 설정 가능
  - 0: 프로듀서는 메시지 송신시 Ack를 기다리지 않고 다음 메시지를 송신
  - 1: Leader Replica에 메시지가 전달되면 Ack를 반환함
  - all: 모든 ISR의 수만큼 복제되면 Ack를 반환함
- 프로듀서는 타임아웃 설정으로 Ack가 돌아오지 않은 Send를 송신 실패로 감지함



### ISR과 Ack =all, 쓰기 계속성의 관계

- 



## 정리

- 스케일 아웃 구성

  - 메시지를 중계하는 브로커를 여러 대 구성할 수 있으며, 브로커 수를 증가함으로써 클러스터 전체의 처리량을 증가시킬 수 있음

- 데이터의 디스크 영속화

  - 브로커에서 수신한 메시지는 디스크에 기록되어 영속화됨. 디스크 용량에 따라 장기간의 과거 데이터를 저장 재취득 가능

- 연계할 수 있는 제품 존재

  - 프로듀서/컨슈머를 구현하기 위한 API가 제공되어 이를 구현한 OSS가 많이 존재함

- 메시지 도달 보증

  - Ack와 Offset Commit 방식으로 메시지가 제대로 송수신 되었음을 확인하고 실패시 재시도를 허용함

  

# #3장 카프카 설치

- 카프카 설치는 책 또는 인터넷 검색으로 참고해서 설치

- confluent.repo

```
[Confluent.dist]
name=Confluent repository (dist)
baseurl=https://packages.confluent.io/rpm/5.0/7
gpgcheck=1
gpgkey=https://packages.confluent.io/rpm/5.0/archive.key

[Confluent]
name=Confluent repository
baseurl=https://packages.confluent.io/rpm/5.0
gpgcheck=1
gpgkey=https://packages.confluent.io/rpm/5.0/archive.key
endable=1
```

**설치**

```
sudo rpm --import https://packages.confluent.io/rpm/5.0/archive.key
위에 repo /etc/yum.repo.d/ 에 저장
yum clean all
sudo yum install confluent-platform-oss-2.11 -y
sudo systemctl start confluent-zookeeper
sudo systemctl start confluent-kafka

sudo systemctl stop confluent-zookeeper
sudo systemctl stop confluent-kafka

sudo systemctl restart confluent-zookeeper
sudo systemctl restart confluent-kafka


```



**topic 생성**

```
kafka-topics --zookeeper localhost:2181 --create --topic first-test --partitions 3 --replication-factor 1

kafka-topics --zookeeper kafka01:2181,kafka02:2181,kafka03:2181 --create --topic first-test --partitions 3 --replication-factor 3

```

- --zookeeper : 카프카 클러스터를 관리하고 있는 주키퍼로의 접속 정보를 지정함 이 정보는 /etc/kafka/server.properties에 있음 n개열경우 쉼표로 지정
- --create: 토픽을 작성함 --create 외에 토픽의 목록을 확인하는 --list 삭제하는 --delete 등이 있음
- --topic: 작성하는 토픽의 이름을 지정함. 여기에서는 토픽의 이름으로 first-test를 사용함 토픽의 이름엔 -와 .를 사용하지 않음
- --partitions: 작성하는 토픽의 파티션 수를 지정함 
- --replication-factor: 작성하는 토픽의 레플리카의 수를 지정함 이 수는 카프카 클러스터에서 브로커의 수보다 많을 수 없음 브로커가 1개라면 최대설정값이 1개임



**topic 확인**

```
kafka-topics --zookeeper localhost:2181 --describe --topic first-test

kafka-topics --zookeeper kafka01:2181,kafka02:2181,kafka03:2181 --describe --topic first-test
```

- Leader : 각 파티션의 현재 Leader Replica가 어떤 브로커에 존재하고 있는지 표시함 여기에 표시되는 번호는 각 브로커에 설정한 브로커 ID 임
- Replicas: 각 파티션의 레플리카를 보유하고 있는 브로커의 목록 표시
- Isr: In-Sync Replicas 레플리카 중 Leader Replica와 올바르게 동기가 실행된 복제본을 보유하고 있는 브로커 목록 장애가 발생하고 있는 브로커를 보유하고 있거나 특정 이유로 Leader Replica의 동기화가 실행되지 않는다면 Isr에 포함되지 않음 Leader Replica는 Isr에 포함됨



**Kafka Console Producer 사용하기**

```
kafka-console-producer --broker-list localhost:9092 --topic first-test

kafka-console-producer --broker-list kafka01:9092,kafka02:9092,kafka03:9092 --topic first-test
```

- --broker-list: 메시지를 보내는 카프카 클러스터의 브로커 호스트명과 포트 번호를 지정함 여러개가 있을 경우 쉼표로 구분 카프카가 통신에 사용하는 포트의 기본값은 9092임
- --topic 메시지 송신처가 되는 토픽을 지정

**Kafka Console Consumer 사용하기**

```
kafka-console-consumer --bootstrap-server localhost:9092 --topic first-test

kafka-console-consumer --bootstrap-server kafka01:9092,kafka02:9092,kafka03:9092 --topic first-test
```

- --bootstrap-server: 메시지를 수신하는 카프카 클러스터의 브로커 호스트명과 포트번호를 지정 위에 --broker-list와 동일
- --topic 메시지 송신처가 되는 토픽을 지정



# #4 자바 API를 사용하여 애플리케이션 만들기

## Kafka Producer 만들기

- 의존성 (컨플루언트가 제공하는 kafka 라이브러리임)

```
  <repositories>
    <repository>
      <id>confluent</id>
      <url>https://packages.confluent.io/maven/</url>
    </repository>
  </repositories>
  
  ..
  
  <dependencies>
    <dependency>
      <groupId>org.apache.kafka</groupId>
      <artifactId>kafka_2.11</artifactId>
      <version>2.0.0-cp1</version>
    </dependency>
  </dependencies>

```

- KafkaProducer.java

```java
    // 1. KafkaProducer에 필요한 설정
    Properties conf = new Properties();
    conf.setProperty("bootstrap.servers", "localhost:9092");
    conf.setProperty("key.serializer", "org.apache.kafka.common.serialization.IntegerSerializer");
    conf.setProperty("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
```
- bootstrap.servers: kafkaProducer가 접근할 브로커의 호스트명과 포트명 topic 생성시에 --broker-list와 동일함
- key.serializer, value,serializer: 카프카에서는 모든 메시지가 직렬화된 상태로 전송됨. 이 직렬화 처리에 이용되는 시리얼라이저 클래스를 지정함 카프카는 기본 자료형에 대한 시리얼라이즈를 다양하게 제공하고 있음



```java

        // 2. Kafka 클러스터에 메시지를 송신(produce)하는 객체를 생성
        Producer<Integer, String> producer = new KafkaProducer<>(conf);

```

- config를 통해서 Producer 객체 생성



```java
            // 3. 송신할 메시지를 생성
            ProducerRecord<Integer, String> record = new ProducerRecord<>(topicName, key, value);
```

-  ProducerRecode를 통해서 레코드 생성

```java
            // 4. 메시지를 송신하고, Ack을 받았을 때에 실행할 작업(Callback)을 등록한다
            producer.send(record, new Callback() {
                @Override
                public void onCompletion(RecordMetadata metadata, Exception e) {
                    if( metadata != null) {
                        // 송신에 성공한 경우의 처리
                        String infoString = String.format("Success partition:%d, offset:%d", metadata.partition(), metadata.offset());
                        System.out.println(infoString);
                    } else {
                        // 송신에 실패한 경우의 처리
                        String infoString = String.format("Failed:%s", e.getMessage());
                        System.err.println(infoString);
                    }
                }
            });
```

- Callback 클래스를 통해서 메시지 전송 이후 동작할 콜백메서드를 지정함
- KafkaProducer는 송신 처리를 비동기로 하기 때문에 콜백 메서드를 통한 Ack 여부를 확인할 수 있음

```java
    // 5. KafkaProducer를 클로즈하여 종료
    producer.close();
```


## Kafka Consumer 만들기

```java
        // 1. KafkaConsumer에 필요한 설정
        Properties conf = new Properties();
        conf.setProperty("bootstrap.servers", "kafka-broker01:9092,kafka-broker02:9092,kafka-broker03:9092");
        conf.setProperty("group.id", "FirstAppConsumerGroup");
        conf.setProperty("enable.auto.commit", "false");
        conf.setProperty("key.deserializer", "org.apache.kafka.common.serialization.IntegerDeserializer");
        conf.setProperty("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

```

- bootstrap.servers: 접속할 브로커의 호스트명과 포트번호
- groupid: 작성할 KafkaConsumer가 속한 Consumer Group 지정
- enable.auto.commit: 오프셋 커밋을 자동으로 실행할지의 여부
- key.deserializer, value.deserializer: 컨슈머에서 디시리얼라이즈 하는 클래스를 지정 프로듀서와 일치해야함

```java
        // 2. Kafka클러스터에서 Message를 수신(Consume)하는 객체를 생성
        Consumer<Integer, String> consumer = new KafkaConsumer<>(conf);
        // 3. 수신(subscribe)하는 Topic을 등록
        consumer.subscribe(Collections.singletonList(topicName));

```

- 프로듀서 어플리케이션과 동일한 Key Value를 가지는 Consumer객체를 생성해서 구독하도록 지정



```java
            // 4. Message를 수신하여, 콘솔에 표시한다
            ConsumerRecords<Integer, String> records = consumer.poll(1);
            
            ...
            
                // 5. 처리가 완료한 Message의 Offset을 Commit한다
                TopicPartition tp = new TopicPartition(record.topic(), record.partition());
                OffsetAndMetadata oam = new OffsetAndMetadata(record.offset() + 1);
                Map<TopicPartition, OffsetAndMetadata> commitInfo = Collections.singletonMap(tp, oam);
                consumer.commitSync(commitInfo);
```

- KafkaConsumer는 카프카 클러스터에서 메시지를 얻을 때 설정된 상한의 범위 내에서 여러 메시지를 취득함 그 다음 사용자에게 한번의 poll로 전달해도 좋다는 메시지 양의 상한까지 ConsumerRecodes에 메시지를 포함하여 전달함 설정가능
- 오프셋 커밋을 자동으로하지않고 수동으로 설정하면 5번처럼 할 수 있는데 여기에서는 하나의 메시지가 처리할때마다 오프셋을 한개씩 늘려서 커밋하고 있음
- 오프셋 커밋의 빈도는 컨슈머 애플리케이션에 장애 등이 발생했을 때에 동일 메시지의 재 처리를 어느정도 허용할 수 있는지에 따라 달라짐
- 오프셋 커밋시 commitSync라는 메서드를 사용하는데 비동기로 처리하려면 commitAsync를 사용하면 됨 commitSync는 말그대로 블로킹처리임

```java
    // 6. KafkaConsumer를 클로즈하여 종료
    consumer.close();
```




# #5 카프카 적용 사례

## 카프카 적용 사례

### 카프카 대표적인 기능

- 메시지 큐 제품/로그 수집 제품/ETL 도구 등 예전부터 존재했던 몇몇 제품의 대체

**카프카의 대표적인 기능 5가지**

1. 데이터 허브
   - 여러 시스템 사이에서 데이터를 상호 교환함
2. 로그 수집
   - BI 도구를 이용한 리포팅과 인공지능 분석을 위해 여러 서버에서 생성된 로그를 수집하고 축적할 곳에 연결
3. 웹 활동 분석
   - 실시간 대시보드와 이상 탐지/부정 검출등 웹에서의 사용자 활동을 실시간으로 파악
4. 사물인터넷
   - 센서 등 다양한 디바이스에서 보낸 데이터를 수신해서 처리한 후 디바이스에 송신
5. 이벤트 소싱
   - 데이터에 대한 일련의 이벤트를 순차적으로 기록하고 CQRS(Command Query Responsibility Segregation : 읽기 및 쓰기 아키텍처를 분리하여 취급하는 개념) 방식으로 대량의 이벤트를 유연하게 처리

### 카프카 특징 복습

- 카프카는 대량의 데이터를 높은 처리량으로 실시간 처리하기 위한 제품
- 카프카는 데이터를 전달하는 파이프라인 그 자체를 구성하기 위한 기반이라고 말할 수 있을 정도의 개념까지 올라옴
- 카프카로 실현할 수 있는 4가지
  1. 확장성: 여러 서버로 확장 구성할 수 있기 때문에 데이터 양에 따라 시스템 확장이 가능
  2. 영속성: 수신한 데이터를 디스크에 유지할 수 있기 때문에 언제라도 데이터를 읽을 수 있음
  3. 유연성: 연계할 수 있는 제품이 많기 때문에 제품이나 시스템을 연결하는 허브 역할을 함
  4. 신뢰성: 메시지 전달 보증을 하므로 데이터 분실을 걱정하지 않아도됨
- 카프카의 각 기능과 특징이 중시되는 상황
  - 실시간: 긴급성이 요구되거나 데이터를 즉시 사용하는 경우
  - 동보 전송: 하나의 동일한 데이터를 후속의 여러 시스템에서 사용하는 경우 데이터를 전달하는 관계 시스템이 단계적으로 증가하는 경우
  - 영속성: 데이터를 버퍼링해야 하는 경우나 처리 시간 간격이 다른 복수의 처리와 관련된 경우
  - 다수의 제휴 제품: 사용되는 제품이 균일하지 않고 다양한 접속을 필요로 하는 경우
  - 송수신 보증: 데이터 손실이 허용되지 않는 경우
  - 순서 보증: 데이터 소스에 있어 데이터의 생성 순서를 중시하여 순서에 따른 판단과 제어를 수반하는 경우 

### 카프카 특징과 사례 대응

| 사례         | 실시간 | 동보전송 | 영속성 | 다수의 제휴 제품 | 송수신 보증 | 순서 보증 |
| ------------ | ------ | -------- | ------ | ---------------- | ----------- | --------- |
| 데이터허브   |        | O        | O      | O                | O           |           |
| 로그 수집    |        |          | O      | O                | O           |           |
| 웹 활동 분석 | O      |          |        | O                | O           | O         |
| 사물 인터넷  | O      |          |        | O                |             |           |
| 이벤트 소싱  | O      |          | O      |                  | O           | O         |





## 데이터 허브

- 데이터 허브는 여러 곳의 데이터 소스가 되는 시스템에서 데이터를 수집하여 여러 시스템에 전달하는 아키텍처

### 데이터 허브에서 해결해야할 과제

- 데이터를 생산하는 자가 직접 송신처와 데이터를 주고 받다보면 시간이 지날수록 결합도가 증가하게됨 ex 새롭게 추가되는 수신처의 새로운 프로토콜을 생산자와 송신처에 둘다 적용해야한다는점 .. 등등 
- 이런 시스템이 분리되어 시스템 간의 연계를 효율적으로 할 수 없는 상황을 사일로화 됐다고함
- 시스템 사일로화에 의한 증가에 따라 다음과 같은 과제를 해결해야함
  1. 데이터 소스에서 생성된 동일한 데이터를 여러 시스템에서 이용함
  2. 후속 시스템마다 데이터를 필요로 하는 시기와 빈도가 다름
  3. 접속원이나 연결 시스템에서 이용되는 연계 방식이 제각각임
  4. 데이터분실을 허용하지 않음



### 카프카로 데이터 허브 구현하기

- 사일로화를 해결하기 위한 개념의 하나로 데이터 허브 아키텍처가 있음
- 데이터 허브 아키텍처란 데이터소스가 되는 시스템에서 데이터를 수집하여 해당 데이터를 여러 시스템에 전달하는 아키텍처임 => 카프카로 가능
  - 동보전송
    - 데이터 소스에서 생성된 동일 데이터를 여러 시스템에서 이용할 수 있음 이것은 펍/섭 모델로 착안했기 때문에 실현 가능
  - 영속화
    - 카프카는 데이터를 영속화함
  - 다수의 연계 제품
    - kafka connect로  연결할 수 있는 다수의 제품이 있음
  - 송수신 보증
    - kafka는 At Least One 등으로 서로 다른 수준의 송수신 보증 구현 가능



## 로그 수집

### 로그 수집으로 실현하고자 하는 것



그외 등등 .. 사물인터넷 웹 활동 분석 등등 ..

# #6 카프카를 이용한 데이터 파이프라인 구축에 필요한 사전 지식

## 카프카를 이용한 데이터 파이프라인 구성 요서

### 데이터 파이프라인이란 ?

### 

### 데이터 파이프라인의 프로듀서 구성 요소

- 데이터를 생성하고 송신하는 미들웨어가 카프카에 어떻게 대응하는지에 따라 2가지 패턴으로 나뉨

  1. 미들웨어가 직접 카프카에 메시지로 송신하는 패턴
     - 말 그대로 미들웨어가 직저 카프카에 메시지를 송신함
  2. 미들웨어가 직접 카프카에 데이터를 송신하지 않고 다른 도구로 메시지를 송신하는 패턴 
     - 웹어플리케이션의 예로 웹 어플리케이션은 로그를 파일로 생성하고 별도의 메시지 송신 도구로 카프카에 메시지를 송신하는 방식 Kafka Connect Fluentd가 있음

  

- Kafka REST Proxy를 이용하면 여러 프로그래밍 언어에서 쉽게 카프카로 메시지 송신할 수 있음 여러 장점들이 존재하지만 병목지점이 되지 않도록 주의해야함



### 데이터 파이프라인의 컨슈머 구성 요소

- 프로듀서와 마찬가지로 미들웨어가 직접 데이터를 전송하는지 아닌지에 따라 두가지 패턴으로 분류

1. 미들웨어가 직접 카프카에서 메시지를 취득해 처리하는 패턴
   - 이 패턴은 배치 처리와 스트림 처리를 모두 대응할 수 있지만 카프카가 스트림 데이터를 취급하는 기반이기 때문에 특히 스트림처리에서 많이 볼 수 있음

미쳤나봐 



**대체적으로 5장과 6장은 카프카를 활용해서 어떤식으로 데이터를 활용할지 등등의 다소 지루하고 아키텍처의 입장에 대한 장이였던 것 같음 현재 본인에게 조금 어려운 주제여서 따로 정리하지는 않겠음 5장 6장은 책을 직접 읽어보는것 추천**



# #7장 카프카와 Kafka Connect로 데이터 허브 구축하기



## Kafka Connect란?

- Kafka Connect는 아파치 카프카에 포함된 프레임워크로 카프카와 다른 시스템과의 데이터 연계에 사용함
- 카프카에 데이터를 넣거나 카프카에서부터 데이터를 추출하는과정을 간단하게 하기 위해 만들어짐
- 프로듀서와 컨슈머 사이에 둘다 구축 가능
- 카프카는 특성상 다양한 시스템과 연계되기때문에 Kafka Connect에서는 다른 시스템과 연결하는 부분을 커넥터라는 플러그인으로 구현하는 방식을 취하고 Kafka 본체 + 플러그인 구성으로 동작함
- 프로듀서쪽 커넥터를 소스라고 부르고 컨슈머쪽의 추출하는 커넥터를 싱크라고 부름
- 이미 굉장히 많은 커넥터가 존재함 컨플루언트 공홈에서 커넥터 확인 가능 소스와 싱크 둘다 구현하지 않을 수 있기때문에 사용전에느 ㄴ반드시 확인할것
- Kafka Connect는 확장 가능해서 클러스터로 구성할 수 있음



## 데이터 허브 아키텍처 응용 사례

- 데이터 허브는 여러 시스템 사이에서 데이터를 전달하기 위한 허브 역할을 함
- 시스템이 많아지면 많아질수록 모든 시스템을 연결하는 것이 현실적으로 어려워짐 => 사일로화
- 책에서 계속 나오는 얘기가 직접 데이터를 서비스끼리 송신하지말고 중간에 데이터 허브를 구축해서 유연하게 동작하도록 하라는 내용인 것 같음 ..



## Kafka Connect를 사용해서 전자상거래 사이트 데이터 허브 구축하기 (실습용)

- Kafka Connect의 돚악에서는 Standalone 모드와 Distributed 모드가 있음 Standalone 모드는 Kafka Connect가 1개만 움직이는 모드이고 개발서버에서 만 사용

```


------

echo '{
  "name" : "sink-zaiko-data",
  "config" : {
    "connector.class" : "org.apache.kafka.connect.file.FileStreamSourceConnector",
    "file" : "/root/zaiko/zaiko.txt",
    "topic" : "zaiko-data"
  }
}
' | curl -X POST -d @- http://localhost:8083/connectors --header "content-Type:application/json"



---

echo '{
  "name" : "load-zaiko-data",
  "config" : {
    "connector.class" : "org.apache.kafka.connect.file.FileStreamSinkConnector",
    "file" : "/root/zaiko/zaiko-a.txt",
    "topics" : "zaiko-data"
  }
}
' | curl -X POST -d @- http://localhost:8083/connectors --header "content-Type:application/json"
```



kafka-topics --delete --zookeeper localhost:2181 --topic zaiko-data

kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic zaiko-data



**postgresql 설치 및 설정**

- sudo yum install https://download.postgresql.org/pub/repos/yum/9.4/redhat/rhel-7-x86_64/pgdg-centos94-9.4-3.noarch.rpm -y

- sudo yum install postgresql94 postgresql94-server -y

- sudo /usr/pgsql-9.4/bin/postgresql94-setup initdb

- sudo vim /var/lib/pgsql/9.4/data/postgresql.conf

- listen_addresses '*'설정

- sudo vim /var/lib/pgsql/9.4/data/pg_hba.conf

- host all all (카프카 클러스터 ip) 

- sudo systemctl start postgresql-9.4

**mariadb 설정**

- yum install -y mariadb mariadb-server

- sudo systemctl start mariadb

  

**kafka mariadb connect/j 설정**

- wget https://downloads.mariadb.com/Connectors/java/connector-java-2.6.0 ./
- sudo cp connector-java-* /usr/share/java/kafka-connect-jdbc



### 판매 예측 시스템 준비

- s3 사용할 경우 설정

```
mkdir ~/.aws
touch ~/.aws/credentials
chmod 600 ~/.aws/credentials
vi ~/.aws/credentials



---


[default]
aws_access_key_id=AKIAJZU4VMQ3VLLQ5WUQ
aws_secret_access_key=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
```





**mariadb connector**

```

cd /usr/share/java/kafka-connect-jdbc/
wget https://downloads.mariadb.com/Connectors/java/connector-java-2.6.0/mariadb-java-client-2.6.0.jar ./


```







### Kafka connect 실행

```
cp /etc/kafka/connect-distributed.properties /etc/kafka/connect-distributed-1.properties
vi /etc/kafka/connect-distributed-1.properties

설정 ..

connect-distributed /etc/kafka/connect-distributed-1.properties 

```



**커넥터 실행**

```

echo '
{
  "name" : "load-ecsales-data",
  "config" : {
    "connector.class" : "io.confluent.connect.jdbc.JdbcSourceConnector",
    "connection.url" : "jdbc:postgresql://ec-data-server/ec",
    "connection.user" : "connectuser",
    "connection.password" : "connectpass",
    "mode": "incrementing",
    "incrementing.column.name" : "seq",
    "table.whitelist" : "ec_uriage",
    "topic.prefix" : "ecsales_",
    "tasks.max" : "3"
  }
}
' | curl -X POST -d @- http://kafka01:8083/connectors --header "content-Type:application/json"



```

- connection.url, connection.user, connection.password
  - DB에 접속하기 위한 설정 정보
- mode, incrementing. colmn.name
  - 실행하고 있는 동안 커넥터는 jdbc를 통해 rdb를 폴링함, 변경이 있으면 카프카에 전달하고 변경 감지는 incrementing 방법으로 진행함. 컬럼 값을 보고 갱신 유무를 판별함
- table.whitelist
  - 로드할 대상의 테이블을 지정 blacklist 도 있음
- topic-prefix
  - 카프카에 데이터를 넣을때 톺이 명을 결정할 접두어를 지정함
- tasks.max
  - 이 커넥터에서 만들어지는 최소의 테스크 수
  - 

```

echo '
{
  "name" : "load-possales-data",
  "config" : {
    "connector.class" : "io.confluent.connect.jdbc.JdbcSourceConnector",
    "connection.url" : "jdbc:mysql://pos-data-server/pos",
    "connection.user" : "connectuser",
    "connection.password" : "connectpass",
    "mode": "incrementing",
    "incrementing.column.name" : "seq",
    "table.whitelist" : "pos_uriage",
    "topic.prefix" : "possales_",
    "tasks.max" : "3"
  }
}
' | curl -X POST -d @- http://kafka01:8083/connectors --header "content-Type:application/json"

```



```

echo '
{
  "name" : "sink-sales-data",
  "config" : {
    "connector.class" : "io.confluent.connect.s3.S3SinkConnector",
    "s3.bucket.name" : "kafka-exam-bucket",
    "s3.region" : "ap-northeast-2",
	"storage.class" : "io.confluent.connect.s3.storage.S3Storage",
	"format.class" : "io.confluent.connect.s3.format.json.JsonFormat",
	"flush.size" : 3,
	"topics" : "possales_pos_uriage, escales_ec_uriage",
    "tasks.max" : "3"
  }
}
' | curl -X POST -d @- http://kafka02:8083/connectors --header "content-Type:application/json"

```





## 데이터 관리와 스키마 에볼루션

### 스키마 에볼루션

- 데이터 허브로 다수의 시스템과 접속하는 경우에는 종종 접속한 시스템의 데이터가 변경되었을 경우를 염두에 두어야함
- 모든 시스템의 애플리케이션을 동시에 수정해서 배포하는 것은 현실적이지 않음
- 이런 변화에 대응하고 스키마가 진화하는 것을 스키마 에볼루션이라고함



### 스키마 호환성

- 호환성은 다음과 같은 것을 고려해야함
  - 후방(하위) 호환성
  - 전방(상위) 호환성
  - 완전 호환성
- 각 뜻은 직접 찾아볼것 ...



### Schema Registry

- Schema Registry는 카프카 클러스터 외부에서 스키마만을 관리하는 기능을 지닌 서비스
- Schema Registry는 카프카에 포함된 것이 아니라 컨플루언트가 제공하고 있음
- Schema Registry가 있는 환경에서는 프로듀서와 컨슈머가 데이터 자체에 스키마를 갖게 해 카프카에 전달하는 것이 아니라, 스키마 정보를 Schema Registry에 등록하고 그때 Schema Registry에서 부여되는 id를 직렬화된 데이터 본체에 부가해서 카프카에게 전송함
- 이런 구조로 프로듀서가 변경하려고 하는 스키마가 필요한 호환성을 충족하는지 여부를 Schema Registry에서 확인하고 호환성이 충족되지 않는다면 Schema Registry에서 받아들이지 않으며 프로듀서는 카프카에게 데이터를 보낼수 없음



### Schema Registry 준비

```
sudo vi /etc/schema-registry/schema-registry.properties
  - kafkastore.connection.url=kafka-broker-01:2181,kafka-broker-02:2181,kafka-broker-03:2181
  - kafkastore.bootstrap.servers=PLAINTEXT://kafka-broker-01:9092,PLAINTEXT://kafka-broker-02:9092,PLAINTEXT://kafka-broker-03:9092
  
  
sudo systemctl start confluent-schema-registry

systemctl status confluent-schema-registry

curl -X GET http://kafka-broker-01:8081/config

connect-distributed-1-sr.propertie생성

bootstrap.servers=kafka-broker-01:9092,kafka-broker-02:9092,kafka-broker-03:9092
group.id=...
key.converter=io.confluent.connect.avro.AvroConverter
key.converter=io.confluent.connect.avro.AvroConverter
key.converter.schema.registry.url=http://kafka-broker-01:8081,http://kafka-broker-02:8081,http://kafka-broker-03:8081
value.converter=io.confluent.connect.avro.AvroConverter
value.converter.schema.registry.url=http://kafka-broker-01:8081,http://kafka-broker-02:8081,http://kafka-broker-03:8081



```

- Schema Registry는 avro가 기본 데이터 직렬화 포맷임

```



connect-distributed /etc/kafka/connect-distributed-1-sr.properties

---

echo '
{
  "name" : "load-possales-data-sr-1",
  "config" : {
    "connector.class" : "io.confluent.connect.jdbc.JdbcSourceConnector",
    "connection.url" : "jdbc:mysql://pos-data-server/pos",
    "connection.user" : "connectuser",
    "connection.password" : "connectpass",
    "mode": "incrementing",
    "incrementing.column.name" : "seq",
    "table.whitelist" : "pos_uriage",
    "topic.prefix" : "possales_sr_1_",
    "tasks.max" : "3"
  }
}
' | curl -X POST -d @- http://kafka-broker-02:8083/connectors --header "content-Type:application/json"



---


echo '
{
  "name" : "sink-sales-data-sr-1",
  "config" : {
    "connector.class" : "io.confluent.connect.s3.S3SinkConnector",
    "s3.bucket.name" : "kafka-exam-bucket",
    "s3.region" : "ap-northeast-2",
    "storage.class" : "io.confluent.connect.s3.storage.S3Storage",
    "format.class" : "io.confluent.connect.s3.format.json.JsonFormat",
    "flush.size" : 3,
    "topics" : "possales_sr_1_pos_uriage",
    "tasks.max" : "3"
  }
}
' | curl -X POST -d @- http://kafka-broker-01:8083/connectors --header "content-Type:application/json"

```

- schema registry는 책내용만 참고하면 절대 안되는 구성임..  자꾸 key.converter가 null로 잡히는거 보니 뭔가 이상한 듯





# #8 스트림 처리 기본

- 스트림 처리는 간헐적으로 유이보디는 데이터를 수시로 처리하는 처리 모델임
- 스트림 처리는 모인 단위 데이터를 한꺼번에 다루는 배치모델과는 대조되는 형태임
- 배치처리는 작업이라는 단위로 실행되어 시작과 끝이 분명한데 스트림은 시작과 끝이 불분명함

## Kafka Streams

- Kafka Streams는 카프카가 빌트인으로 제공하는 스트림 처리를 위한 API임



## 컴퓨터 시스템의 매트릭스 

- cpu 사용량과 메모리 사용량은 순간순간의 하드웨어와 소프트웨어 상태를 나타내는 통곗값으로 이를 매트릭스라 부름
- 네트워크나 디스크 I/O 같은 OS에서 얻을 수 있는 매트릭스 이외에 요청 처리 수와 같은 각종 미들웨어가 제공하는 다양한 매트릭스가 존재
- 매트릭스의 집계 처리는 카프카가 자랑하는 사례의 하나로 생각 할 수 있음

## 카프카 브로커의 매트릭스를 시각화하기

- 카프카 자체도 브로커와프로듀서 컨슈머의 상태를 모니터링하기 위한 매트릭스를 출력함



### 매트릭스 처리의 흐름

1. Fluentd로 카프카 브로커의 매트릭스를 정기적으로 취득하고 카프카의 토픽에 기록함
2. Kafka Streams로 매트릭스 데이터를 가공함
3. 처리된 매트릭스를 Fluentd로 꺼내 InfluxDB에 저장함
4. InfluxDB에 저장된 매트릭스를 Grafana로 시각화함

### 

### 카프카 설정

### Jolokia 설정

- 카프카는 JMX를 이용하여 매트릭스를 제공하고 있음
- 자바 이외의 언어로 구현된 도구에서 JMX 정보를 얻기 위해 Jolokia를 사용함
- Jolokia는 자바 에이전트 라이브러리로 자바 프로그램에서 로드되어 HTTP를 통해 JMX 정보를 얻을 수 있는 기능을 제공함



**설치**

```
curl -L -O https://github.com/rhuss/jolokia/releases/download/v1.5.0/jolokia-1.5.0-bin.tar.gz
tar zxf jolokia-1.5.0-bin.tar.gz
mv jolokia-1.5.0 /opt/
```



- 카프카에서 서비스 설정을 수정하여 JVM 옵션을 추가하고 Jolokia 라이브러리가 카프카에서 로드되도록 설정

```
cp /lib/systemd/system/confluent-kafka.service /tmp/
sudo vi /lib/systemd/system/confluent-kafka.service
 - Service 섹션 내부
 - Environment=KAFKA_OPTS=-javaagent:/opt/jolokia-1.5.0/agents/jolokia-jvm.jar
 
diff /tmp/confluent-kafka.service /lib/systemd/system/confluent-kafka.service
 
 
 systemctl daemon-reload
 systemctl restart confluent-kafka
 sudo ss -anlp | grep 8778
 

```

- curl로 매트릭스 취득

```
curl -s 'http://localhost:8778/jolokia/read/kafka.server:type=RelicaManager,name=UnderReplicatedPartitions' | python -m json.tool
curl -s 'http://localhost:8778/jolokia/read/kafka.server:*' | python -m json.tool
```





### Fluentd 설치

- 플러그인을 조합하여 다양한 데이터 저장소 사이에서 연계할 수 있는 점이 특징
- 데이터 수집기

```
curl -L https://toolbelt.treasuredata.com/sh/install-redhat-td-agent3.sh | sh
```

- fluentd 설정파일 (/etc/td-agent/td-agent.conf)

```
<source>
  @type exec
  tag kafka.metrics.raw.broker
  command curl -s 'http://localhost:8778/jolokia/read/kafka.server:*'
  run_interval 10s
  <parse>
    @type json
  </parse>
  
  
</source>

...

<filter kafka.metrics.raw.*>
  @type record_transformer
  <recode>
    topic kafka.metrics
    hostname "#{Socket.gethostname}"
  </recode>
</filter>

...

<match kafka.metrics.raw.*>
  @type kafka2
  brokers localhost:9092
  topic_key topic
  partition_key_key hostname
  default_message_key nohostname
  max_send_retries 1
  required_acks -1
  <format>
    @type json
  </format>
  <buffer topic>
    flush_interval 10s
  </buffer>
</match>



```



### kafka streams에 의한 데이터 처리

- chapter8에 소스 있음

```
sudo mkdir -p /var/lib/kafka-streams/streaming-example-1
sudo chown root:root /var/lib/kafka-streams/streaming-example-1

CLASSPATH=./target/kafka-metrics-processor-1.0-SNAPSHOT.jar kafka-run-class com.example.StreamingExample1

kafka-console-consumer --bootstrap-server localhost:9092 --topic kafka.metrics.processed

```



### InfluxDB에서 데이터 로드

- 데이터 시각화를 위한 준비로 InfluxDB에 기록함
- InfluxDB는 시계열 데이터 저장에 특화된 데이터 저장소로, 매트릭스의 정보를 시각화하기 위해 사용하는 Grafana의 데이터 소스로 사용됨
- 설치

```
cat <<EOF | sudo tee /etc/yum.repos.d/influxdb.repo
[influxdb]
name = InfluxDB Repository - RHEL \$releasever
baseurl = https://repos.influxdata.com/rhel/\$releasever/\$basearch/stable
enable = 1
gpgcheck = 1
gpgkey = https://repos.influxdata.com/influxdb.key
EOF


sudo yum install -y influxdb

sudo systemctl start influxdb

influx -execute 'CREATE DATABASE kmetrics'

sudo td-agent-gem install fluent-plugin-influxdb


```

- /etc/td-agent/td-agent.conf 에 추가

```
<source>
  @type kafka_group
  brokers localhost:9092
  consumer_group kafka-fluentd-influxdb
  topics kafka.metrics.processed
  format json
  offset_commit_interval 60
</source>

<match kafka.metrics.processed>
  @type influxdb
  host localhost
  prot 8086
  dbname kmetrics
  measurement kafka.broker
  tag_keys ["hostname"]
  time_key "timestamp"
  <buffer>
    @type memory
    flush_interval 10s
  </buffer>
</match>


---

systemctl restart td-agent

influx -database kmetrics


select * from "kafka.broker" LIMIT 5

```

- 시계열 데이터를 보관하는 influxdb는 기존 rdb랑은 조금 다름
- 데이터 집합을 나타내는 개념이 measurement이며 이는 데이터베이스의 테이블에 해당함
- 데이터 레코드는 /타임스탬프/필드값/태그값임

### Grafana 설정

- 설치

```
sudo yum install https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana-5.2.1-1.x86_64.rpm

systemctl start grafana-server
```



### 프로그램 분석 (StreamingExample1.java)



```
KStream<String, String> metrics = builder.stream("kafka.metrics",
                                                     Consumed.with(Serdes.String(), Serdes.String()));
```

- KStream은 Kafka Streams가 제공하는 클래스로Stream DSL이라는 추상도가 높은 API의 일부임 비슷한걸로 java stream api

```
    metrics.flatMapValues(wrap(text -> mapper.readTree(text)))
           .filter((host, root) -> root.has("value") && root.has("hostname") && root.has("timestamp"))
           .flatMapValues(wrap(root -> {
               ObjectNode newroot = mapper.createObjectNode();
               newroot.put("hostname", root.get("hostname"));
               newroot.put("timestamp", root.get("timestamp"));
               newroot.put("BytesIn",
                           root.get("value")
                               .get("kafka.server:name=BytesInPerSec,type=BrokerTopicMetrics")
                               .get("Count"));
               return  mapper.writeValueAsString(newroot);
             }))
           .to("kafka.metrics.processed",
               Produced.with(Serdes.String(), Serdes.String()));

```

- KStream의 각 요소는 간헐적으로 카프카의 토픽에 계속 저장되는 레코드이고 처리도 간헐적으로 이루어짐

### 

### 스트림 처리 오류 다루기

- 예외처리에 있어서 try-catch와 stream을 사용하는 코드 말고 try-catch stream 사용코드를 메서로 빼고 실제 사용부분에서는 이 메서드를 호출하는 걸로 작성할 것





## Kafka Streams의 장점

- Kafka Streams를 이용해서 애플리케이션을 구현하는 것이 기본적인 카프카 API를 이용하는 경우보다 가독성이 좋으며 유지보수 또한 쉬운 코드 작성이 가능함
- Kafka Streams는 카프카에 부속된 라이브러리이기 때문에 도입하기 쉬움
- 여러 프로세스에 의한 병렬 처리를 쉽게 구현할 수 있다는 점이 핵심
- 어려운부분은 partition, consumer group으로 해결하고 에러처리는 kafka offset으로 처리함
- kafka streams로 처리한 데이터를 반드시 카프카 토픽에 기록할 필요는 없음

















